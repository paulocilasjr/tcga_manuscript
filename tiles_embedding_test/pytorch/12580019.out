Setting random seed to 42
Using device: cuda
Loading CSV from ./../../../../trident/TCGA_BRCA/convert_h5_to_csv/csv_files/tcga_patches_with_labels.csv
Preparing 10-fold cross-validation

Starting Fold 1/10
Fold 1 - Training label counts: {1: 188459, 0: 187582}
Fold 1 - Validation label counts: {1: 34792, 0: 16938}
Fold 1 - Test label counts: {1: 61491, 0: 54408}
Preparing dataset with 376041 samples
Extracting features
Normalizing embeddings
Dataset ready with 376041 samples
Preparing dataset with 51730 samples
Extracting features
Normalizing embeddings
Dataset ready with 51730 samples
Preparing dataset with 115899 samples
Extracting features
Normalizing embeddings
Dataset ready with 115899 samples
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0880
Batch 20000/94011 completed, running loss: 0.0647
Batch 30000/94011 completed, running loss: 0.0539
Batch 40000/94011 completed, running loss: 0.0469
Batch 50000/94011 completed, running loss: 0.0430
Batch 60000/94011 completed, running loss: 0.0399
Batch 70000/94011 completed, running loss: 0.0372
Batch 80000/94011 completed, running loss: 0.0349
Batch 90000/94011 completed, running loss: 0.0332
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 1/50
Train Loss: 0.0326, Train Acc: 0.9876
Val Loss: 4.1132, Val Acc: 0.4826, ROC-AUC: 0.4722
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0134
Batch 20000/94011 completed, running loss: 0.0135
Batch 30000/94011 completed, running loss: 0.0139
Batch 40000/94011 completed, running loss: 0.0141
Batch 50000/94011 completed, running loss: 0.0142
Batch 60000/94011 completed, running loss: 0.0138
Batch 70000/94011 completed, running loss: 0.0139
Batch 80000/94011 completed, running loss: 0.0138
Batch 90000/94011 completed, running loss: 0.0137
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 2/50
Train Loss: 0.0137, Train Acc: 0.9949
Val Loss: 4.3333, Val Acc: 0.5534, ROC-AUC: 0.6063
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0095
Batch 20000/94011 completed, running loss: 0.0096
Batch 30000/94011 completed, running loss: 0.0095
Batch 40000/94011 completed, running loss: 0.0101
Batch 50000/94011 completed, running loss: 0.0104
Batch 60000/94011 completed, running loss: 0.0100
Batch 70000/94011 completed, running loss: 0.0100
Batch 80000/94011 completed, running loss: 0.0099
Batch 90000/94011 completed, running loss: 0.0100
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 3/50
Train Loss: 0.0099, Train Acc: 0.9964
Val Loss: 5.6561, Val Acc: 0.5126, ROC-AUC: 0.5260
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0060
Batch 20000/94011 completed, running loss: 0.0069
Batch 30000/94011 completed, running loss: 0.0072
Batch 40000/94011 completed, running loss: 0.0072
Batch 50000/94011 completed, running loss: 0.0073
Batch 60000/94011 completed, running loss: 0.0074
Batch 70000/94011 completed, running loss: 0.0075
Batch 80000/94011 completed, running loss: 0.0075
Batch 90000/94011 completed, running loss: 0.0076
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 4/50
Train Loss: 0.0076, Train Acc: 0.9973
Val Loss: 5.9200, Val Acc: 0.5455, ROC-AUC: 0.5532
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0064
Batch 20000/94011 completed, running loss: 0.0062
Batch 30000/94011 completed, running loss: 0.0062
Batch 40000/94011 completed, running loss: 0.0061
Batch 50000/94011 completed, running loss: 0.0061
Batch 60000/94011 completed, running loss: 0.0060
Batch 70000/94011 completed, running loss: 0.0061
Batch 80000/94011 completed, running loss: 0.0061
Batch 90000/94011 completed, running loss: 0.0061
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 5/50
Train Loss: 0.0061, Train Acc: 0.9978
Val Loss: 7.2017, Val Acc: 0.5230, ROC-AUC: 0.5500
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0046
Batch 20000/94011 completed, running loss: 0.0051
Batch 30000/94011 completed, running loss: 0.0047
Batch 40000/94011 completed, running loss: 0.0048
Batch 50000/94011 completed, running loss: 0.0050
Batch 60000/94011 completed, running loss: 0.0051
Batch 70000/94011 completed, running loss: 0.0051
Batch 80000/94011 completed, running loss: 0.0052
Batch 90000/94011 completed, running loss: 0.0052
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 6/50
Train Loss: 0.0052, Train Acc: 0.9982
Val Loss: 9.5888, Val Acc: 0.5093, ROC-AUC: 0.5059
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0038
Batch 20000/94011 completed, running loss: 0.0043
Batch 30000/94011 completed, running loss: 0.0041
Batch 40000/94011 completed, running loss: 0.0040
Batch 50000/94011 completed, running loss: 0.0041
Batch 60000/94011 completed, running loss: 0.0043
Batch 70000/94011 completed, running loss: 0.0043
Batch 80000/94011 completed, running loss: 0.0042
Batch 90000/94011 completed, running loss: 0.0042
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 7/50
Train Loss: 0.0043, Train Acc: 0.9985
Val Loss: 11.2557, Val Acc: 0.4856, ROC-AUC: 0.4822
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0039
Batch 20000/94011 completed, running loss: 0.0042
Batch 30000/94011 completed, running loss: 0.0041
Batch 40000/94011 completed, running loss: 0.0041
Batch 50000/94011 completed, running loss: 0.0041
Batch 60000/94011 completed, running loss: 0.0040
Batch 70000/94011 completed, running loss: 0.0041
Batch 80000/94011 completed, running loss: 0.0041
Batch 90000/94011 completed, running loss: 0.0041
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 8/50
Train Loss: 0.0042, Train Acc: 0.9985
Val Loss: 12.3125, Val Acc: 0.4998, ROC-AUC: 0.4917
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0035
Batch 20000/94011 completed, running loss: 0.0034
Batch 30000/94011 completed, running loss: 0.0031
Batch 40000/94011 completed, running loss: 0.0034
Batch 50000/94011 completed, running loss: 0.0034
Batch 60000/94011 completed, running loss: 0.0033
Batch 70000/94011 completed, running loss: 0.0033
Batch 80000/94011 completed, running loss: 0.0034
Batch 90000/94011 completed, running loss: 0.0033
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 9/50
Train Loss: 0.0033, Train Acc: 0.9989
Val Loss: 13.4349, Val Acc: 0.4957, ROC-AUC: 0.4624
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0029
Batch 20000/94011 completed, running loss: 0.0031
Batch 30000/94011 completed, running loss: 0.0031
Batch 40000/94011 completed, running loss: 0.0029
Batch 50000/94011 completed, running loss: 0.0031
Batch 60000/94011 completed, running loss: 0.0032
Batch 70000/94011 completed, running loss: 0.0032
Batch 80000/94011 completed, running loss: 0.0031
Batch 90000/94011 completed, running loss: 0.0032
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 10/50
Train Loss: 0.0031, Train Acc: 0.9989
Val Loss: 15.3654, Val Acc: 0.5030, ROC-AUC: 0.4871
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0020
Batch 20000/94011 completed, running loss: 0.0022
Batch 30000/94011 completed, running loss: 0.0025
Batch 40000/94011 completed, running loss: 0.0025
Batch 50000/94011 completed, running loss: 0.0026
Batch 60000/94011 completed, running loss: 0.0026
Batch 70000/94011 completed, running loss: 0.0028
Batch 80000/94011 completed, running loss: 0.0028
Batch 90000/94011 completed, running loss: 0.0027
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 11/50
Train Loss: 0.0027, Train Acc: 0.9991
Val Loss: 16.8136, Val Acc: 0.5338, ROC-AUC: 0.5279
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0019
Batch 20000/94011 completed, running loss: 0.0023
Batch 30000/94011 completed, running loss: 0.0028
Batch 40000/94011 completed, running loss: 0.0027
Batch 50000/94011 completed, running loss: 0.0027
Batch 60000/94011 completed, running loss: 0.0027
Batch 70000/94011 completed, running loss: 0.0026
Batch 80000/94011 completed, running loss: 0.0028
Batch 90000/94011 completed, running loss: 0.0027
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 12/50
Train Loss: 0.0027, Train Acc: 0.9991
Val Loss: 15.9049, Val Acc: 0.5186, ROC-AUC: 0.4907
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0030
Batch 20000/94011 completed, running loss: 0.0028
Batch 30000/94011 completed, running loss: 0.0026
Batch 40000/94011 completed, running loss: 0.0025
Batch 50000/94011 completed, running loss: 0.0024
Batch 60000/94011 completed, running loss: 0.0024
Batch 70000/94011 completed, running loss: 0.0024
Batch 80000/94011 completed, running loss: 0.0025
Batch 90000/94011 completed, running loss: 0.0025
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 13/50
Train Loss: 0.0025, Train Acc: 0.9992
Val Loss: 15.5274, Val Acc: 0.5414, ROC-AUC: 0.5364
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0020
Batch 20000/94011 completed, running loss: 0.0021
Batch 30000/94011 completed, running loss: 0.0027
Batch 40000/94011 completed, running loss: 0.0026
Batch 50000/94011 completed, running loss: 0.0025
Batch 60000/94011 completed, running loss: 0.0025
Batch 70000/94011 completed, running loss: 0.0025
Batch 80000/94011 completed, running loss: 0.0024
Batch 90000/94011 completed, running loss: 0.0025
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 14/50
Train Loss: 0.0024, Train Acc: 0.9993
Val Loss: 18.6150, Val Acc: 0.5242, ROC-AUC: 0.5295
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0019
Batch 20000/94011 completed, running loss: 0.0021
Batch 30000/94011 completed, running loss: 0.0023
Batch 40000/94011 completed, running loss: 0.0022
Batch 50000/94011 completed, running loss: 0.0021
Batch 60000/94011 completed, running loss: 0.0022
Batch 70000/94011 completed, running loss: 0.0023
Batch 80000/94011 completed, running loss: 0.0023
Batch 90000/94011 completed, running loss: 0.0023
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 15/50
Train Loss: 0.0023, Train Acc: 0.9993
Val Loss: 21.4076, Val Acc: 0.4633, ROC-AUC: 0.4278
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0014
Batch 20000/94011 completed, running loss: 0.0017
Batch 30000/94011 completed, running loss: 0.0017
Batch 40000/94011 completed, running loss: 0.0017
Batch 50000/94011 completed, running loss: 0.0018
Batch 60000/94011 completed, running loss: 0.0018
Batch 70000/94011 completed, running loss: 0.0019
Batch 80000/94011 completed, running loss: 0.0020
Batch 90000/94011 completed, running loss: 0.0021
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 16/50
Train Loss: 0.0021, Train Acc: 0.9994
Val Loss: 18.6443, Val Acc: 0.5142, ROC-AUC: 0.5020
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0017
Batch 20000/94011 completed, running loss: 0.0023
Batch 30000/94011 completed, running loss: 0.0021
Batch 40000/94011 completed, running loss: 0.0021
Batch 50000/94011 completed, running loss: 0.0020
Batch 60000/94011 completed, running loss: 0.0020
Batch 70000/94011 completed, running loss: 0.0020
Batch 80000/94011 completed, running loss: 0.0020
Batch 90000/94011 completed, running loss: 0.0021
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 17/50
Train Loss: 0.0021, Train Acc: 0.9994
Val Loss: 16.0499, Val Acc: 0.5277, ROC-AUC: 0.5114
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0017
Batch 20000/94011 completed, running loss: 0.0015
Batch 30000/94011 completed, running loss: 0.0015
Batch 40000/94011 completed, running loss: 0.0016
Batch 50000/94011 completed, running loss: 0.0016
Batch 60000/94011 completed, running loss: 0.0016
Batch 70000/94011 completed, running loss: 0.0017
Batch 80000/94011 completed, running loss: 0.0017
Batch 90000/94011 completed, running loss: 0.0018
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 18/50
Train Loss: 0.0018, Train Acc: 0.9994
Val Loss: 20.1487, Val Acc: 0.5096, ROC-AUC: 0.5019
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0007
Batch 20000/94011 completed, running loss: 0.0007
Batch 30000/94011 completed, running loss: 0.0007
Batch 40000/94011 completed, running loss: 0.0008
Batch 50000/94011 completed, running loss: 0.0009
Batch 60000/94011 completed, running loss: 0.0009
Batch 70000/94011 completed, running loss: 0.0008
Batch 80000/94011 completed, running loss: 0.0008
Batch 90000/94011 completed, running loss: 0.0009
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 19/50
Train Loss: 0.0009, Train Acc: 0.9997
Val Loss: 23.9169, Val Acc: 0.5093, ROC-AUC: 0.5065
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0004
Batch 20000/94011 completed, running loss: 0.0005
Batch 30000/94011 completed, running loss: 0.0004
Batch 40000/94011 completed, running loss: 0.0004
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0005
Batch 70000/94011 completed, running loss: 0.0005
Batch 80000/94011 completed, running loss: 0.0005
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 20/50
Train Loss: 0.0005, Train Acc: 0.9998
Val Loss: 26.6019, Val Acc: 0.5007, ROC-AUC: 0.4851
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0002
Batch 20000/94011 completed, running loss: 0.0003
Batch 30000/94011 completed, running loss: 0.0004
Batch 40000/94011 completed, running loss: 0.0003
Batch 50000/94011 completed, running loss: 0.0003
Batch 60000/94011 completed, running loss: 0.0004
Batch 70000/94011 completed, running loss: 0.0004
Batch 80000/94011 completed, running loss: 0.0004
Batch 90000/94011 completed, running loss: 0.0004
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 21/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 36.2962, Val Acc: 0.4783, ROC-AUC: 0.4721
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0005
Batch 20000/94011 completed, running loss: 0.0006
Batch 30000/94011 completed, running loss: 0.0005
Batch 40000/94011 completed, running loss: 0.0004
Batch 50000/94011 completed, running loss: 0.0004
Batch 60000/94011 completed, running loss: 0.0004
Batch 70000/94011 completed, running loss: 0.0004
Batch 80000/94011 completed, running loss: 0.0004
Batch 90000/94011 completed, running loss: 0.0004
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 22/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 38.8131, Val Acc: 0.4775, ROC-AUC: 0.4624
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0005
Batch 20000/94011 completed, running loss: 0.0003
Batch 30000/94011 completed, running loss: 0.0003
Batch 40000/94011 completed, running loss: 0.0004
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0005
Batch 70000/94011 completed, running loss: 0.0005
Batch 80000/94011 completed, running loss: 0.0004
Batch 90000/94011 completed, running loss: 0.0004
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 23/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 42.5494, Val Acc: 0.5086, ROC-AUC: 0.5010
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0004
Batch 20000/94011 completed, running loss: 0.0007
Batch 30000/94011 completed, running loss: 0.0005
Batch 40000/94011 completed, running loss: 0.0005
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0005
Batch 70000/94011 completed, running loss: 0.0006
Batch 80000/94011 completed, running loss: 0.0006
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 24/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 42.3948, Val Acc: 0.4958, ROC-AUC: 0.4958
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0003
Batch 20000/94011 completed, running loss: 0.0004
Batch 30000/94011 completed, running loss: 0.0004
Batch 40000/94011 completed, running loss: 0.0004
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0005
Batch 70000/94011 completed, running loss: 0.0005
Batch 80000/94011 completed, running loss: 0.0004
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 25/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 46.3677, Val Acc: 0.4564, ROC-AUC: 0.4452
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0002
Batch 20000/94011 completed, running loss: 0.0001
Batch 30000/94011 completed, running loss: 0.0002
Batch 40000/94011 completed, running loss: 0.0003
Batch 50000/94011 completed, running loss: 0.0003
Batch 60000/94011 completed, running loss: 0.0003
Batch 70000/94011 completed, running loss: 0.0003
Batch 80000/94011 completed, running loss: 0.0003
Batch 90000/94011 completed, running loss: 0.0003
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 26/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 48.0963, Val Acc: 0.4719, ROC-AUC: 0.4575
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0006
Batch 20000/94011 completed, running loss: 0.0005
Batch 30000/94011 completed, running loss: 0.0004
Batch 40000/94011 completed, running loss: 0.0004
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0006
Batch 70000/94011 completed, running loss: 0.0006
Batch 80000/94011 completed, running loss: 0.0005
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 27/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 48.8462, Val Acc: 0.4525, ROC-AUC: 0.4263
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0006
Batch 20000/94011 completed, running loss: 0.0004
Batch 30000/94011 completed, running loss: 0.0003
Batch 40000/94011 completed, running loss: 0.0004
Batch 50000/94011 completed, running loss: 0.0003
Batch 60000/94011 completed, running loss: 0.0004
Batch 70000/94011 completed, running loss: 0.0004
Batch 80000/94011 completed, running loss: 0.0004
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 28/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 50.4319, Val Acc: 0.4661, ROC-AUC: 0.4350
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0005
Batch 20000/94011 completed, running loss: 0.0006
Batch 30000/94011 completed, running loss: 0.0005
Batch 40000/94011 completed, running loss: 0.0005
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0004
Batch 70000/94011 completed, running loss: 0.0004
Batch 80000/94011 completed, running loss: 0.0004
Batch 90000/94011 completed, running loss: 0.0004
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 29/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 52.4813, Val Acc: 0.4659, ROC-AUC: 0.4481
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0005
Batch 20000/94011 completed, running loss: 0.0006
Batch 30000/94011 completed, running loss: 0.0005
Batch 40000/94011 completed, running loss: 0.0005
Batch 50000/94011 completed, running loss: 0.0005
Batch 60000/94011 completed, running loss: 0.0004
Batch 70000/94011 completed, running loss: 0.0005
Batch 80000/94011 completed, running loss: 0.0005
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 30/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 57.8332, Val Acc: 0.4423, ROC-AUC: 0.4132
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0005
Batch 20000/94011 completed, running loss: 0.0008
Batch 30000/94011 completed, running loss: 0.0007
Batch 40000/94011 completed, running loss: 0.0007
Batch 50000/94011 completed, running loss: 0.0007
Batch 60000/94011 completed, running loss: 0.0007
Batch 70000/94011 completed, running loss: 0.0006
Batch 80000/94011 completed, running loss: 0.0007
Batch 90000/94011 completed, running loss: 0.0006
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 31/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 57.9206, Val Acc: 0.4601, ROC-AUC: 0.4457
Starting training epoch with 94011 batches
Batch 10000/94011 completed, running loss: 0.0007
Batch 20000/94011 completed, running loss: 0.0008
Batch 30000/94011 completed, running loss: 0.0008
Batch 40000/94011 completed, running loss: 0.0007
Batch 50000/94011 completed, running loss: 0.0006
Batch 60000/94011 completed, running loss: 0.0006
Batch 70000/94011 completed, running loss: 0.0006
Batch 80000/94011 completed, running loss: 0.0006
Batch 90000/94011 completed, running loss: 0.0005
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Fold 1 - Epoch 32/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 55.8415, Val Acc: 0.4847, ROC-AUC: 0.4724
Fold 1 - Early stopping triggered
Starting evaluation with 94011 batches
Evaluation batch 10000/94011 completed
Evaluation batch 20000/94011 completed
Evaluation batch 30000/94011 completed
Evaluation batch 40000/94011 completed
Evaluation batch 50000/94011 completed
Evaluation batch 60000/94011 completed
Evaluation batch 70000/94011 completed
Evaluation batch 80000/94011 completed
Evaluation batch 90000/94011 completed
Starting evaluation with 12933 batches
Evaluation batch 10000/12933 completed
Starting evaluation with 28975 batches
Evaluation batch 10000/28975 completed
Evaluation batch 20000/28975 completed
Fold 1 - Test Metrics:
Loss: 1.1155
Accuracy: 0.7866
Precision: 0.9149
Recall: 0.6591
Roc_auc: 0.9125
Specificity: 0.9307
F1: 0.7662

Starting Fold 2/10
Fold 2 - Training label counts: {0: 211110, 1: 210451}
Fold 2 - Validation label counts: {1: 46444, 0: 32311}
Fold 2 - Test label counts: {1: 27847, 0: 15507}
Preparing dataset with 421561 samples
Extracting features
Normalizing embeddings
Dataset ready with 421561 samples
Preparing dataset with 78755 samples
Extracting features
Normalizing embeddings
Dataset ready with 78755 samples
Preparing dataset with 43354 samples
Extracting features
Normalizing embeddings
Dataset ready with 43354 samples
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0766
Batch 20000/105391 completed, running loss: 0.0577
Batch 30000/105391 completed, running loss: 0.0486
Batch 40000/105391 completed, running loss: 0.0431
Batch 50000/105391 completed, running loss: 0.0394
Batch 60000/105391 completed, running loss: 0.0364
Batch 70000/105391 completed, running loss: 0.0340
Batch 80000/105391 completed, running loss: 0.0322
Batch 90000/105391 completed, running loss: 0.0307
Batch 100000/105391 completed, running loss: 0.0293
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 1/50
Train Loss: 0.0285, Train Acc: 0.9891
Val Loss: 3.4061, Val Acc: 0.6332, ROC-AUC: 0.6543
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0154
Batch 20000/105391 completed, running loss: 0.0148
Batch 30000/105391 completed, running loss: 0.0142
Batch 40000/105391 completed, running loss: 0.0142
Batch 50000/105391 completed, running loss: 0.0140
Batch 60000/105391 completed, running loss: 0.0137
Batch 70000/105391 completed, running loss: 0.0138
Batch 80000/105391 completed, running loss: 0.0136
Batch 90000/105391 completed, running loss: 0.0135
Batch 100000/105391 completed, running loss: 0.0134
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 2/50
Train Loss: 0.0133, Train Acc: 0.9951
Val Loss: 3.4536, Val Acc: 0.6740, ROC-AUC: 0.7152
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0108
Batch 20000/105391 completed, running loss: 0.0098
Batch 30000/105391 completed, running loss: 0.0096
Batch 40000/105391 completed, running loss: 0.0096
Batch 50000/105391 completed, running loss: 0.0097
Batch 60000/105391 completed, running loss: 0.0096
Batch 70000/105391 completed, running loss: 0.0097
Batch 80000/105391 completed, running loss: 0.0095
Batch 90000/105391 completed, running loss: 0.0094
Batch 100000/105391 completed, running loss: 0.0094
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 3/50
Train Loss: 0.0094, Train Acc: 0.9965
Val Loss: 3.6850, Val Acc: 0.6275, ROC-AUC: 0.6707
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0076
Batch 20000/105391 completed, running loss: 0.0074
Batch 30000/105391 completed, running loss: 0.0075
Batch 40000/105391 completed, running loss: 0.0074
Batch 50000/105391 completed, running loss: 0.0074
Batch 60000/105391 completed, running loss: 0.0075
Batch 70000/105391 completed, running loss: 0.0075
Batch 80000/105391 completed, running loss: 0.0074
Batch 90000/105391 completed, running loss: 0.0075
Batch 100000/105391 completed, running loss: 0.0075
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 4/50
Train Loss: 0.0076, Train Acc: 0.9973
Val Loss: 4.4595, Val Acc: 0.6526, ROC-AUC: 0.6888
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0056
Batch 20000/105391 completed, running loss: 0.0058
Batch 30000/105391 completed, running loss: 0.0059
Batch 40000/105391 completed, running loss: 0.0058
Batch 50000/105391 completed, running loss: 0.0057
Batch 60000/105391 completed, running loss: 0.0058
Batch 70000/105391 completed, running loss: 0.0058
Batch 80000/105391 completed, running loss: 0.0059
Batch 90000/105391 completed, running loss: 0.0059
Batch 100000/105391 completed, running loss: 0.0060
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 5/50
Train Loss: 0.0061, Train Acc: 0.9978
Val Loss: 5.5442, Val Acc: 0.6312, ROC-AUC: 0.6586
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0049
Batch 20000/105391 completed, running loss: 0.0046
Batch 30000/105391 completed, running loss: 0.0047
Batch 40000/105391 completed, running loss: 0.0049
Batch 50000/105391 completed, running loss: 0.0048
Batch 60000/105391 completed, running loss: 0.0050
Batch 70000/105391 completed, running loss: 0.0050
Batch 80000/105391 completed, running loss: 0.0051
Batch 90000/105391 completed, running loss: 0.0052
Batch 100000/105391 completed, running loss: 0.0052
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 6/50
Train Loss: 0.0053, Train Acc: 0.9981
Val Loss: 6.7197, Val Acc: 0.5993, ROC-AUC: 0.6195
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0034
Batch 20000/105391 completed, running loss: 0.0038
Batch 30000/105391 completed, running loss: 0.0040
Batch 40000/105391 completed, running loss: 0.0043
Batch 50000/105391 completed, running loss: 0.0043
Batch 60000/105391 completed, running loss: 0.0044
Batch 70000/105391 completed, running loss: 0.0044
Batch 80000/105391 completed, running loss: 0.0045
Batch 90000/105391 completed, running loss: 0.0045
Batch 100000/105391 completed, running loss: 0.0046
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 7/50
Train Loss: 0.0046, Train Acc: 0.9983
Val Loss: 5.8557, Val Acc: 0.6335, ROC-AUC: 0.6664
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0040
Batch 20000/105391 completed, running loss: 0.0035
Batch 30000/105391 completed, running loss: 0.0036
Batch 40000/105391 completed, running loss: 0.0038
Batch 50000/105391 completed, running loss: 0.0039
Batch 60000/105391 completed, running loss: 0.0039
Batch 70000/105391 completed, running loss: 0.0040
Batch 80000/105391 completed, running loss: 0.0041
Batch 90000/105391 completed, running loss: 0.0041
Batch 100000/105391 completed, running loss: 0.0041
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 8/50
Train Loss: 0.0041, Train Acc: 0.9985
Val Loss: 6.9271, Val Acc: 0.6133, ROC-AUC: 0.6466
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0023
Batch 20000/105391 completed, running loss: 0.0029
Batch 30000/105391 completed, running loss: 0.0030
Batch 40000/105391 completed, running loss: 0.0030
Batch 50000/105391 completed, running loss: 0.0031
Batch 60000/105391 completed, running loss: 0.0032
Batch 70000/105391 completed, running loss: 0.0033
Batch 80000/105391 completed, running loss: 0.0033
Batch 90000/105391 completed, running loss: 0.0033
Batch 100000/105391 completed, running loss: 0.0034
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 9/50
Train Loss: 0.0034, Train Acc: 0.9988
Val Loss: 7.6072, Val Acc: 0.6154, ROC-AUC: 0.6555
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0027
Batch 20000/105391 completed, running loss: 0.0029
Batch 30000/105391 completed, running loss: 0.0029
Batch 40000/105391 completed, running loss: 0.0031
Batch 50000/105391 completed, running loss: 0.0033
Batch 60000/105391 completed, running loss: 0.0033
Batch 70000/105391 completed, running loss: 0.0033
Batch 80000/105391 completed, running loss: 0.0033
Batch 90000/105391 completed, running loss: 0.0033
Batch 100000/105391 completed, running loss: 0.0033
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 10/50
Train Loss: 0.0033, Train Acc: 0.9989
Val Loss: 10.4327, Val Acc: 0.5690, ROC-AUC: 0.5918
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0029
Batch 20000/105391 completed, running loss: 0.0027
Batch 30000/105391 completed, running loss: 0.0029
Batch 40000/105391 completed, running loss: 0.0028
Batch 50000/105391 completed, running loss: 0.0030
Batch 60000/105391 completed, running loss: 0.0029
Batch 70000/105391 completed, running loss: 0.0030
Batch 80000/105391 completed, running loss: 0.0030
Batch 90000/105391 completed, running loss: 0.0029
Batch 100000/105391 completed, running loss: 0.0030
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 11/50
Train Loss: 0.0030, Train Acc: 0.9990
Val Loss: 10.0580, Val Acc: 0.5963, ROC-AUC: 0.6246
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0030
Batch 20000/105391 completed, running loss: 0.0028
Batch 30000/105391 completed, running loss: 0.0026
Batch 40000/105391 completed, running loss: 0.0024
Batch 50000/105391 completed, running loss: 0.0025
Batch 60000/105391 completed, running loss: 0.0024
Batch 70000/105391 completed, running loss: 0.0025
Batch 80000/105391 completed, running loss: 0.0025
Batch 90000/105391 completed, running loss: 0.0025
Batch 100000/105391 completed, running loss: 0.0026
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 12/50
Train Loss: 0.0026, Train Acc: 0.9992
Val Loss: 11.9425, Val Acc: 0.5842, ROC-AUC: 0.6327
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0011
Batch 20000/105391 completed, running loss: 0.0016
Batch 30000/105391 completed, running loss: 0.0020
Batch 40000/105391 completed, running loss: 0.0020
Batch 50000/105391 completed, running loss: 0.0020
Batch 60000/105391 completed, running loss: 0.0022
Batch 70000/105391 completed, running loss: 0.0023
Batch 80000/105391 completed, running loss: 0.0023
Batch 90000/105391 completed, running loss: 0.0023
Batch 100000/105391 completed, running loss: 0.0023
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 13/50
Train Loss: 0.0023, Train Acc: 0.9993
Val Loss: 9.5257, Val Acc: 0.6419, ROC-AUC: 0.6793
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0024
Batch 20000/105391 completed, running loss: 0.0024
Batch 30000/105391 completed, running loss: 0.0022
Batch 40000/105391 completed, running loss: 0.0022
Batch 50000/105391 completed, running loss: 0.0023
Batch 60000/105391 completed, running loss: 0.0022
Batch 70000/105391 completed, running loss: 0.0022
Batch 80000/105391 completed, running loss: 0.0022
Batch 90000/105391 completed, running loss: 0.0022
Batch 100000/105391 completed, running loss: 0.0022
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 14/50
Train Loss: 0.0022, Train Acc: 0.9994
Val Loss: 14.6965, Val Acc: 0.5755, ROC-AUC: 0.6339
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0021
Batch 20000/105391 completed, running loss: 0.0020
Batch 30000/105391 completed, running loss: 0.0020
Batch 40000/105391 completed, running loss: 0.0021
Batch 50000/105391 completed, running loss: 0.0019
Batch 60000/105391 completed, running loss: 0.0019
Batch 70000/105391 completed, running loss: 0.0019
Batch 80000/105391 completed, running loss: 0.0019
Batch 90000/105391 completed, running loss: 0.0020
Batch 100000/105391 completed, running loss: 0.0020
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 15/50
Train Loss: 0.0020, Train Acc: 0.9994
Val Loss: 9.8500, Val Acc: 0.5967, ROC-AUC: 0.6366
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0024
Batch 20000/105391 completed, running loss: 0.0025
Batch 30000/105391 completed, running loss: 0.0024
Batch 40000/105391 completed, running loss: 0.0024
Batch 50000/105391 completed, running loss: 0.0022
Batch 60000/105391 completed, running loss: 0.0021
Batch 70000/105391 completed, running loss: 0.0021
Batch 80000/105391 completed, running loss: 0.0020
Batch 90000/105391 completed, running loss: 0.0020
Batch 100000/105391 completed, running loss: 0.0020
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 16/50
Train Loss: 0.0020, Train Acc: 0.9994
Val Loss: 14.5504, Val Acc: 0.5993, ROC-AUC: 0.6310
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0021
Batch 20000/105391 completed, running loss: 0.0020
Batch 30000/105391 completed, running loss: 0.0019
Batch 40000/105391 completed, running loss: 0.0018
Batch 50000/105391 completed, running loss: 0.0018
Batch 60000/105391 completed, running loss: 0.0018
Batch 70000/105391 completed, running loss: 0.0019
Batch 80000/105391 completed, running loss: 0.0020
Batch 90000/105391 completed, running loss: 0.0019
Batch 100000/105391 completed, running loss: 0.0019
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 17/50
Train Loss: 0.0019, Train Acc: 0.9994
Val Loss: 12.3913, Val Acc: 0.6056, ROC-AUC: 0.6332
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0009
Batch 20000/105391 completed, running loss: 0.0011
Batch 30000/105391 completed, running loss: 0.0013
Batch 40000/105391 completed, running loss: 0.0014
Batch 50000/105391 completed, running loss: 0.0015
Batch 60000/105391 completed, running loss: 0.0015
Batch 70000/105391 completed, running loss: 0.0016
Batch 80000/105391 completed, running loss: 0.0016
Batch 90000/105391 completed, running loss: 0.0017
Batch 100000/105391 completed, running loss: 0.0016
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 18/50
Train Loss: 0.0017, Train Acc: 0.9995
Val Loss: 12.8317, Val Acc: 0.6189, ROC-AUC: 0.6483
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0013
Batch 20000/105391 completed, running loss: 0.0010
Batch 30000/105391 completed, running loss: 0.0010
Batch 40000/105391 completed, running loss: 0.0009
Batch 50000/105391 completed, running loss: 0.0009
Batch 60000/105391 completed, running loss: 0.0009
Batch 70000/105391 completed, running loss: 0.0008
Batch 80000/105391 completed, running loss: 0.0009
Batch 90000/105391 completed, running loss: 0.0009
Batch 100000/105391 completed, running loss: 0.0009
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 19/50
Train Loss: 0.0009, Train Acc: 0.9998
Val Loss: 17.3844, Val Acc: 0.6110, ROC-AUC: 0.6395
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0003
Batch 20000/105391 completed, running loss: 0.0005
Batch 30000/105391 completed, running loss: 0.0005
Batch 40000/105391 completed, running loss: 0.0005
Batch 50000/105391 completed, running loss: 0.0006
Batch 60000/105391 completed, running loss: 0.0006
Batch 70000/105391 completed, running loss: 0.0006
Batch 80000/105391 completed, running loss: 0.0006
Batch 90000/105391 completed, running loss: 0.0006
Batch 100000/105391 completed, running loss: 0.0006
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 20/50
Train Loss: 0.0006, Train Acc: 0.9998
Val Loss: 21.8515, Val Acc: 0.5880, ROC-AUC: 0.6207
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0004
Batch 20000/105391 completed, running loss: 0.0005
Batch 30000/105391 completed, running loss: 0.0005
Batch 40000/105391 completed, running loss: 0.0005
Batch 50000/105391 completed, running loss: 0.0005
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0005
Batch 80000/105391 completed, running loss: 0.0005
Batch 90000/105391 completed, running loss: 0.0005
Batch 100000/105391 completed, running loss: 0.0006
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 21/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 26.8287, Val Acc: 0.5803, ROC-AUC: 0.6124
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0009
Batch 20000/105391 completed, running loss: 0.0006
Batch 30000/105391 completed, running loss: 0.0005
Batch 40000/105391 completed, running loss: 0.0005
Batch 50000/105391 completed, running loss: 0.0005
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0005
Batch 80000/105391 completed, running loss: 0.0005
Batch 90000/105391 completed, running loss: 0.0005
Batch 100000/105391 completed, running loss: 0.0005
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 22/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 27.6056, Val Acc: 0.6031, ROC-AUC: 0.6322
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0002
Batch 20000/105391 completed, running loss: 0.0003
Batch 30000/105391 completed, running loss: 0.0004
Batch 40000/105391 completed, running loss: 0.0003
Batch 50000/105391 completed, running loss: 0.0003
Batch 60000/105391 completed, running loss: 0.0003
Batch 70000/105391 completed, running loss: 0.0004
Batch 80000/105391 completed, running loss: 0.0004
Batch 90000/105391 completed, running loss: 0.0004
Batch 100000/105391 completed, running loss: 0.0004
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 23/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 33.9954, Val Acc: 0.5675, ROC-AUC: 0.5964
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0007
Batch 20000/105391 completed, running loss: 0.0006
Batch 30000/105391 completed, running loss: 0.0006
Batch 40000/105391 completed, running loss: 0.0005
Batch 50000/105391 completed, running loss: 0.0004
Batch 60000/105391 completed, running loss: 0.0004
Batch 70000/105391 completed, running loss: 0.0004
Batch 80000/105391 completed, running loss: 0.0004
Batch 90000/105391 completed, running loss: 0.0004
Batch 100000/105391 completed, running loss: 0.0005
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 24/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 31.4986, Val Acc: 0.5994, ROC-AUC: 0.6315
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0006
Batch 20000/105391 completed, running loss: 0.0006
Batch 30000/105391 completed, running loss: 0.0005
Batch 40000/105391 completed, running loss: 0.0005
Batch 50000/105391 completed, running loss: 0.0005
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0006
Batch 80000/105391 completed, running loss: 0.0006
Batch 90000/105391 completed, running loss: 0.0006
Batch 100000/105391 completed, running loss: 0.0006
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 25/50
Train Loss: 0.0006, Train Acc: 0.9998
Val Loss: 32.5291, Val Acc: 0.5896, ROC-AUC: 0.6212
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0005
Batch 20000/105391 completed, running loss: 0.0004
Batch 30000/105391 completed, running loss: 0.0003
Batch 40000/105391 completed, running loss: 0.0003
Batch 50000/105391 completed, running loss: 0.0004
Batch 60000/105391 completed, running loss: 0.0004
Batch 70000/105391 completed, running loss: 0.0004
Batch 80000/105391 completed, running loss: 0.0005
Batch 90000/105391 completed, running loss: 0.0006
Batch 100000/105391 completed, running loss: 0.0006
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 26/50
Train Loss: 0.0006, Train Acc: 0.9998
Val Loss: 33.0894, Val Acc: 0.5706, ROC-AUC: 0.5949
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0008
Batch 20000/105391 completed, running loss: 0.0010
Batch 30000/105391 completed, running loss: 0.0007
Batch 40000/105391 completed, running loss: 0.0006
Batch 50000/105391 completed, running loss: 0.0006
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0005
Batch 80000/105391 completed, running loss: 0.0005
Batch 90000/105391 completed, running loss: 0.0005
Batch 100000/105391 completed, running loss: 0.0004
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 27/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 44.7153, Val Acc: 0.5497, ROC-AUC: 0.5688
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0002
Batch 20000/105391 completed, running loss: 0.0003
Batch 30000/105391 completed, running loss: 0.0004
Batch 40000/105391 completed, running loss: 0.0003
Batch 50000/105391 completed, running loss: 0.0004
Batch 60000/105391 completed, running loss: 0.0003
Batch 70000/105391 completed, running loss: 0.0004
Batch 80000/105391 completed, running loss: 0.0004
Batch 90000/105391 completed, running loss: 0.0004
Batch 100000/105391 completed, running loss: 0.0005
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 28/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 46.2758, Val Acc: 0.5695, ROC-AUC: 0.5869
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0005
Batch 20000/105391 completed, running loss: 0.0007
Batch 30000/105391 completed, running loss: 0.0006
Batch 40000/105391 completed, running loss: 0.0006
Batch 50000/105391 completed, running loss: 0.0007
Batch 60000/105391 completed, running loss: 0.0006
Batch 70000/105391 completed, running loss: 0.0006
Batch 80000/105391 completed, running loss: 0.0005
Batch 90000/105391 completed, running loss: 0.0006
Batch 100000/105391 completed, running loss: 0.0006
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 29/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 48.0299, Val Acc: 0.5611, ROC-AUC: 0.5799
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0003
Batch 20000/105391 completed, running loss: 0.0004
Batch 30000/105391 completed, running loss: 0.0006
Batch 40000/105391 completed, running loss: 0.0007
Batch 50000/105391 completed, running loss: 0.0006
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0005
Batch 80000/105391 completed, running loss: 0.0006
Batch 90000/105391 completed, running loss: 0.0006
Batch 100000/105391 completed, running loss: 0.0006
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 30/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 45.9335, Val Acc: 0.5817, ROC-AUC: 0.6051
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0003
Batch 20000/105391 completed, running loss: 0.0004
Batch 30000/105391 completed, running loss: 0.0005
Batch 40000/105391 completed, running loss: 0.0005
Batch 50000/105391 completed, running loss: 0.0005
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0005
Batch 80000/105391 completed, running loss: 0.0007
Batch 90000/105391 completed, running loss: 0.0007
Batch 100000/105391 completed, running loss: 0.0007
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 31/50
Train Loss: 0.0007, Train Acc: 0.9999
Val Loss: 42.7948, Val Acc: 0.5794, ROC-AUC: 0.6066
Starting training epoch with 105391 batches
Batch 10000/105391 completed, running loss: 0.0005
Batch 20000/105391 completed, running loss: 0.0004
Batch 30000/105391 completed, running loss: 0.0004
Batch 40000/105391 completed, running loss: 0.0003
Batch 50000/105391 completed, running loss: 0.0004
Batch 60000/105391 completed, running loss: 0.0005
Batch 70000/105391 completed, running loss: 0.0005
Batch 80000/105391 completed, running loss: 0.0004
Batch 90000/105391 completed, running loss: 0.0004
Batch 100000/105391 completed, running loss: 0.0004
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Fold 2 - Epoch 32/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 55.5523, Val Acc: 0.5532, ROC-AUC: 0.5810
Fold 2 - Early stopping triggered
Starting evaluation with 105391 batches
Evaluation batch 10000/105391 completed
Evaluation batch 20000/105391 completed
Evaluation batch 30000/105391 completed
Evaluation batch 40000/105391 completed
Evaluation batch 50000/105391 completed
Evaluation batch 60000/105391 completed
Evaluation batch 70000/105391 completed
Evaluation batch 80000/105391 completed
Evaluation batch 90000/105391 completed
Evaluation batch 100000/105391 completed
Starting evaluation with 19689 batches
Evaluation batch 10000/19689 completed
Starting evaluation with 10839 batches
Evaluation batch 10000/10839 completed
Fold 2 - Test Metrics:
Loss: 3.5376
Accuracy: 0.6645
Precision: 0.8576
Recall: 0.5727
Roc_auc: 0.7154
Specificity: 0.8292
F1: 0.6868

Starting Fold 3/10
Fold 3 - Training label counts: {1: 223612, 0: 211079}
Fold 3 - Validation label counts: {1: 39118, 0: 30911}
Fold 3 - Test label counts: {1: 22012, 0: 16938}
Preparing dataset with 434691 samples
Extracting features
Normalizing embeddings
Dataset ready with 434691 samples
Preparing dataset with 70029 samples
Extracting features
Normalizing embeddings
Dataset ready with 70029 samples
Preparing dataset with 38950 samples
Extracting features
Normalizing embeddings
Dataset ready with 38950 samples
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0783
Batch 20000/108673 completed, running loss: 0.0569
Batch 30000/108673 completed, running loss: 0.0472
Batch 40000/108673 completed, running loss: 0.0412
Batch 50000/108673 completed, running loss: 0.0373
Batch 60000/108673 completed, running loss: 0.0344
Batch 70000/108673 completed, running loss: 0.0320
Batch 80000/108673 completed, running loss: 0.0304
Batch 90000/108673 completed, running loss: 0.0289
Batch 100000/108673 completed, running loss: 0.0275
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 1/50
Train Loss: 0.0265, Train Acc: 0.9898
Val Loss: 4.6908, Val Acc: 0.5878, ROC-AUC: 0.5825
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0117
Batch 20000/108673 completed, running loss: 0.0123
Batch 30000/108673 completed, running loss: 0.0119
Batch 40000/108673 completed, running loss: 0.0121
Batch 50000/108673 completed, running loss: 0.0119
Batch 60000/108673 completed, running loss: 0.0120
Batch 70000/108673 completed, running loss: 0.0119
Batch 80000/108673 completed, running loss: 0.0118
Batch 90000/108673 completed, running loss: 0.0117
Batch 100000/108673 completed, running loss: 0.0116
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 2/50
Train Loss: 0.0114, Train Acc: 0.9957
Val Loss: 6.7029, Val Acc: 0.5674, ROC-AUC: 0.5201
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0084
Batch 20000/108673 completed, running loss: 0.0083
Batch 30000/108673 completed, running loss: 0.0086
Batch 40000/108673 completed, running loss: 0.0085
Batch 50000/108673 completed, running loss: 0.0086
Batch 60000/108673 completed, running loss: 0.0085
Batch 70000/108673 completed, running loss: 0.0085
Batch 80000/108673 completed, running loss: 0.0084
Batch 90000/108673 completed, running loss: 0.0083
Batch 100000/108673 completed, running loss: 0.0084
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 3/50
Train Loss: 0.0084, Train Acc: 0.9970
Val Loss: 7.6844, Val Acc: 0.5880, ROC-AUC: 0.5559
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0053
Batch 20000/108673 completed, running loss: 0.0060
Batch 30000/108673 completed, running loss: 0.0062
Batch 40000/108673 completed, running loss: 0.0064
Batch 50000/108673 completed, running loss: 0.0064
Batch 60000/108673 completed, running loss: 0.0065
Batch 70000/108673 completed, running loss: 0.0065
Batch 80000/108673 completed, running loss: 0.0064
Batch 90000/108673 completed, running loss: 0.0065
Batch 100000/108673 completed, running loss: 0.0065
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 4/50
Train Loss: 0.0067, Train Acc: 0.9975
Val Loss: 8.5194, Val Acc: 0.5532, ROC-AUC: 0.4902
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0048
Batch 20000/108673 completed, running loss: 0.0055
Batch 30000/108673 completed, running loss: 0.0053
Batch 40000/108673 completed, running loss: 0.0054
Batch 50000/108673 completed, running loss: 0.0054
Batch 60000/108673 completed, running loss: 0.0054
Batch 70000/108673 completed, running loss: 0.0054
Batch 80000/108673 completed, running loss: 0.0053
Batch 90000/108673 completed, running loss: 0.0053
Batch 100000/108673 completed, running loss: 0.0054
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 5/50
Train Loss: 0.0054, Train Acc: 0.9980
Val Loss: 9.4326, Val Acc: 0.6049, ROC-AUC: 0.5643
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0046
Batch 20000/108673 completed, running loss: 0.0045
Batch 30000/108673 completed, running loss: 0.0044
Batch 40000/108673 completed, running loss: 0.0045
Batch 50000/108673 completed, running loss: 0.0045
Batch 60000/108673 completed, running loss: 0.0045
Batch 70000/108673 completed, running loss: 0.0045
Batch 80000/108673 completed, running loss: 0.0046
Batch 90000/108673 completed, running loss: 0.0047
Batch 100000/108673 completed, running loss: 0.0048
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 6/50
Train Loss: 0.0047, Train Acc: 0.9983
Val Loss: 10.9283, Val Acc: 0.5800, ROC-AUC: 0.5279
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0039
Batch 20000/108673 completed, running loss: 0.0039
Batch 30000/108673 completed, running loss: 0.0040
Batch 40000/108673 completed, running loss: 0.0040
Batch 50000/108673 completed, running loss: 0.0039
Batch 60000/108673 completed, running loss: 0.0040
Batch 70000/108673 completed, running loss: 0.0040
Batch 80000/108673 completed, running loss: 0.0039
Batch 90000/108673 completed, running loss: 0.0040
Batch 100000/108673 completed, running loss: 0.0040
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 7/50
Train Loss: 0.0041, Train Acc: 0.9985
Val Loss: 9.7486, Val Acc: 0.6185, ROC-AUC: 0.5835
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0033
Batch 20000/108673 completed, running loss: 0.0036
Batch 30000/108673 completed, running loss: 0.0032
Batch 40000/108673 completed, running loss: 0.0035
Batch 50000/108673 completed, running loss: 0.0035
Batch 60000/108673 completed, running loss: 0.0035
Batch 70000/108673 completed, running loss: 0.0035
Batch 80000/108673 completed, running loss: 0.0036
Batch 90000/108673 completed, running loss: 0.0037
Batch 100000/108673 completed, running loss: 0.0037
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 8/50
Train Loss: 0.0037, Train Acc: 0.9988
Val Loss: 12.9346, Val Acc: 0.5788, ROC-AUC: 0.5337
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0026
Batch 20000/108673 completed, running loss: 0.0026
Batch 30000/108673 completed, running loss: 0.0028
Batch 40000/108673 completed, running loss: 0.0028
Batch 50000/108673 completed, running loss: 0.0029
Batch 60000/108673 completed, running loss: 0.0029
Batch 70000/108673 completed, running loss: 0.0029
Batch 80000/108673 completed, running loss: 0.0029
Batch 90000/108673 completed, running loss: 0.0029
Batch 100000/108673 completed, running loss: 0.0030
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 9/50
Train Loss: 0.0030, Train Acc: 0.9989
Val Loss: 13.6542, Val Acc: 0.5821, ROC-AUC: 0.5013
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0018
Batch 20000/108673 completed, running loss: 0.0024
Batch 30000/108673 completed, running loss: 0.0026
Batch 40000/108673 completed, running loss: 0.0027
Batch 50000/108673 completed, running loss: 0.0026
Batch 60000/108673 completed, running loss: 0.0027
Batch 70000/108673 completed, running loss: 0.0027
Batch 80000/108673 completed, running loss: 0.0028
Batch 90000/108673 completed, running loss: 0.0029
Batch 100000/108673 completed, running loss: 0.0028
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 10/50
Train Loss: 0.0028, Train Acc: 0.9990
Val Loss: 12.6223, Val Acc: 0.5614, ROC-AUC: 0.4952
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0025
Batch 20000/108673 completed, running loss: 0.0026
Batch 30000/108673 completed, running loss: 0.0026
Batch 40000/108673 completed, running loss: 0.0025
Batch 50000/108673 completed, running loss: 0.0026
Batch 60000/108673 completed, running loss: 0.0026
Batch 70000/108673 completed, running loss: 0.0025
Batch 80000/108673 completed, running loss: 0.0025
Batch 90000/108673 completed, running loss: 0.0026
Batch 100000/108673 completed, running loss: 0.0025
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 11/50
Train Loss: 0.0025, Train Acc: 0.9992
Val Loss: 17.6595, Val Acc: 0.5906, ROC-AUC: 0.5080
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0023
Batch 20000/108673 completed, running loss: 0.0021
Batch 30000/108673 completed, running loss: 0.0023
Batch 40000/108673 completed, running loss: 0.0024
Batch 50000/108673 completed, running loss: 0.0024
Batch 60000/108673 completed, running loss: 0.0024
Batch 70000/108673 completed, running loss: 0.0023
Batch 80000/108673 completed, running loss: 0.0024
Batch 90000/108673 completed, running loss: 0.0024
Batch 100000/108673 completed, running loss: 0.0025
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 12/50
Train Loss: 0.0024, Train Acc: 0.9992
Val Loss: 17.3081, Val Acc: 0.5984, ROC-AUC: 0.5266
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0015
Batch 20000/108673 completed, running loss: 0.0014
Batch 30000/108673 completed, running loss: 0.0014
Batch 40000/108673 completed, running loss: 0.0016
Batch 50000/108673 completed, running loss: 0.0018
Batch 60000/108673 completed, running loss: 0.0018
Batch 70000/108673 completed, running loss: 0.0019
Batch 80000/108673 completed, running loss: 0.0020
Batch 90000/108673 completed, running loss: 0.0021
Batch 100000/108673 completed, running loss: 0.0022
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 13/50
Train Loss: 0.0022, Train Acc: 0.9993
Val Loss: 13.1354, Val Acc: 0.5946, ROC-AUC: 0.5505
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0017
Batch 20000/108673 completed, running loss: 0.0019
Batch 30000/108673 completed, running loss: 0.0018
Batch 40000/108673 completed, running loss: 0.0019
Batch 50000/108673 completed, running loss: 0.0020
Batch 60000/108673 completed, running loss: 0.0019
Batch 70000/108673 completed, running loss: 0.0020
Batch 80000/108673 completed, running loss: 0.0020
Batch 90000/108673 completed, running loss: 0.0019
Batch 100000/108673 completed, running loss: 0.0019
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 14/50
Train Loss: 0.0020, Train Acc: 0.9994
Val Loss: 15.7995, Val Acc: 0.5914, ROC-AUC: 0.5435
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0015
Batch 20000/108673 completed, running loss: 0.0015
Batch 30000/108673 completed, running loss: 0.0016
Batch 40000/108673 completed, running loss: 0.0016
Batch 50000/108673 completed, running loss: 0.0016
Batch 60000/108673 completed, running loss: 0.0017
Batch 70000/108673 completed, running loss: 0.0018
Batch 80000/108673 completed, running loss: 0.0018
Batch 90000/108673 completed, running loss: 0.0019
Batch 100000/108673 completed, running loss: 0.0019
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 15/50
Train Loss: 0.0019, Train Acc: 0.9995
Val Loss: 19.9156, Val Acc: 0.5764, ROC-AUC: 0.5002
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0018
Batch 20000/108673 completed, running loss: 0.0015
Batch 30000/108673 completed, running loss: 0.0013
Batch 40000/108673 completed, running loss: 0.0013
Batch 50000/108673 completed, running loss: 0.0013
Batch 60000/108673 completed, running loss: 0.0015
Batch 70000/108673 completed, running loss: 0.0015
Batch 80000/108673 completed, running loss: 0.0015
Batch 90000/108673 completed, running loss: 0.0016
Batch 100000/108673 completed, running loss: 0.0016
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 16/50
Train Loss: 0.0016, Train Acc: 0.9995
Val Loss: 19.4290, Val Acc: 0.5742, ROC-AUC: 0.5139
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0014
Batch 20000/108673 completed, running loss: 0.0015
Batch 30000/108673 completed, running loss: 0.0016
Batch 40000/108673 completed, running loss: 0.0014
Batch 50000/108673 completed, running loss: 0.0015
Batch 60000/108673 completed, running loss: 0.0015
Batch 70000/108673 completed, running loss: 0.0015
Batch 80000/108673 completed, running loss: 0.0015
Batch 90000/108673 completed, running loss: 0.0016
Batch 100000/108673 completed, running loss: 0.0016
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 17/50
Train Loss: 0.0016, Train Acc: 0.9995
Val Loss: 20.4776, Val Acc: 0.5927, ROC-AUC: 0.5257
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0009
Batch 20000/108673 completed, running loss: 0.0012
Batch 30000/108673 completed, running loss: 0.0014
Batch 40000/108673 completed, running loss: 0.0014
Batch 50000/108673 completed, running loss: 0.0014
Batch 60000/108673 completed, running loss: 0.0015
Batch 70000/108673 completed, running loss: 0.0016
Batch 80000/108673 completed, running loss: 0.0017
Batch 90000/108673 completed, running loss: 0.0016
Batch 100000/108673 completed, running loss: 0.0015
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 18/50
Train Loss: 0.0016, Train Acc: 0.9996
Val Loss: 19.2179, Val Acc: 0.5869, ROC-AUC: 0.5291
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0011
Batch 20000/108673 completed, running loss: 0.0016
Batch 30000/108673 completed, running loss: 0.0013
Batch 40000/108673 completed, running loss: 0.0013
Batch 50000/108673 completed, running loss: 0.0012
Batch 60000/108673 completed, running loss: 0.0013
Batch 70000/108673 completed, running loss: 0.0013
Batch 80000/108673 completed, running loss: 0.0013
Batch 90000/108673 completed, running loss: 0.0013
Batch 100000/108673 completed, running loss: 0.0014
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 19/50
Train Loss: 0.0015, Train Acc: 0.9996
Val Loss: 20.2187, Val Acc: 0.5890, ROC-AUC: 0.5219
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0015
Batch 20000/108673 completed, running loss: 0.0017
Batch 30000/108673 completed, running loss: 0.0015
Batch 40000/108673 completed, running loss: 0.0015
Batch 50000/108673 completed, running loss: 0.0015
Batch 60000/108673 completed, running loss: 0.0015
Batch 70000/108673 completed, running loss: 0.0015
Batch 80000/108673 completed, running loss: 0.0014
Batch 90000/108673 completed, running loss: 0.0015
Batch 100000/108673 completed, running loss: 0.0015
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 20/50
Train Loss: 0.0015, Train Acc: 0.9996
Val Loss: 24.8633, Val Acc: 0.5964, ROC-AUC: 0.5364
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0012
Batch 20000/108673 completed, running loss: 0.0009
Batch 30000/108673 completed, running loss: 0.0011
Batch 40000/108673 completed, running loss: 0.0010
Batch 50000/108673 completed, running loss: 0.0012
Batch 60000/108673 completed, running loss: 0.0013
Batch 70000/108673 completed, running loss: 0.0012
Batch 80000/108673 completed, running loss: 0.0013
Batch 90000/108673 completed, running loss: 0.0012
Batch 100000/108673 completed, running loss: 0.0012
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 21/50
Train Loss: 0.0013, Train Acc: 0.9996
Val Loss: 28.5546, Val Acc: 0.5731, ROC-AUC: 0.4931
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0008
Batch 20000/108673 completed, running loss: 0.0009
Batch 30000/108673 completed, running loss: 0.0014
Batch 40000/108673 completed, running loss: 0.0014
Batch 50000/108673 completed, running loss: 0.0013
Batch 60000/108673 completed, running loss: 0.0013
Batch 70000/108673 completed, running loss: 0.0014
Batch 80000/108673 completed, running loss: 0.0014
Batch 90000/108673 completed, running loss: 0.0013
Batch 100000/108673 completed, running loss: 0.0013
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 22/50
Train Loss: 0.0014, Train Acc: 0.9996
Val Loss: 29.7498, Val Acc: 0.5705, ROC-AUC: 0.4875
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0005
Batch 20000/108673 completed, running loss: 0.0011
Batch 30000/108673 completed, running loss: 0.0013
Batch 40000/108673 completed, running loss: 0.0012
Batch 50000/108673 completed, running loss: 0.0012
Batch 60000/108673 completed, running loss: 0.0011
Batch 70000/108673 completed, running loss: 0.0013
Batch 80000/108673 completed, running loss: 0.0012
Batch 90000/108673 completed, running loss: 0.0012
Batch 100000/108673 completed, running loss: 0.0013
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 23/50
Train Loss: 0.0013, Train Acc: 0.9997
Val Loss: 27.3587, Val Acc: 0.5659, ROC-AUC: 0.4756
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0010
Batch 20000/108673 completed, running loss: 0.0010
Batch 30000/108673 completed, running loss: 0.0008
Batch 40000/108673 completed, running loss: 0.0007
Batch 50000/108673 completed, running loss: 0.0007
Batch 60000/108673 completed, running loss: 0.0008
Batch 70000/108673 completed, running loss: 0.0007
Batch 80000/108673 completed, running loss: 0.0007
Batch 90000/108673 completed, running loss: 0.0007
Batch 100000/108673 completed, running loss: 0.0006
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 24/50
Train Loss: 0.0006, Train Acc: 0.9998
Val Loss: 39.7801, Val Acc: 0.5738, ROC-AUC: 0.4896
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0002
Batch 20000/108673 completed, running loss: 0.0001
Batch 30000/108673 completed, running loss: 0.0003
Batch 40000/108673 completed, running loss: 0.0003
Batch 50000/108673 completed, running loss: 0.0004
Batch 60000/108673 completed, running loss: 0.0005
Batch 70000/108673 completed, running loss: 0.0005
Batch 80000/108673 completed, running loss: 0.0005
Batch 90000/108673 completed, running loss: 0.0004
Batch 100000/108673 completed, running loss: 0.0004
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 25/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 55.4298, Val Acc: 0.5664, ROC-AUC: 0.4745
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0002
Batch 20000/108673 completed, running loss: 0.0004
Batch 30000/108673 completed, running loss: 0.0003
Batch 40000/108673 completed, running loss: 0.0004
Batch 50000/108673 completed, running loss: 0.0003
Batch 60000/108673 completed, running loss: 0.0003
Batch 70000/108673 completed, running loss: 0.0004
Batch 80000/108673 completed, running loss: 0.0004
Batch 90000/108673 completed, running loss: 0.0005
Batch 100000/108673 completed, running loss: 0.0005
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 26/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 49.3376, Val Acc: 0.5630, ROC-AUC: 0.4850
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0004
Batch 20000/108673 completed, running loss: 0.0002
Batch 30000/108673 completed, running loss: 0.0002
Batch 40000/108673 completed, running loss: 0.0002
Batch 50000/108673 completed, running loss: 0.0003
Batch 60000/108673 completed, running loss: 0.0003
Batch 70000/108673 completed, running loss: 0.0003
Batch 80000/108673 completed, running loss: 0.0003
Batch 90000/108673 completed, running loss: 0.0003
Batch 100000/108673 completed, running loss: 0.0003
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 27/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 58.5366, Val Acc: 0.5756, ROC-AUC: 0.4900
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0004
Batch 20000/108673 completed, running loss: 0.0002
Batch 30000/108673 completed, running loss: 0.0002
Batch 40000/108673 completed, running loss: 0.0002
Batch 50000/108673 completed, running loss: 0.0002
Batch 60000/108673 completed, running loss: 0.0002
Batch 70000/108673 completed, running loss: 0.0001
Batch 80000/108673 completed, running loss: 0.0002
Batch 90000/108673 completed, running loss: 0.0002
Batch 100000/108673 completed, running loss: 0.0002
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 28/50
Train Loss: 0.0002, Train Acc: 0.9999
Val Loss: 60.8284, Val Acc: 0.5735, ROC-AUC: 0.5109
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0001
Batch 20000/108673 completed, running loss: 0.0001
Batch 30000/108673 completed, running loss: 0.0002
Batch 40000/108673 completed, running loss: 0.0002
Batch 50000/108673 completed, running loss: 0.0002
Batch 60000/108673 completed, running loss: 0.0002
Batch 70000/108673 completed, running loss: 0.0002
Batch 80000/108673 completed, running loss: 0.0003
Batch 90000/108673 completed, running loss: 0.0003
Batch 100000/108673 completed, running loss: 0.0003
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 29/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 68.9023, Val Acc: 0.5688, ROC-AUC: 0.4972
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0003
Batch 20000/108673 completed, running loss: 0.0004
Batch 30000/108673 completed, running loss: 0.0003
Batch 40000/108673 completed, running loss: 0.0003
Batch 50000/108673 completed, running loss: 0.0002
Batch 60000/108673 completed, running loss: 0.0002
Batch 70000/108673 completed, running loss: 0.0002
Batch 80000/108673 completed, running loss: 0.0003
Batch 90000/108673 completed, running loss: 0.0003
Batch 100000/108673 completed, running loss: 0.0003
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 30/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 76.2317, Val Acc: 0.5721, ROC-AUC: 0.5022
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0001
Batch 20000/108673 completed, running loss: 0.0002
Batch 30000/108673 completed, running loss: 0.0004
Batch 40000/108673 completed, running loss: 0.0004
Batch 50000/108673 completed, running loss: 0.0004
Batch 60000/108673 completed, running loss: 0.0004
Batch 70000/108673 completed, running loss: 0.0005
Batch 80000/108673 completed, running loss: 0.0005
Batch 90000/108673 completed, running loss: 0.0005
Batch 100000/108673 completed, running loss: 0.0005
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 31/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 81.6769, Val Acc: 0.5660, ROC-AUC: 0.4974
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0002
Batch 20000/108673 completed, running loss: 0.0003
Batch 30000/108673 completed, running loss: 0.0003
Batch 40000/108673 completed, running loss: 0.0003
Batch 50000/108673 completed, running loss: 0.0004
Batch 60000/108673 completed, running loss: 0.0005
Batch 70000/108673 completed, running loss: 0.0004
Batch 80000/108673 completed, running loss: 0.0005
Batch 90000/108673 completed, running loss: 0.0006
Batch 100000/108673 completed, running loss: 0.0005
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 32/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 99.0612, Val Acc: 0.5777, ROC-AUC: 0.5066
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0003
Batch 20000/108673 completed, running loss: 0.0004
Batch 30000/108673 completed, running loss: 0.0004
Batch 40000/108673 completed, running loss: 0.0005
Batch 50000/108673 completed, running loss: 0.0004
Batch 60000/108673 completed, running loss: 0.0004
Batch 70000/108673 completed, running loss: 0.0005
Batch 80000/108673 completed, running loss: 0.0005
Batch 90000/108673 completed, running loss: 0.0005
Batch 100000/108673 completed, running loss: 0.0005
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 33/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 97.3207, Val Acc: 0.5688, ROC-AUC: 0.5029
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0002
Batch 20000/108673 completed, running loss: 0.0007
Batch 30000/108673 completed, running loss: 0.0006
Batch 40000/108673 completed, running loss: 0.0006
Batch 50000/108673 completed, running loss: 0.0006
Batch 60000/108673 completed, running loss: 0.0005
Batch 70000/108673 completed, running loss: 0.0006
Batch 80000/108673 completed, running loss: 0.0006
Batch 90000/108673 completed, running loss: 0.0006
Batch 100000/108673 completed, running loss: 0.0006
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 34/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 94.3320, Val Acc: 0.5671, ROC-AUC: 0.5074
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0004
Batch 20000/108673 completed, running loss: 0.0004
Batch 30000/108673 completed, running loss: 0.0004
Batch 40000/108673 completed, running loss: 0.0006
Batch 50000/108673 completed, running loss: 0.0006
Batch 60000/108673 completed, running loss: 0.0006
Batch 70000/108673 completed, running loss: 0.0006
Batch 80000/108673 completed, running loss: 0.0006
Batch 90000/108673 completed, running loss: 0.0006
Batch 100000/108673 completed, running loss: 0.0005
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 35/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 104.5458, Val Acc: 0.5675, ROC-AUC: 0.5078
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0000
Batch 20000/108673 completed, running loss: 0.0001
Batch 30000/108673 completed, running loss: 0.0003
Batch 40000/108673 completed, running loss: 0.0004
Batch 50000/108673 completed, running loss: 0.0003
Batch 60000/108673 completed, running loss: 0.0003
Batch 70000/108673 completed, running loss: 0.0004
Batch 80000/108673 completed, running loss: 0.0003
Batch 90000/108673 completed, running loss: 0.0004
Batch 100000/108673 completed, running loss: 0.0004
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 36/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 100.8517, Val Acc: 0.5685, ROC-AUC: 0.5125
Starting training epoch with 108673 batches
Batch 10000/108673 completed, running loss: 0.0001
Batch 20000/108673 completed, running loss: 0.0001
Batch 30000/108673 completed, running loss: 0.0001
Batch 40000/108673 completed, running loss: 0.0003
Batch 50000/108673 completed, running loss: 0.0004
Batch 60000/108673 completed, running loss: 0.0003
Batch 70000/108673 completed, running loss: 0.0003
Batch 80000/108673 completed, running loss: 0.0003
Batch 90000/108673 completed, running loss: 0.0004
Batch 100000/108673 completed, running loss: 0.0004
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Fold 3 - Epoch 37/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 109.9065, Val Acc: 0.5701, ROC-AUC: 0.5143
Fold 3 - Early stopping triggered
Starting evaluation with 108673 batches
Evaluation batch 10000/108673 completed
Evaluation batch 20000/108673 completed
Evaluation batch 30000/108673 completed
Evaluation batch 40000/108673 completed
Evaluation batch 50000/108673 completed
Evaluation batch 60000/108673 completed
Evaluation batch 70000/108673 completed
Evaluation batch 80000/108673 completed
Evaluation batch 90000/108673 completed
Evaluation batch 100000/108673 completed
Starting evaluation with 17508 batches
Evaluation batch 10000/17508 completed
Starting evaluation with 9738 batches
Fold 3 - Test Metrics:
Loss: 5.9041
Accuracy: 0.6107
Precision: 0.6483
Recall: 0.6800
Roc_auc: 0.6989
Specificity: 0.5205
F1: 0.6638

Starting Fold 4/10
Fold 4 - Training label counts: {1: 214906, 0: 192717}
Fold 4 - Validation label counts: {1: 35173, 0: 30702}
Fold 4 - Test label counts: {0: 35509, 1: 34663}
Preparing dataset with 407623 samples
Extracting features
Normalizing embeddings
Dataset ready with 407623 samples
Preparing dataset with 65875 samples
Extracting features
Normalizing embeddings
Dataset ready with 65875 samples
Preparing dataset with 70172 samples
Extracting features
Normalizing embeddings
Dataset ready with 70172 samples
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0801
Batch 20000/101906 completed, running loss: 0.0584
Batch 30000/101906 completed, running loss: 0.0487
Batch 40000/101906 completed, running loss: 0.0430
Batch 50000/101906 completed, running loss: 0.0393
Batch 60000/101906 completed, running loss: 0.0362
Batch 70000/101906 completed, running loss: 0.0340
Batch 80000/101906 completed, running loss: 0.0322
Batch 90000/101906 completed, running loss: 0.0306
Batch 100000/101906 completed, running loss: 0.0292
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 1/50
Train Loss: 0.0290, Train Acc: 0.9890
Val Loss: 3.2913, Val Acc: 0.5400, ROC-AUC: 0.5726
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0134
Batch 20000/101906 completed, running loss: 0.0134
Batch 30000/101906 completed, running loss: 0.0135
Batch 40000/101906 completed, running loss: 0.0134
Batch 50000/101906 completed, running loss: 0.0132
Batch 60000/101906 completed, running loss: 0.0131
Batch 70000/101906 completed, running loss: 0.0130
Batch 80000/101906 completed, running loss: 0.0129
Batch 90000/101906 completed, running loss: 0.0127
Batch 100000/101906 completed, running loss: 0.0126
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 2/50
Train Loss: 0.0126, Train Acc: 0.9954
Val Loss: 3.6663, Val Acc: 0.5787, ROC-AUC: 0.6338
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0090
Batch 20000/101906 completed, running loss: 0.0090
Batch 30000/101906 completed, running loss: 0.0087
Batch 40000/101906 completed, running loss: 0.0088
Batch 50000/101906 completed, running loss: 0.0090
Batch 60000/101906 completed, running loss: 0.0089
Batch 70000/101906 completed, running loss: 0.0089
Batch 80000/101906 completed, running loss: 0.0088
Batch 90000/101906 completed, running loss: 0.0089
Batch 100000/101906 completed, running loss: 0.0088
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 3/50
Train Loss: 0.0088, Train Acc: 0.9968
Val Loss: 4.0531, Val Acc: 0.5625, ROC-AUC: 0.6068
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0073
Batch 20000/101906 completed, running loss: 0.0072
Batch 30000/101906 completed, running loss: 0.0070
Batch 40000/101906 completed, running loss: 0.0069
Batch 50000/101906 completed, running loss: 0.0069
Batch 60000/101906 completed, running loss: 0.0070
Batch 70000/101906 completed, running loss: 0.0071
Batch 80000/101906 completed, running loss: 0.0071
Batch 90000/101906 completed, running loss: 0.0071
Batch 100000/101906 completed, running loss: 0.0072
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 4/50
Train Loss: 0.0072, Train Acc: 0.9973
Val Loss: 3.9621, Val Acc: 0.6056, ROC-AUC: 0.6453
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0060
Batch 20000/101906 completed, running loss: 0.0058
Batch 30000/101906 completed, running loss: 0.0061
Batch 40000/101906 completed, running loss: 0.0060
Batch 50000/101906 completed, running loss: 0.0059
Batch 60000/101906 completed, running loss: 0.0059
Batch 70000/101906 completed, running loss: 0.0059
Batch 80000/101906 completed, running loss: 0.0059
Batch 90000/101906 completed, running loss: 0.0059
Batch 100000/101906 completed, running loss: 0.0058
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 5/50
Train Loss: 0.0058, Train Acc: 0.9978
Val Loss: 6.2743, Val Acc: 0.5545, ROC-AUC: 0.5872
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0041
Batch 20000/101906 completed, running loss: 0.0049
Batch 30000/101906 completed, running loss: 0.0047
Batch 40000/101906 completed, running loss: 0.0046
Batch 50000/101906 completed, running loss: 0.0048
Batch 60000/101906 completed, running loss: 0.0049
Batch 70000/101906 completed, running loss: 0.0049
Batch 80000/101906 completed, running loss: 0.0048
Batch 90000/101906 completed, running loss: 0.0048
Batch 100000/101906 completed, running loss: 0.0048
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 6/50
Train Loss: 0.0048, Train Acc: 0.9983
Val Loss: 6.3299, Val Acc: 0.5566, ROC-AUC: 0.6038
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0033
Batch 20000/101906 completed, running loss: 0.0041
Batch 30000/101906 completed, running loss: 0.0040
Batch 40000/101906 completed, running loss: 0.0040
Batch 50000/101906 completed, running loss: 0.0039
Batch 60000/101906 completed, running loss: 0.0042
Batch 70000/101906 completed, running loss: 0.0042
Batch 80000/101906 completed, running loss: 0.0042
Batch 90000/101906 completed, running loss: 0.0043
Batch 100000/101906 completed, running loss: 0.0043
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 7/50
Train Loss: 0.0043, Train Acc: 0.9985
Val Loss: 5.8928, Val Acc: 0.5542, ROC-AUC: 0.5992
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0034
Batch 20000/101906 completed, running loss: 0.0036
Batch 30000/101906 completed, running loss: 0.0037
Batch 40000/101906 completed, running loss: 0.0038
Batch 50000/101906 completed, running loss: 0.0038
Batch 60000/101906 completed, running loss: 0.0038
Batch 70000/101906 completed, running loss: 0.0037
Batch 80000/101906 completed, running loss: 0.0037
Batch 90000/101906 completed, running loss: 0.0037
Batch 100000/101906 completed, running loss: 0.0037
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 8/50
Train Loss: 0.0037, Train Acc: 0.9987
Val Loss: 5.4570, Val Acc: 0.5930, ROC-AUC: 0.6409
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0030
Batch 20000/101906 completed, running loss: 0.0032
Batch 30000/101906 completed, running loss: 0.0033
Batch 40000/101906 completed, running loss: 0.0032
Batch 50000/101906 completed, running loss: 0.0032
Batch 60000/101906 completed, running loss: 0.0032
Batch 70000/101906 completed, running loss: 0.0031
Batch 80000/101906 completed, running loss: 0.0033
Batch 90000/101906 completed, running loss: 0.0033
Batch 100000/101906 completed, running loss: 0.0033
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 9/50
Train Loss: 0.0033, Train Acc: 0.9989
Val Loss: 5.9801, Val Acc: 0.5962, ROC-AUC: 0.6414
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0017
Batch 20000/101906 completed, running loss: 0.0016
Batch 30000/101906 completed, running loss: 0.0022
Batch 40000/101906 completed, running loss: 0.0024
Batch 50000/101906 completed, running loss: 0.0025
Batch 60000/101906 completed, running loss: 0.0028
Batch 70000/101906 completed, running loss: 0.0029
Batch 80000/101906 completed, running loss: 0.0028
Batch 90000/101906 completed, running loss: 0.0028
Batch 100000/101906 completed, running loss: 0.0029
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 10/50
Train Loss: 0.0029, Train Acc: 0.9991
Val Loss: 6.4751, Val Acc: 0.5758, ROC-AUC: 0.6231
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0015
Batch 20000/101906 completed, running loss: 0.0020
Batch 30000/101906 completed, running loss: 0.0020
Batch 40000/101906 completed, running loss: 0.0021
Batch 50000/101906 completed, running loss: 0.0023
Batch 60000/101906 completed, running loss: 0.0023
Batch 70000/101906 completed, running loss: 0.0023
Batch 80000/101906 completed, running loss: 0.0024
Batch 90000/101906 completed, running loss: 0.0024
Batch 100000/101906 completed, running loss: 0.0024
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 11/50
Train Loss: 0.0024, Train Acc: 0.9992
Val Loss: 6.5490, Val Acc: 0.5923, ROC-AUC: 0.6482
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0019
Batch 20000/101906 completed, running loss: 0.0019
Batch 30000/101906 completed, running loss: 0.0023
Batch 40000/101906 completed, running loss: 0.0023
Batch 50000/101906 completed, running loss: 0.0023
Batch 60000/101906 completed, running loss: 0.0023
Batch 70000/101906 completed, running loss: 0.0023
Batch 80000/101906 completed, running loss: 0.0023
Batch 90000/101906 completed, running loss: 0.0023
Batch 100000/101906 completed, running loss: 0.0023
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 12/50
Train Loss: 0.0023, Train Acc: 0.9993
Val Loss: 6.8604, Val Acc: 0.5995, ROC-AUC: 0.6368
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0018
Batch 20000/101906 completed, running loss: 0.0017
Batch 30000/101906 completed, running loss: 0.0019
Batch 40000/101906 completed, running loss: 0.0018
Batch 50000/101906 completed, running loss: 0.0018
Batch 60000/101906 completed, running loss: 0.0017
Batch 70000/101906 completed, running loss: 0.0018
Batch 80000/101906 completed, running loss: 0.0019
Batch 90000/101906 completed, running loss: 0.0019
Batch 100000/101906 completed, running loss: 0.0019
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 13/50
Train Loss: 0.0019, Train Acc: 0.9993
Val Loss: 8.5931, Val Acc: 0.5801, ROC-AUC: 0.6126
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0018
Batch 20000/101906 completed, running loss: 0.0018
Batch 30000/101906 completed, running loss: 0.0018
Batch 40000/101906 completed, running loss: 0.0019
Batch 50000/101906 completed, running loss: 0.0021
Batch 60000/101906 completed, running loss: 0.0021
Batch 70000/101906 completed, running loss: 0.0021
Batch 80000/101906 completed, running loss: 0.0022
Batch 90000/101906 completed, running loss: 0.0022
Batch 100000/101906 completed, running loss: 0.0021
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 14/50
Train Loss: 0.0021, Train Acc: 0.9993
Val Loss: 8.4371, Val Acc: 0.6054, ROC-AUC: 0.6331
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0021
Batch 20000/101906 completed, running loss: 0.0016
Batch 30000/101906 completed, running loss: 0.0016
Batch 40000/101906 completed, running loss: 0.0018
Batch 50000/101906 completed, running loss: 0.0020
Batch 60000/101906 completed, running loss: 0.0019
Batch 70000/101906 completed, running loss: 0.0019
Batch 80000/101906 completed, running loss: 0.0020
Batch 90000/101906 completed, running loss: 0.0019
Batch 100000/101906 completed, running loss: 0.0019
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 15/50
Train Loss: 0.0019, Train Acc: 0.9994
Val Loss: 8.7190, Val Acc: 0.5903, ROC-AUC: 0.6269
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0013
Batch 20000/101906 completed, running loss: 0.0013
Batch 30000/101906 completed, running loss: 0.0013
Batch 40000/101906 completed, running loss: 0.0014
Batch 50000/101906 completed, running loss: 0.0015
Batch 60000/101906 completed, running loss: 0.0014
Batch 70000/101906 completed, running loss: 0.0015
Batch 80000/101906 completed, running loss: 0.0016
Batch 90000/101906 completed, running loss: 0.0016
Batch 100000/101906 completed, running loss: 0.0016
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 16/50
Train Loss: 0.0016, Train Acc: 0.9995
Val Loss: 9.6275, Val Acc: 0.5553, ROC-AUC: 0.5953
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0009
Batch 20000/101906 completed, running loss: 0.0018
Batch 30000/101906 completed, running loss: 0.0017
Batch 40000/101906 completed, running loss: 0.0016
Batch 50000/101906 completed, running loss: 0.0016
Batch 60000/101906 completed, running loss: 0.0015
Batch 70000/101906 completed, running loss: 0.0015
Batch 80000/101906 completed, running loss: 0.0014
Batch 90000/101906 completed, running loss: 0.0015
Batch 100000/101906 completed, running loss: 0.0015
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 17/50
Train Loss: 0.0015, Train Acc: 0.9995
Val Loss: 9.6005, Val Acc: 0.5693, ROC-AUC: 0.6120
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0013
Batch 20000/101906 completed, running loss: 0.0014
Batch 30000/101906 completed, running loss: 0.0013
Batch 40000/101906 completed, running loss: 0.0012
Batch 50000/101906 completed, running loss: 0.0012
Batch 60000/101906 completed, running loss: 0.0011
Batch 70000/101906 completed, running loss: 0.0012
Batch 80000/101906 completed, running loss: 0.0013
Batch 90000/101906 completed, running loss: 0.0013
Batch 100000/101906 completed, running loss: 0.0014
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 18/50
Train Loss: 0.0014, Train Acc: 0.9996
Val Loss: 9.5364, Val Acc: 0.5960, ROC-AUC: 0.6219
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0017
Batch 20000/101906 completed, running loss: 0.0012
Batch 30000/101906 completed, running loss: 0.0014
Batch 40000/101906 completed, running loss: 0.0013
Batch 50000/101906 completed, running loss: 0.0014
Batch 60000/101906 completed, running loss: 0.0014
Batch 70000/101906 completed, running loss: 0.0014
Batch 80000/101906 completed, running loss: 0.0014
Batch 90000/101906 completed, running loss: 0.0015
Batch 100000/101906 completed, running loss: 0.0015
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 19/50
Train Loss: 0.0015, Train Acc: 0.9996
Val Loss: 9.1107, Val Acc: 0.6044, ROC-AUC: 0.6364
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0014
Batch 20000/101906 completed, running loss: 0.0012
Batch 30000/101906 completed, running loss: 0.0011
Batch 40000/101906 completed, running loss: 0.0011
Batch 50000/101906 completed, running loss: 0.0012
Batch 60000/101906 completed, running loss: 0.0012
Batch 70000/101906 completed, running loss: 0.0012
Batch 80000/101906 completed, running loss: 0.0012
Batch 90000/101906 completed, running loss: 0.0012
Batch 100000/101906 completed, running loss: 0.0012
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 20/50
Train Loss: 0.0012, Train Acc: 0.9996
Val Loss: 13.8912, Val Acc: 0.5461, ROC-AUC: 0.5647
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0013
Batch 20000/101906 completed, running loss: 0.0011
Batch 30000/101906 completed, running loss: 0.0011
Batch 40000/101906 completed, running loss: 0.0012
Batch 50000/101906 completed, running loss: 0.0012
Batch 60000/101906 completed, running loss: 0.0013
Batch 70000/101906 completed, running loss: 0.0012
Batch 80000/101906 completed, running loss: 0.0013
Batch 90000/101906 completed, running loss: 0.0013
Batch 100000/101906 completed, running loss: 0.0013
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 21/50
Train Loss: 0.0013, Train Acc: 0.9997
Val Loss: 15.0018, Val Acc: 0.5287, ROC-AUC: 0.5517
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0011
Batch 20000/101906 completed, running loss: 0.0012
Batch 30000/101906 completed, running loss: 0.0012
Batch 40000/101906 completed, running loss: 0.0012
Batch 50000/101906 completed, running loss: 0.0011
Batch 60000/101906 completed, running loss: 0.0010
Batch 70000/101906 completed, running loss: 0.0011
Batch 80000/101906 completed, running loss: 0.0012
Batch 90000/101906 completed, running loss: 0.0012
Batch 100000/101906 completed, running loss: 0.0012
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 22/50
Train Loss: 0.0012, Train Acc: 0.9996
Val Loss: 11.8480, Val Acc: 0.5626, ROC-AUC: 0.5969
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0008
Batch 20000/101906 completed, running loss: 0.0006
Batch 30000/101906 completed, running loss: 0.0008
Batch 40000/101906 completed, running loss: 0.0010
Batch 50000/101906 completed, running loss: 0.0008
Batch 60000/101906 completed, running loss: 0.0010
Batch 70000/101906 completed, running loss: 0.0012
Batch 80000/101906 completed, running loss: 0.0011
Batch 90000/101906 completed, running loss: 0.0012
Batch 100000/101906 completed, running loss: 0.0012
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 23/50
Train Loss: 0.0012, Train Acc: 0.9997
Val Loss: 11.9623, Val Acc: 0.5687, ROC-AUC: 0.5909
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0009
Batch 20000/101906 completed, running loss: 0.0008
Batch 30000/101906 completed, running loss: 0.0007
Batch 40000/101906 completed, running loss: 0.0008
Batch 50000/101906 completed, running loss: 0.0009
Batch 60000/101906 completed, running loss: 0.0009
Batch 70000/101906 completed, running loss: 0.0009
Batch 80000/101906 completed, running loss: 0.0009
Batch 90000/101906 completed, running loss: 0.0009
Batch 100000/101906 completed, running loss: 0.0009
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 24/50
Train Loss: 0.0009, Train Acc: 0.9997
Val Loss: 17.2478, Val Acc: 0.5563, ROC-AUC: 0.5909
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0012
Batch 20000/101906 completed, running loss: 0.0010
Batch 30000/101906 completed, running loss: 0.0009
Batch 40000/101906 completed, running loss: 0.0010
Batch 50000/101906 completed, running loss: 0.0010
Batch 60000/101906 completed, running loss: 0.0011
Batch 70000/101906 completed, running loss: 0.0011
Batch 80000/101906 completed, running loss: 0.0010
Batch 90000/101906 completed, running loss: 0.0010
Batch 100000/101906 completed, running loss: 0.0010
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 25/50
Train Loss: 0.0010, Train Acc: 0.9997
Val Loss: 16.6376, Val Acc: 0.5910, ROC-AUC: 0.6154
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0009
Batch 20000/101906 completed, running loss: 0.0011
Batch 30000/101906 completed, running loss: 0.0011
Batch 40000/101906 completed, running loss: 0.0010
Batch 50000/101906 completed, running loss: 0.0010
Batch 60000/101906 completed, running loss: 0.0010
Batch 70000/101906 completed, running loss: 0.0009
Batch 80000/101906 completed, running loss: 0.0010
Batch 90000/101906 completed, running loss: 0.0010
Batch 100000/101906 completed, running loss: 0.0010
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 26/50
Train Loss: 0.0010, Train Acc: 0.9997
Val Loss: 15.4866, Val Acc: 0.5535, ROC-AUC: 0.5855
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0008
Batch 20000/101906 completed, running loss: 0.0007
Batch 30000/101906 completed, running loss: 0.0008
Batch 40000/101906 completed, running loss: 0.0009
Batch 50000/101906 completed, running loss: 0.0009
Batch 60000/101906 completed, running loss: 0.0009
Batch 70000/101906 completed, running loss: 0.0009
Batch 80000/101906 completed, running loss: 0.0009
Batch 90000/101906 completed, running loss: 0.0009
Batch 100000/101906 completed, running loss: 0.0010
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 27/50
Train Loss: 0.0010, Train Acc: 0.9997
Val Loss: 13.8146, Val Acc: 0.5752, ROC-AUC: 0.5984
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0009
Batch 20000/101906 completed, running loss: 0.0007
Batch 30000/101906 completed, running loss: 0.0005
Batch 40000/101906 completed, running loss: 0.0006
Batch 50000/101906 completed, running loss: 0.0006
Batch 60000/101906 completed, running loss: 0.0005
Batch 70000/101906 completed, running loss: 0.0005
Batch 80000/101906 completed, running loss: 0.0005
Batch 90000/101906 completed, running loss: 0.0005
Batch 100000/101906 completed, running loss: 0.0004
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 28/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 21.0627, Val Acc: 0.5606, ROC-AUC: 0.5768
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0002
Batch 20000/101906 completed, running loss: 0.0003
Batch 30000/101906 completed, running loss: 0.0003
Batch 40000/101906 completed, running loss: 0.0003
Batch 50000/101906 completed, running loss: 0.0003
Batch 60000/101906 completed, running loss: 0.0003
Batch 70000/101906 completed, running loss: 0.0003
Batch 80000/101906 completed, running loss: 0.0003
Batch 90000/101906 completed, running loss: 0.0003
Batch 100000/101906 completed, running loss: 0.0003
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 29/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 22.8148, Val Acc: 0.5814, ROC-AUC: 0.6005
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0006
Batch 20000/101906 completed, running loss: 0.0003
Batch 30000/101906 completed, running loss: 0.0003
Batch 40000/101906 completed, running loss: 0.0003
Batch 50000/101906 completed, running loss: 0.0003
Batch 60000/101906 completed, running loss: 0.0003
Batch 70000/101906 completed, running loss: 0.0003
Batch 80000/101906 completed, running loss: 0.0003
Batch 90000/101906 completed, running loss: 0.0003
Batch 100000/101906 completed, running loss: 0.0003
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 30/50
Train Loss: 0.0003, Train Acc: 1.0000
Val Loss: 25.0444, Val Acc: 0.5681, ROC-AUC: 0.5810
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0010
Batch 20000/101906 completed, running loss: 0.0006
Batch 30000/101906 completed, running loss: 0.0005
Batch 40000/101906 completed, running loss: 0.0004
Batch 50000/101906 completed, running loss: 0.0004
Batch 60000/101906 completed, running loss: 0.0004
Batch 70000/101906 completed, running loss: 0.0004
Batch 80000/101906 completed, running loss: 0.0004
Batch 90000/101906 completed, running loss: 0.0004
Batch 100000/101906 completed, running loss: 0.0004
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 31/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 24.6240, Val Acc: 0.5823, ROC-AUC: 0.5943
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0006
Batch 20000/101906 completed, running loss: 0.0004
Batch 30000/101906 completed, running loss: 0.0004
Batch 40000/101906 completed, running loss: 0.0004
Batch 50000/101906 completed, running loss: 0.0003
Batch 60000/101906 completed, running loss: 0.0003
Batch 70000/101906 completed, running loss: 0.0003
Batch 80000/101906 completed, running loss: 0.0003
Batch 90000/101906 completed, running loss: 0.0003
Batch 100000/101906 completed, running loss: 0.0003
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 32/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 24.7865, Val Acc: 0.5800, ROC-AUC: 0.5919
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0000
Batch 30000/101906 completed, running loss: 0.0001
Batch 40000/101906 completed, running loss: 0.0001
Batch 50000/101906 completed, running loss: 0.0001
Batch 60000/101906 completed, running loss: 0.0001
Batch 70000/101906 completed, running loss: 0.0001
Batch 80000/101906 completed, running loss: 0.0000
Batch 90000/101906 completed, running loss: 0.0000
Batch 100000/101906 completed, running loss: 0.0001
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 33/50
Train Loss: 0.0001, Train Acc: 1.0000
Val Loss: 27.9013, Val Acc: 0.5808, ROC-AUC: 0.5925
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0002
Batch 20000/101906 completed, running loss: 0.0002
Batch 30000/101906 completed, running loss: 0.0003
Batch 40000/101906 completed, running loss: 0.0004
Batch 50000/101906 completed, running loss: 0.0003
Batch 60000/101906 completed, running loss: 0.0004
Batch 70000/101906 completed, running loss: 0.0003
Batch 80000/101906 completed, running loss: 0.0003
Batch 90000/101906 completed, running loss: 0.0003
Batch 100000/101906 completed, running loss: 0.0003
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 34/50
Train Loss: 0.0003, Train Acc: 1.0000
Val Loss: 30.2277, Val Acc: 0.5722, ROC-AUC: 0.5803
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0000
Batch 30000/101906 completed, running loss: 0.0000
Batch 40000/101906 completed, running loss: 0.0000
Batch 50000/101906 completed, running loss: 0.0001
Batch 60000/101906 completed, running loss: 0.0001
Batch 70000/101906 completed, running loss: 0.0002
Batch 80000/101906 completed, running loss: 0.0002
Batch 90000/101906 completed, running loss: 0.0002
Batch 100000/101906 completed, running loss: 0.0002
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 35/50
Train Loss: 0.0002, Train Acc: 1.0000
Val Loss: 32.4748, Val Acc: 0.5718, ROC-AUC: 0.5830
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0001
Batch 30000/101906 completed, running loss: 0.0004
Batch 40000/101906 completed, running loss: 0.0004
Batch 50000/101906 completed, running loss: 0.0004
Batch 60000/101906 completed, running loss: 0.0003
Batch 70000/101906 completed, running loss: 0.0003
Batch 80000/101906 completed, running loss: 0.0002
Batch 90000/101906 completed, running loss: 0.0002
Batch 100000/101906 completed, running loss: 0.0002
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 36/50
Train Loss: 0.0002, Train Acc: 1.0000
Val Loss: 32.5116, Val Acc: 0.5782, ROC-AUC: 0.5886
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0002
Batch 30000/101906 completed, running loss: 0.0002
Batch 40000/101906 completed, running loss: 0.0002
Batch 50000/101906 completed, running loss: 0.0001
Batch 60000/101906 completed, running loss: 0.0001
Batch 70000/101906 completed, running loss: 0.0001
Batch 80000/101906 completed, running loss: 0.0002
Batch 90000/101906 completed, running loss: 0.0002
Batch 100000/101906 completed, running loss: 0.0002
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 37/50
Train Loss: 0.0002, Train Acc: 1.0000
Val Loss: 34.1736, Val Acc: 0.5714, ROC-AUC: 0.5816
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0001
Batch 20000/101906 completed, running loss: 0.0000
Batch 30000/101906 completed, running loss: 0.0002
Batch 40000/101906 completed, running loss: 0.0002
Batch 50000/101906 completed, running loss: 0.0002
Batch 60000/101906 completed, running loss: 0.0002
Batch 70000/101906 completed, running loss: 0.0002
Batch 80000/101906 completed, running loss: 0.0002
Batch 90000/101906 completed, running loss: 0.0002
Batch 100000/101906 completed, running loss: 0.0002
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 38/50
Train Loss: 0.0002, Train Acc: 1.0000
Val Loss: 35.5204, Val Acc: 0.5765, ROC-AUC: 0.5808
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0000
Batch 30000/101906 completed, running loss: 0.0001
Batch 40000/101906 completed, running loss: 0.0001
Batch 50000/101906 completed, running loss: 0.0001
Batch 60000/101906 completed, running loss: 0.0001
Batch 70000/101906 completed, running loss: 0.0001
Batch 80000/101906 completed, running loss: 0.0001
Batch 90000/101906 completed, running loss: 0.0001
Batch 100000/101906 completed, running loss: 0.0001
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 39/50
Train Loss: 0.0001, Train Acc: 1.0000
Val Loss: 34.4674, Val Acc: 0.5766, ROC-AUC: 0.5858
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0004
Batch 30000/101906 completed, running loss: 0.0003
Batch 40000/101906 completed, running loss: 0.0002
Batch 50000/101906 completed, running loss: 0.0002
Batch 60000/101906 completed, running loss: 0.0002
Batch 70000/101906 completed, running loss: 0.0002
Batch 80000/101906 completed, running loss: 0.0002
Batch 90000/101906 completed, running loss: 0.0002
Batch 100000/101906 completed, running loss: 0.0003
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 40/50
Train Loss: 0.0002, Train Acc: 1.0000
Val Loss: 36.6058, Val Acc: 0.5725, ROC-AUC: 0.5841
Starting training epoch with 101906 batches
Batch 10000/101906 completed, running loss: 0.0000
Batch 20000/101906 completed, running loss: 0.0001
Batch 30000/101906 completed, running loss: 0.0001
Batch 40000/101906 completed, running loss: 0.0001
Batch 50000/101906 completed, running loss: 0.0001
Batch 60000/101906 completed, running loss: 0.0001
Batch 70000/101906 completed, running loss: 0.0001
Batch 80000/101906 completed, running loss: 0.0001
Batch 90000/101906 completed, running loss: 0.0001
Batch 100000/101906 completed, running loss: 0.0001
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Fold 4 - Epoch 41/50
Train Loss: 0.0001, Train Acc: 1.0000
Val Loss: 38.5319, Val Acc: 0.5545, ROC-AUC: 0.5660
Fold 4 - Early stopping triggered
Starting evaluation with 101906 batches
Evaluation batch 10000/101906 completed
Evaluation batch 20000/101906 completed
Evaluation batch 30000/101906 completed
Evaluation batch 40000/101906 completed
Evaluation batch 50000/101906 completed
Evaluation batch 60000/101906 completed
Evaluation batch 70000/101906 completed
Evaluation batch 80000/101906 completed
Evaluation batch 90000/101906 completed
Evaluation batch 100000/101906 completed
Starting evaluation with 16469 batches
Evaluation batch 10000/16469 completed
Starting evaluation with 17543 batches
Evaluation batch 10000/17543 completed
Fold 4 - Test Metrics:
Loss: 3.2643
Accuracy: 0.7075
Precision: 0.7020
Recall: 0.7086
Roc_auc: 0.7854
Specificity: 0.7064
F1: 0.7053

Starting Fold 5/10
Fold 5 - Training label counts: {1: 221083, 0: 214562}
Fold 5 - Validation label counts: {1: 39118, 0: 30702}
Fold 5 - Test label counts: {1: 24541, 0: 13664}
Preparing dataset with 435645 samples
Extracting features
Normalizing embeddings
Dataset ready with 435645 samples
Preparing dataset with 69820 samples
Extracting features
Normalizing embeddings
Dataset ready with 69820 samples
Preparing dataset with 38205 samples
Extracting features
Normalizing embeddings
Dataset ready with 38205 samples
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0787
Batch 20000/108912 completed, running loss: 0.0579
Batch 30000/108912 completed, running loss: 0.0486
Batch 40000/108912 completed, running loss: 0.0426
Batch 50000/108912 completed, running loss: 0.0386
Batch 60000/108912 completed, running loss: 0.0357
Batch 70000/108912 completed, running loss: 0.0336
Batch 80000/108912 completed, running loss: 0.0317
Batch 90000/108912 completed, running loss: 0.0302
Batch 100000/108912 completed, running loss: 0.0288
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 1/50
Train Loss: 0.0278, Train Acc: 0.9896
Val Loss: 2.8724, Val Acc: 0.6659, ROC-AUC: 0.7104
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0128
Batch 20000/108912 completed, running loss: 0.0132
Batch 30000/108912 completed, running loss: 0.0129
Batch 40000/108912 completed, running loss: 0.0126
Batch 50000/108912 completed, running loss: 0.0126
Batch 60000/108912 completed, running loss: 0.0125
Batch 70000/108912 completed, running loss: 0.0123
Batch 80000/108912 completed, running loss: 0.0124
Batch 90000/108912 completed, running loss: 0.0123
Batch 100000/108912 completed, running loss: 0.0123
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 2/50
Train Loss: 0.0121, Train Acc: 0.9956
Val Loss: 3.6935, Val Acc: 0.6650, ROC-AUC: 0.6976
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0083
Batch 20000/108912 completed, running loss: 0.0088
Batch 30000/108912 completed, running loss: 0.0087
Batch 40000/108912 completed, running loss: 0.0087
Batch 50000/108912 completed, running loss: 0.0087
Batch 60000/108912 completed, running loss: 0.0087
Batch 70000/108912 completed, running loss: 0.0086
Batch 80000/108912 completed, running loss: 0.0086
Batch 90000/108912 completed, running loss: 0.0088
Batch 100000/108912 completed, running loss: 0.0086
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 3/50
Train Loss: 0.0087, Train Acc: 0.9969
Val Loss: 4.2031, Val Acc: 0.6359, ROC-AUC: 0.6694
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0064
Batch 20000/108912 completed, running loss: 0.0064
Batch 30000/108912 completed, running loss: 0.0066
Batch 40000/108912 completed, running loss: 0.0066
Batch 50000/108912 completed, running loss: 0.0067
Batch 60000/108912 completed, running loss: 0.0067
Batch 70000/108912 completed, running loss: 0.0067
Batch 80000/108912 completed, running loss: 0.0067
Batch 90000/108912 completed, running loss: 0.0067
Batch 100000/108912 completed, running loss: 0.0067
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 4/50
Train Loss: 0.0067, Train Acc: 0.9976
Val Loss: 5.7517, Val Acc: 0.6211, ROC-AUC: 0.6334
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0054
Batch 20000/108912 completed, running loss: 0.0055
Batch 30000/108912 completed, running loss: 0.0055
Batch 40000/108912 completed, running loss: 0.0053
Batch 50000/108912 completed, running loss: 0.0053
Batch 60000/108912 completed, running loss: 0.0053
Batch 70000/108912 completed, running loss: 0.0053
Batch 80000/108912 completed, running loss: 0.0054
Batch 90000/108912 completed, running loss: 0.0054
Batch 100000/108912 completed, running loss: 0.0054
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 5/50
Train Loss: 0.0055, Train Acc: 0.9980
Val Loss: 5.5581, Val Acc: 0.6526, ROC-AUC: 0.6832
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0051
Batch 20000/108912 completed, running loss: 0.0045
Batch 30000/108912 completed, running loss: 0.0045
Batch 40000/108912 completed, running loss: 0.0047
Batch 50000/108912 completed, running loss: 0.0046
Batch 60000/108912 completed, running loss: 0.0046
Batch 70000/108912 completed, running loss: 0.0046
Batch 80000/108912 completed, running loss: 0.0045
Batch 90000/108912 completed, running loss: 0.0045
Batch 100000/108912 completed, running loss: 0.0045
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 6/50
Train Loss: 0.0046, Train Acc: 0.9984
Val Loss: 9.0016, Val Acc: 0.6018, ROC-AUC: 0.6083
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0036
Batch 20000/108912 completed, running loss: 0.0040
Batch 30000/108912 completed, running loss: 0.0039
Batch 40000/108912 completed, running loss: 0.0040
Batch 50000/108912 completed, running loss: 0.0039
Batch 60000/108912 completed, running loss: 0.0038
Batch 70000/108912 completed, running loss: 0.0039
Batch 80000/108912 completed, running loss: 0.0040
Batch 90000/108912 completed, running loss: 0.0040
Batch 100000/108912 completed, running loss: 0.0040
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 7/50
Train Loss: 0.0039, Train Acc: 0.9987
Val Loss: 7.9193, Val Acc: 0.6268, ROC-AUC: 0.6544
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0028
Batch 20000/108912 completed, running loss: 0.0032
Batch 30000/108912 completed, running loss: 0.0034
Batch 40000/108912 completed, running loss: 0.0034
Batch 50000/108912 completed, running loss: 0.0034
Batch 60000/108912 completed, running loss: 0.0035
Batch 70000/108912 completed, running loss: 0.0035
Batch 80000/108912 completed, running loss: 0.0035
Batch 90000/108912 completed, running loss: 0.0035
Batch 100000/108912 completed, running loss: 0.0034
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 8/50
Train Loss: 0.0035, Train Acc: 0.9988
Val Loss: 5.4722, Val Acc: 0.6498, ROC-AUC: 0.6888
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0025
Batch 20000/108912 completed, running loss: 0.0028
Batch 30000/108912 completed, running loss: 0.0030
Batch 40000/108912 completed, running loss: 0.0031
Batch 50000/108912 completed, running loss: 0.0032
Batch 60000/108912 completed, running loss: 0.0031
Batch 70000/108912 completed, running loss: 0.0032
Batch 80000/108912 completed, running loss: 0.0032
Batch 90000/108912 completed, running loss: 0.0033
Batch 100000/108912 completed, running loss: 0.0032
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 9/50
Train Loss: 0.0033, Train Acc: 0.9989
Val Loss: 8.0758, Val Acc: 0.6354, ROC-AUC: 0.6779
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0028
Batch 20000/108912 completed, running loss: 0.0026
Batch 30000/108912 completed, running loss: 0.0025
Batch 40000/108912 completed, running loss: 0.0025
Batch 50000/108912 completed, running loss: 0.0027
Batch 60000/108912 completed, running loss: 0.0027
Batch 70000/108912 completed, running loss: 0.0028
Batch 80000/108912 completed, running loss: 0.0028
Batch 90000/108912 completed, running loss: 0.0028
Batch 100000/108912 completed, running loss: 0.0028
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 10/50
Train Loss: 0.0027, Train Acc: 0.9991
Val Loss: 9.7673, Val Acc: 0.6308, ROC-AUC: 0.6535
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0025
Batch 20000/108912 completed, running loss: 0.0022
Batch 30000/108912 completed, running loss: 0.0024
Batch 40000/108912 completed, running loss: 0.0024
Batch 50000/108912 completed, running loss: 0.0023
Batch 60000/108912 completed, running loss: 0.0023
Batch 70000/108912 completed, running loss: 0.0023
Batch 80000/108912 completed, running loss: 0.0024
Batch 90000/108912 completed, running loss: 0.0024
Batch 100000/108912 completed, running loss: 0.0025
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 11/50
Train Loss: 0.0025, Train Acc: 0.9991
Val Loss: 7.7871, Val Acc: 0.6356, ROC-AUC: 0.6687
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0018
Batch 20000/108912 completed, running loss: 0.0023
Batch 30000/108912 completed, running loss: 0.0022
Batch 40000/108912 completed, running loss: 0.0021
Batch 50000/108912 completed, running loss: 0.0022
Batch 60000/108912 completed, running loss: 0.0022
Batch 70000/108912 completed, running loss: 0.0023
Batch 80000/108912 completed, running loss: 0.0023
Batch 90000/108912 completed, running loss: 0.0023
Batch 100000/108912 completed, running loss: 0.0023
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 12/50
Train Loss: 0.0023, Train Acc: 0.9993
Val Loss: 10.6125, Val Acc: 0.6462, ROC-AUC: 0.6645
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0017
Batch 20000/108912 completed, running loss: 0.0019
Batch 30000/108912 completed, running loss: 0.0017
Batch 40000/108912 completed, running loss: 0.0017
Batch 50000/108912 completed, running loss: 0.0020
Batch 60000/108912 completed, running loss: 0.0020
Batch 70000/108912 completed, running loss: 0.0021
Batch 80000/108912 completed, running loss: 0.0021
Batch 90000/108912 completed, running loss: 0.0020
Batch 100000/108912 completed, running loss: 0.0020
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 13/50
Train Loss: 0.0020, Train Acc: 0.9994
Val Loss: 11.6979, Val Acc: 0.5759, ROC-AUC: 0.5864
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0028
Batch 20000/108912 completed, running loss: 0.0021
Batch 30000/108912 completed, running loss: 0.0019
Batch 40000/108912 completed, running loss: 0.0020
Batch 50000/108912 completed, running loss: 0.0021
Batch 60000/108912 completed, running loss: 0.0022
Batch 70000/108912 completed, running loss: 0.0021
Batch 80000/108912 completed, running loss: 0.0021
Batch 90000/108912 completed, running loss: 0.0020
Batch 100000/108912 completed, running loss: 0.0021
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 14/50
Train Loss: 0.0022, Train Acc: 0.9993
Val Loss: 11.2457, Val Acc: 0.5992, ROC-AUC: 0.6173
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0019
Batch 20000/108912 completed, running loss: 0.0016
Batch 30000/108912 completed, running loss: 0.0015
Batch 40000/108912 completed, running loss: 0.0017
Batch 50000/108912 completed, running loss: 0.0017
Batch 60000/108912 completed, running loss: 0.0018
Batch 70000/108912 completed, running loss: 0.0018
Batch 80000/108912 completed, running loss: 0.0018
Batch 90000/108912 completed, running loss: 0.0017
Batch 100000/108912 completed, running loss: 0.0018
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 15/50
Train Loss: 0.0018, Train Acc: 0.9995
Val Loss: 10.1722, Val Acc: 0.6322, ROC-AUC: 0.6624
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0015
Batch 20000/108912 completed, running loss: 0.0012
Batch 30000/108912 completed, running loss: 0.0015
Batch 40000/108912 completed, running loss: 0.0016
Batch 50000/108912 completed, running loss: 0.0016
Batch 60000/108912 completed, running loss: 0.0016
Batch 70000/108912 completed, running loss: 0.0017
Batch 80000/108912 completed, running loss: 0.0017
Batch 90000/108912 completed, running loss: 0.0016
Batch 100000/108912 completed, running loss: 0.0016
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 16/50
Train Loss: 0.0016, Train Acc: 0.9995
Val Loss: 10.9906, Val Acc: 0.6431, ROC-AUC: 0.6724
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0018
Batch 20000/108912 completed, running loss: 0.0013
Batch 30000/108912 completed, running loss: 0.0015
Batch 40000/108912 completed, running loss: 0.0014
Batch 50000/108912 completed, running loss: 0.0015
Batch 60000/108912 completed, running loss: 0.0016
Batch 70000/108912 completed, running loss: 0.0015
Batch 80000/108912 completed, running loss: 0.0016
Batch 90000/108912 completed, running loss: 0.0015
Batch 100000/108912 completed, running loss: 0.0015
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 17/50
Train Loss: 0.0015, Train Acc: 0.9995
Val Loss: 13.6035, Val Acc: 0.6017, ROC-AUC: 0.6252
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0008
Batch 20000/108912 completed, running loss: 0.0010
Batch 30000/108912 completed, running loss: 0.0008
Batch 40000/108912 completed, running loss: 0.0009
Batch 50000/108912 completed, running loss: 0.0008
Batch 60000/108912 completed, running loss: 0.0008
Batch 70000/108912 completed, running loss: 0.0008
Batch 80000/108912 completed, running loss: 0.0007
Batch 90000/108912 completed, running loss: 0.0007
Batch 100000/108912 completed, running loss: 0.0007
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 18/50
Train Loss: 0.0008, Train Acc: 0.9997
Val Loss: 21.4364, Val Acc: 0.5753, ROC-AUC: 0.5853
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0003
Batch 20000/108912 completed, running loss: 0.0006
Batch 30000/108912 completed, running loss: 0.0007
Batch 40000/108912 completed, running loss: 0.0008
Batch 50000/108912 completed, running loss: 0.0007
Batch 60000/108912 completed, running loss: 0.0007
Batch 70000/108912 completed, running loss: 0.0006
Batch 80000/108912 completed, running loss: 0.0006
Batch 90000/108912 completed, running loss: 0.0006
Batch 100000/108912 completed, running loss: 0.0006
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 19/50
Train Loss: 0.0006, Train Acc: 0.9998
Val Loss: 19.9734, Val Acc: 0.6109, ROC-AUC: 0.6235
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0004
Batch 20000/108912 completed, running loss: 0.0005
Batch 30000/108912 completed, running loss: 0.0004
Batch 40000/108912 completed, running loss: 0.0003
Batch 50000/108912 completed, running loss: 0.0003
Batch 60000/108912 completed, running loss: 0.0003
Batch 70000/108912 completed, running loss: 0.0004
Batch 80000/108912 completed, running loss: 0.0004
Batch 90000/108912 completed, running loss: 0.0005
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 20/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 27.4727, Val Acc: 0.5923, ROC-AUC: 0.5987
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0004
Batch 20000/108912 completed, running loss: 0.0004
Batch 30000/108912 completed, running loss: 0.0003
Batch 40000/108912 completed, running loss: 0.0003
Batch 50000/108912 completed, running loss: 0.0004
Batch 60000/108912 completed, running loss: 0.0004
Batch 70000/108912 completed, running loss: 0.0004
Batch 80000/108912 completed, running loss: 0.0005
Batch 90000/108912 completed, running loss: 0.0004
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 21/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 27.3460, Val Acc: 0.5983, ROC-AUC: 0.6048
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0006
Batch 20000/108912 completed, running loss: 0.0005
Batch 30000/108912 completed, running loss: 0.0005
Batch 40000/108912 completed, running loss: 0.0005
Batch 50000/108912 completed, running loss: 0.0004
Batch 60000/108912 completed, running loss: 0.0004
Batch 70000/108912 completed, running loss: 0.0005
Batch 80000/108912 completed, running loss: 0.0005
Batch 90000/108912 completed, running loss: 0.0005
Batch 100000/108912 completed, running loss: 0.0005
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 22/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 31.6350, Val Acc: 0.5673, ROC-AUC: 0.5733
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0005
Batch 20000/108912 completed, running loss: 0.0003
Batch 30000/108912 completed, running loss: 0.0004
Batch 40000/108912 completed, running loss: 0.0004
Batch 50000/108912 completed, running loss: 0.0005
Batch 60000/108912 completed, running loss: 0.0004
Batch 70000/108912 completed, running loss: 0.0005
Batch 80000/108912 completed, running loss: 0.0005
Batch 90000/108912 completed, running loss: 0.0005
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 23/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 33.8275, Val Acc: 0.5740, ROC-AUC: 0.5855
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0005
Batch 20000/108912 completed, running loss: 0.0005
Batch 30000/108912 completed, running loss: 0.0004
Batch 40000/108912 completed, running loss: 0.0006
Batch 50000/108912 completed, running loss: 0.0005
Batch 60000/108912 completed, running loss: 0.0006
Batch 70000/108912 completed, running loss: 0.0006
Batch 80000/108912 completed, running loss: 0.0006
Batch 90000/108912 completed, running loss: 0.0006
Batch 100000/108912 completed, running loss: 0.0006
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 24/50
Train Loss: 0.0006, Train Acc: 0.9999
Val Loss: 34.7576, Val Acc: 0.5678, ROC-AUC: 0.5820
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0012
Batch 20000/108912 completed, running loss: 0.0007
Batch 30000/108912 completed, running loss: 0.0005
Batch 40000/108912 completed, running loss: 0.0004
Batch 50000/108912 completed, running loss: 0.0004
Batch 60000/108912 completed, running loss: 0.0004
Batch 70000/108912 completed, running loss: 0.0004
Batch 80000/108912 completed, running loss: 0.0004
Batch 90000/108912 completed, running loss: 0.0005
Batch 100000/108912 completed, running loss: 0.0005
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 25/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 33.9269, Val Acc: 0.5820, ROC-AUC: 0.5957
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0002
Batch 20000/108912 completed, running loss: 0.0004
Batch 30000/108912 completed, running loss: 0.0004
Batch 40000/108912 completed, running loss: 0.0004
Batch 50000/108912 completed, running loss: 0.0004
Batch 60000/108912 completed, running loss: 0.0004
Batch 70000/108912 completed, running loss: 0.0004
Batch 80000/108912 completed, running loss: 0.0004
Batch 90000/108912 completed, running loss: 0.0005
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 26/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 36.7451, Val Acc: 0.5841, ROC-AUC: 0.5986
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0002
Batch 20000/108912 completed, running loss: 0.0009
Batch 30000/108912 completed, running loss: 0.0008
Batch 40000/108912 completed, running loss: 0.0008
Batch 50000/108912 completed, running loss: 0.0008
Batch 60000/108912 completed, running loss: 0.0008
Batch 70000/108912 completed, running loss: 0.0007
Batch 80000/108912 completed, running loss: 0.0008
Batch 90000/108912 completed, running loss: 0.0007
Batch 100000/108912 completed, running loss: 0.0007
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 27/50
Train Loss: 0.0007, Train Acc: 0.9998
Val Loss: 43.8983, Val Acc: 0.5662, ROC-AUC: 0.5743
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0009
Batch 20000/108912 completed, running loss: 0.0006
Batch 30000/108912 completed, running loss: 0.0006
Batch 40000/108912 completed, running loss: 0.0006
Batch 50000/108912 completed, running loss: 0.0006
Batch 60000/108912 completed, running loss: 0.0007
Batch 70000/108912 completed, running loss: 0.0007
Batch 80000/108912 completed, running loss: 0.0007
Batch 90000/108912 completed, running loss: 0.0007
Batch 100000/108912 completed, running loss: 0.0008
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 28/50
Train Loss: 0.0008, Train Acc: 0.9998
Val Loss: 37.9603, Val Acc: 0.5795, ROC-AUC: 0.5963
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0005
Batch 20000/108912 completed, running loss: 0.0004
Batch 30000/108912 completed, running loss: 0.0003
Batch 40000/108912 completed, running loss: 0.0003
Batch 50000/108912 completed, running loss: 0.0004
Batch 60000/108912 completed, running loss: 0.0004
Batch 70000/108912 completed, running loss: 0.0004
Batch 80000/108912 completed, running loss: 0.0004
Batch 90000/108912 completed, running loss: 0.0004
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 29/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 46.4529, Val Acc: 0.5706, ROC-AUC: 0.5845
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0005
Batch 20000/108912 completed, running loss: 0.0004
Batch 30000/108912 completed, running loss: 0.0005
Batch 40000/108912 completed, running loss: 0.0005
Batch 50000/108912 completed, running loss: 0.0005
Batch 60000/108912 completed, running loss: 0.0005
Batch 70000/108912 completed, running loss: 0.0005
Batch 80000/108912 completed, running loss: 0.0004
Batch 90000/108912 completed, running loss: 0.0004
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 30/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 46.5420, Val Acc: 0.5715, ROC-AUC: 0.5901
Starting training epoch with 108912 batches
Batch 10000/108912 completed, running loss: 0.0011
Batch 20000/108912 completed, running loss: 0.0006
Batch 30000/108912 completed, running loss: 0.0005
Batch 40000/108912 completed, running loss: 0.0005
Batch 50000/108912 completed, running loss: 0.0005
Batch 60000/108912 completed, running loss: 0.0005
Batch 70000/108912 completed, running loss: 0.0005
Batch 80000/108912 completed, running loss: 0.0005
Batch 90000/108912 completed, running loss: 0.0005
Batch 100000/108912 completed, running loss: 0.0004
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 5 - Epoch 31/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 43.1629, Val Acc: 0.5848, ROC-AUC: 0.5928
Fold 5 - Early stopping triggered
Starting evaluation with 108912 batches
Evaluation batch 10000/108912 completed
Evaluation batch 20000/108912 completed
Evaluation batch 30000/108912 completed
Evaluation batch 40000/108912 completed
Evaluation batch 50000/108912 completed
Evaluation batch 60000/108912 completed
Evaluation batch 70000/108912 completed
Evaluation batch 80000/108912 completed
Evaluation batch 90000/108912 completed
Evaluation batch 100000/108912 completed
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Starting evaluation with 9552 batches
Fold 5 - Test Metrics:
Loss: 2.5324
Accuracy: 0.7504
Precision: 0.9116
Recall: 0.6772
Roc_auc: 0.7951
Specificity: 0.8820
F1: 0.7771

Starting Fold 6/10
Fold 6 - Training label counts: {1: 221660, 0: 212853}
Fold 6 - Validation label counts: {1: 39118, 0: 32311}
Fold 6 - Test label counts: {1: 23964, 0: 13764}
Preparing dataset with 434513 samples
Extracting features
Normalizing embeddings
Dataset ready with 434513 samples
Preparing dataset with 71429 samples
Extracting features
Normalizing embeddings
Dataset ready with 71429 samples
Preparing dataset with 37728 samples
Extracting features
Normalizing embeddings
Dataset ready with 37728 samples
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0828
Batch 20000/108629 completed, running loss: 0.0607
Batch 30000/108629 completed, running loss: 0.0509
Batch 40000/108629 completed, running loss: 0.0452
Batch 50000/108629 completed, running loss: 0.0410
Batch 60000/108629 completed, running loss: 0.0381
Batch 70000/108629 completed, running loss: 0.0358
Batch 80000/108629 completed, running loss: 0.0337
Batch 90000/108629 completed, running loss: 0.0322
Batch 100000/108629 completed, running loss: 0.0307
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 1/50
Train Loss: 0.0297, Train Acc: 0.9887
Val Loss: 3.2594, Val Acc: 0.5665, ROC-AUC: 0.5936
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0139
Batch 20000/108629 completed, running loss: 0.0140
Batch 30000/108629 completed, running loss: 0.0134
Batch 40000/108629 completed, running loss: 0.0135
Batch 50000/108629 completed, running loss: 0.0136
Batch 60000/108629 completed, running loss: 0.0133
Batch 70000/108629 completed, running loss: 0.0134
Batch 80000/108629 completed, running loss: 0.0133
Batch 90000/108629 completed, running loss: 0.0131
Batch 100000/108629 completed, running loss: 0.0131
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 2/50
Train Loss: 0.0130, Train Acc: 0.9952
Val Loss: 3.0927, Val Acc: 0.6765, ROC-AUC: 0.7246
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0088
Batch 20000/108629 completed, running loss: 0.0094
Batch 30000/108629 completed, running loss: 0.0102
Batch 40000/108629 completed, running loss: 0.0099
Batch 50000/108629 completed, running loss: 0.0096
Batch 60000/108629 completed, running loss: 0.0095
Batch 70000/108629 completed, running loss: 0.0096
Batch 80000/108629 completed, running loss: 0.0097
Batch 90000/108629 completed, running loss: 0.0097
Batch 100000/108629 completed, running loss: 0.0096
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 3/50
Train Loss: 0.0096, Train Acc: 0.9965
Val Loss: 3.9958, Val Acc: 0.6510, ROC-AUC: 0.6959
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0073
Batch 20000/108629 completed, running loss: 0.0071
Batch 30000/108629 completed, running loss: 0.0070
Batch 40000/108629 completed, running loss: 0.0071
Batch 50000/108629 completed, running loss: 0.0073
Batch 60000/108629 completed, running loss: 0.0074
Batch 70000/108629 completed, running loss: 0.0076
Batch 80000/108629 completed, running loss: 0.0076
Batch 90000/108629 completed, running loss: 0.0076
Batch 100000/108629 completed, running loss: 0.0076
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 4/50
Train Loss: 0.0076, Train Acc: 0.9972
Val Loss: 4.6613, Val Acc: 0.6575, ROC-AUC: 0.6969
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0065
Batch 20000/108629 completed, running loss: 0.0063
Batch 30000/108629 completed, running loss: 0.0061
Batch 40000/108629 completed, running loss: 0.0061
Batch 50000/108629 completed, running loss: 0.0061
Batch 60000/108629 completed, running loss: 0.0062
Batch 70000/108629 completed, running loss: 0.0062
Batch 80000/108629 completed, running loss: 0.0061
Batch 90000/108629 completed, running loss: 0.0062
Batch 100000/108629 completed, running loss: 0.0063
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 5/50
Train Loss: 0.0062, Train Acc: 0.9977
Val Loss: 4.8243, Val Acc: 0.6224, ROC-AUC: 0.6621
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0040
Batch 20000/108629 completed, running loss: 0.0043
Batch 30000/108629 completed, running loss: 0.0046
Batch 40000/108629 completed, running loss: 0.0048
Batch 50000/108629 completed, running loss: 0.0049
Batch 60000/108629 completed, running loss: 0.0051
Batch 70000/108629 completed, running loss: 0.0052
Batch 80000/108629 completed, running loss: 0.0053
Batch 90000/108629 completed, running loss: 0.0053
Batch 100000/108629 completed, running loss: 0.0053
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 6/50
Train Loss: 0.0055, Train Acc: 0.9981
Val Loss: 4.1516, Val Acc: 0.6557, ROC-AUC: 0.6924
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0039
Batch 20000/108629 completed, running loss: 0.0041
Batch 30000/108629 completed, running loss: 0.0039
Batch 40000/108629 completed, running loss: 0.0040
Batch 50000/108629 completed, running loss: 0.0040
Batch 60000/108629 completed, running loss: 0.0040
Batch 70000/108629 completed, running loss: 0.0041
Batch 80000/108629 completed, running loss: 0.0042
Batch 90000/108629 completed, running loss: 0.0043
Batch 100000/108629 completed, running loss: 0.0044
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 7/50
Train Loss: 0.0045, Train Acc: 0.9983
Val Loss: 4.4020, Val Acc: 0.6696, ROC-AUC: 0.7138
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0036
Batch 20000/108629 completed, running loss: 0.0035
Batch 30000/108629 completed, running loss: 0.0036
Batch 40000/108629 completed, running loss: 0.0036
Batch 50000/108629 completed, running loss: 0.0039
Batch 60000/108629 completed, running loss: 0.0039
Batch 70000/108629 completed, running loss: 0.0040
Batch 80000/108629 completed, running loss: 0.0041
Batch 90000/108629 completed, running loss: 0.0041
Batch 100000/108629 completed, running loss: 0.0040
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 8/50
Train Loss: 0.0041, Train Acc: 0.9985
Val Loss: 6.4921, Val Acc: 0.6381, ROC-AUC: 0.6675
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0031
Batch 20000/108629 completed, running loss: 0.0033
Batch 30000/108629 completed, running loss: 0.0033
Batch 40000/108629 completed, running loss: 0.0033
Batch 50000/108629 completed, running loss: 0.0033
Batch 60000/108629 completed, running loss: 0.0033
Batch 70000/108629 completed, running loss: 0.0033
Batch 80000/108629 completed, running loss: 0.0034
Batch 90000/108629 completed, running loss: 0.0034
Batch 100000/108629 completed, running loss: 0.0034
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 9/50
Train Loss: 0.0034, Train Acc: 0.9988
Val Loss: 7.1001, Val Acc: 0.6707, ROC-AUC: 0.6981
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0027
Batch 20000/108629 completed, running loss: 0.0029
Batch 30000/108629 completed, running loss: 0.0030
Batch 40000/108629 completed, running loss: 0.0030
Batch 50000/108629 completed, running loss: 0.0029
Batch 60000/108629 completed, running loss: 0.0030
Batch 70000/108629 completed, running loss: 0.0030
Batch 80000/108629 completed, running loss: 0.0030
Batch 90000/108629 completed, running loss: 0.0031
Batch 100000/108629 completed, running loss: 0.0031
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 10/50
Train Loss: 0.0031, Train Acc: 0.9989
Val Loss: 7.6447, Val Acc: 0.6477, ROC-AUC: 0.6801
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0024
Batch 20000/108629 completed, running loss: 0.0023
Batch 30000/108629 completed, running loss: 0.0026
Batch 40000/108629 completed, running loss: 0.0029
Batch 50000/108629 completed, running loss: 0.0027
Batch 60000/108629 completed, running loss: 0.0028
Batch 70000/108629 completed, running loss: 0.0029
Batch 80000/108629 completed, running loss: 0.0029
Batch 90000/108629 completed, running loss: 0.0030
Batch 100000/108629 completed, running loss: 0.0030
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 11/50
Train Loss: 0.0030, Train Acc: 0.9990
Val Loss: 8.9033, Val Acc: 0.6637, ROC-AUC: 0.6821
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0021
Batch 20000/108629 completed, running loss: 0.0021
Batch 30000/108629 completed, running loss: 0.0025
Batch 40000/108629 completed, running loss: 0.0024
Batch 50000/108629 completed, running loss: 0.0024
Batch 60000/108629 completed, running loss: 0.0025
Batch 70000/108629 completed, running loss: 0.0025
Batch 80000/108629 completed, running loss: 0.0025
Batch 90000/108629 completed, running loss: 0.0026
Batch 100000/108629 completed, running loss: 0.0026
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 12/50
Train Loss: 0.0027, Train Acc: 0.9991
Val Loss: 7.2427, Val Acc: 0.6457, ROC-AUC: 0.6747
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0017
Batch 20000/108629 completed, running loss: 0.0016
Batch 30000/108629 completed, running loss: 0.0023
Batch 40000/108629 completed, running loss: 0.0023
Batch 50000/108629 completed, running loss: 0.0024
Batch 60000/108629 completed, running loss: 0.0023
Batch 70000/108629 completed, running loss: 0.0023
Batch 80000/108629 completed, running loss: 0.0024
Batch 90000/108629 completed, running loss: 0.0024
Batch 100000/108629 completed, running loss: 0.0024
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 13/50
Train Loss: 0.0025, Train Acc: 0.9993
Val Loss: 7.2197, Val Acc: 0.6618, ROC-AUC: 0.7033
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0020
Batch 20000/108629 completed, running loss: 0.0020
Batch 30000/108629 completed, running loss: 0.0021
Batch 40000/108629 completed, running loss: 0.0020
Batch 50000/108629 completed, running loss: 0.0022
Batch 60000/108629 completed, running loss: 0.0022
Batch 70000/108629 completed, running loss: 0.0022
Batch 80000/108629 completed, running loss: 0.0022
Batch 90000/108629 completed, running loss: 0.0022
Batch 100000/108629 completed, running loss: 0.0022
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 14/50
Train Loss: 0.0022, Train Acc: 0.9993
Val Loss: 9.4126, Val Acc: 0.6389, ROC-AUC: 0.6784
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0015
Batch 20000/108629 completed, running loss: 0.0019
Batch 30000/108629 completed, running loss: 0.0020
Batch 40000/108629 completed, running loss: 0.0019
Batch 50000/108629 completed, running loss: 0.0019
Batch 60000/108629 completed, running loss: 0.0020
Batch 70000/108629 completed, running loss: 0.0021
Batch 80000/108629 completed, running loss: 0.0022
Batch 90000/108629 completed, running loss: 0.0021
Batch 100000/108629 completed, running loss: 0.0022
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 15/50
Train Loss: 0.0022, Train Acc: 0.9993
Val Loss: 10.0853, Val Acc: 0.6292, ROC-AUC: 0.6608
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0010
Batch 20000/108629 completed, running loss: 0.0015
Batch 30000/108629 completed, running loss: 0.0015
Batch 40000/108629 completed, running loss: 0.0016
Batch 50000/108629 completed, running loss: 0.0016
Batch 60000/108629 completed, running loss: 0.0018
Batch 70000/108629 completed, running loss: 0.0018
Batch 80000/108629 completed, running loss: 0.0017
Batch 90000/108629 completed, running loss: 0.0019
Batch 100000/108629 completed, running loss: 0.0018
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 16/50
Train Loss: 0.0018, Train Acc: 0.9994
Val Loss: 14.0042, Val Acc: 0.6152, ROC-AUC: 0.6347
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0016
Batch 20000/108629 completed, running loss: 0.0019
Batch 30000/108629 completed, running loss: 0.0019
Batch 40000/108629 completed, running loss: 0.0019
Batch 50000/108629 completed, running loss: 0.0018
Batch 60000/108629 completed, running loss: 0.0018
Batch 70000/108629 completed, running loss: 0.0018
Batch 80000/108629 completed, running loss: 0.0019
Batch 90000/108629 completed, running loss: 0.0019
Batch 100000/108629 completed, running loss: 0.0019
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 17/50
Train Loss: 0.0019, Train Acc: 0.9994
Val Loss: 10.1613, Val Acc: 0.6271, ROC-AUC: 0.6473
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0014
Batch 20000/108629 completed, running loss: 0.0013
Batch 30000/108629 completed, running loss: 0.0012
Batch 40000/108629 completed, running loss: 0.0014
Batch 50000/108629 completed, running loss: 0.0014
Batch 60000/108629 completed, running loss: 0.0014
Batch 70000/108629 completed, running loss: 0.0015
Batch 80000/108629 completed, running loss: 0.0015
Batch 90000/108629 completed, running loss: 0.0015
Batch 100000/108629 completed, running loss: 0.0015
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 18/50
Train Loss: 0.0016, Train Acc: 0.9995
Val Loss: 11.3577, Val Acc: 0.6156, ROC-AUC: 0.6396
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0012
Batch 20000/108629 completed, running loss: 0.0013
Batch 30000/108629 completed, running loss: 0.0012
Batch 40000/108629 completed, running loss: 0.0010
Batch 50000/108629 completed, running loss: 0.0010
Batch 60000/108629 completed, running loss: 0.0010
Batch 70000/108629 completed, running loss: 0.0010
Batch 80000/108629 completed, running loss: 0.0010
Batch 90000/108629 completed, running loss: 0.0009
Batch 100000/108629 completed, running loss: 0.0010
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 19/50
Train Loss: 0.0009, Train Acc: 0.9997
Val Loss: 12.8897, Val Acc: 0.6180, ROC-AUC: 0.6431
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0004
Batch 20000/108629 completed, running loss: 0.0003
Batch 30000/108629 completed, running loss: 0.0003
Batch 40000/108629 completed, running loss: 0.0003
Batch 50000/108629 completed, running loss: 0.0004
Batch 60000/108629 completed, running loss: 0.0004
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0005
Batch 90000/108629 completed, running loss: 0.0005
Batch 100000/108629 completed, running loss: 0.0005
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 20/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 16.9339, Val Acc: 0.6180, ROC-AUC: 0.6371
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0005
Batch 20000/108629 completed, running loss: 0.0004
Batch 30000/108629 completed, running loss: 0.0004
Batch 40000/108629 completed, running loss: 0.0004
Batch 50000/108629 completed, running loss: 0.0004
Batch 60000/108629 completed, running loss: 0.0004
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0004
Batch 90000/108629 completed, running loss: 0.0005
Batch 100000/108629 completed, running loss: 0.0005
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 21/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 16.2117, Val Acc: 0.6414, ROC-AUC: 0.6606
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0014
Batch 20000/108629 completed, running loss: 0.0009
Batch 30000/108629 completed, running loss: 0.0008
Batch 40000/108629 completed, running loss: 0.0007
Batch 50000/108629 completed, running loss: 0.0006
Batch 60000/108629 completed, running loss: 0.0006
Batch 70000/108629 completed, running loss: 0.0006
Batch 80000/108629 completed, running loss: 0.0006
Batch 90000/108629 completed, running loss: 0.0006
Batch 100000/108629 completed, running loss: 0.0006
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 22/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 18.4941, Val Acc: 0.6309, ROC-AUC: 0.6494
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0005
Batch 20000/108629 completed, running loss: 0.0004
Batch 30000/108629 completed, running loss: 0.0004
Batch 40000/108629 completed, running loss: 0.0003
Batch 50000/108629 completed, running loss: 0.0004
Batch 60000/108629 completed, running loss: 0.0004
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0004
Batch 90000/108629 completed, running loss: 0.0004
Batch 100000/108629 completed, running loss: 0.0004
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 23/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 20.4004, Val Acc: 0.6275, ROC-AUC: 0.6420
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0001
Batch 20000/108629 completed, running loss: 0.0003
Batch 30000/108629 completed, running loss: 0.0004
Batch 40000/108629 completed, running loss: 0.0005
Batch 50000/108629 completed, running loss: 0.0004
Batch 60000/108629 completed, running loss: 0.0004
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0004
Batch 90000/108629 completed, running loss: 0.0004
Batch 100000/108629 completed, running loss: 0.0004
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 24/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 21.6942, Val Acc: 0.6186, ROC-AUC: 0.6364
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0006
Batch 20000/108629 completed, running loss: 0.0004
Batch 30000/108629 completed, running loss: 0.0003
Batch 40000/108629 completed, running loss: 0.0003
Batch 50000/108629 completed, running loss: 0.0003
Batch 60000/108629 completed, running loss: 0.0003
Batch 70000/108629 completed, running loss: 0.0003
Batch 80000/108629 completed, running loss: 0.0003
Batch 90000/108629 completed, running loss: 0.0003
Batch 100000/108629 completed, running loss: 0.0003
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 25/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 22.0737, Val Acc: 0.6282, ROC-AUC: 0.6493
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0001
Batch 20000/108629 completed, running loss: 0.0003
Batch 30000/108629 completed, running loss: 0.0004
Batch 40000/108629 completed, running loss: 0.0004
Batch 50000/108629 completed, running loss: 0.0005
Batch 60000/108629 completed, running loss: 0.0004
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0004
Batch 90000/108629 completed, running loss: 0.0004
Batch 100000/108629 completed, running loss: 0.0004
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 26/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 20.1740, Val Acc: 0.6377, ROC-AUC: 0.6605
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0003
Batch 20000/108629 completed, running loss: 0.0005
Batch 30000/108629 completed, running loss: 0.0007
Batch 40000/108629 completed, running loss: 0.0006
Batch 50000/108629 completed, running loss: 0.0006
Batch 60000/108629 completed, running loss: 0.0005
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0004
Batch 90000/108629 completed, running loss: 0.0005
Batch 100000/108629 completed, running loss: 0.0005
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 27/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 23.3303, Val Acc: 0.6190, ROC-AUC: 0.6350
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0003
Batch 20000/108629 completed, running loss: 0.0002
Batch 30000/108629 completed, running loss: 0.0003
Batch 40000/108629 completed, running loss: 0.0004
Batch 50000/108629 completed, running loss: 0.0003
Batch 60000/108629 completed, running loss: 0.0003
Batch 70000/108629 completed, running loss: 0.0003
Batch 80000/108629 completed, running loss: 0.0003
Batch 90000/108629 completed, running loss: 0.0003
Batch 100000/108629 completed, running loss: 0.0003
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 28/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 22.1435, Val Acc: 0.6283, ROC-AUC: 0.6473
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0003
Batch 20000/108629 completed, running loss: 0.0004
Batch 30000/108629 completed, running loss: 0.0003
Batch 40000/108629 completed, running loss: 0.0003
Batch 50000/108629 completed, running loss: 0.0004
Batch 60000/108629 completed, running loss: 0.0004
Batch 70000/108629 completed, running loss: 0.0004
Batch 80000/108629 completed, running loss: 0.0004
Batch 90000/108629 completed, running loss: 0.0004
Batch 100000/108629 completed, running loss: 0.0004
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 29/50
Train Loss: 0.0004, Train Acc: 0.9999
Val Loss: 27.8020, Val Acc: 0.6109, ROC-AUC: 0.6259
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0006
Batch 20000/108629 completed, running loss: 0.0003
Batch 30000/108629 completed, running loss: 0.0002
Batch 40000/108629 completed, running loss: 0.0002
Batch 50000/108629 completed, running loss: 0.0003
Batch 60000/108629 completed, running loss: 0.0003
Batch 70000/108629 completed, running loss: 0.0002
Batch 80000/108629 completed, running loss: 0.0003
Batch 90000/108629 completed, running loss: 0.0003
Batch 100000/108629 completed, running loss: 0.0003
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 30/50
Train Loss: 0.0002, Train Acc: 0.9999
Val Loss: 24.6661, Val Acc: 0.6270, ROC-AUC: 0.6408
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0002
Batch 20000/108629 completed, running loss: 0.0001
Batch 30000/108629 completed, running loss: 0.0002
Batch 40000/108629 completed, running loss: 0.0002
Batch 50000/108629 completed, running loss: 0.0002
Batch 60000/108629 completed, running loss: 0.0002
Batch 70000/108629 completed, running loss: 0.0003
Batch 80000/108629 completed, running loss: 0.0002
Batch 90000/108629 completed, running loss: 0.0003
Batch 100000/108629 completed, running loss: 0.0003
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 31/50
Train Loss: 0.0002, Train Acc: 0.9999
Val Loss: 27.3797, Val Acc: 0.6211, ROC-AUC: 0.6367
Starting training epoch with 108629 batches
Batch 10000/108629 completed, running loss: 0.0005
Batch 20000/108629 completed, running loss: 0.0003
Batch 30000/108629 completed, running loss: 0.0002
Batch 40000/108629 completed, running loss: 0.0002
Batch 50000/108629 completed, running loss: 0.0003
Batch 60000/108629 completed, running loss: 0.0002
Batch 70000/108629 completed, running loss: 0.0002
Batch 80000/108629 completed, running loss: 0.0002
Batch 90000/108629 completed, running loss: 0.0002
Batch 100000/108629 completed, running loss: 0.0002
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Fold 6 - Epoch 32/50
Train Loss: 0.0002, Train Acc: 1.0000
Val Loss: 31.6378, Val Acc: 0.6080, ROC-AUC: 0.6269
Fold 6 - Early stopping triggered
Starting evaluation with 108629 batches
Evaluation batch 10000/108629 completed
Evaluation batch 20000/108629 completed
Evaluation batch 30000/108629 completed
Evaluation batch 40000/108629 completed
Evaluation batch 50000/108629 completed
Evaluation batch 60000/108629 completed
Evaluation batch 70000/108629 completed
Evaluation batch 80000/108629 completed
Evaluation batch 90000/108629 completed
Evaluation batch 100000/108629 completed
Starting evaluation with 17858 batches
Evaluation batch 10000/17858 completed
Starting evaluation with 9432 batches
Fold 6 - Test Metrics:
Loss: 3.8525
Accuracy: 0.7773
Precision: 0.8908
Recall: 0.7401
Roc_auc: 0.7881
Specificity: 0.8421
F1: 0.8085

Starting Fold 7/10
Fold 7 - Training label counts: {1: 215439, 0: 213038}
Fold 7 - Validation label counts: {1: 39118, 0: 30702}
Fold 7 - Test label counts: {1: 30185, 0: 15188}
Preparing dataset with 428477 samples
Extracting features
Normalizing embeddings
Dataset ready with 428477 samples
Preparing dataset with 69820 samples
Extracting features
Normalizing embeddings
Dataset ready with 69820 samples
Preparing dataset with 45373 samples
Extracting features
Normalizing embeddings
Dataset ready with 45373 samples
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0733
Batch 20000/107120 completed, running loss: 0.0533
Batch 30000/107120 completed, running loss: 0.0450
Batch 40000/107120 completed, running loss: 0.0394
Batch 50000/107120 completed, running loss: 0.0357
Batch 60000/107120 completed, running loss: 0.0329
Batch 70000/107120 completed, running loss: 0.0310
Batch 80000/107120 completed, running loss: 0.0293
Batch 90000/107120 completed, running loss: 0.0279
Batch 100000/107120 completed, running loss: 0.0267
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 1/50
Train Loss: 0.0259, Train Acc: 0.9902
Val Loss: 4.6643, Val Acc: 0.5748, ROC-AUC: 0.5607
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0130
Batch 20000/107120 completed, running loss: 0.0129
Batch 30000/107120 completed, running loss: 0.0126
Batch 40000/107120 completed, running loss: 0.0126
Batch 50000/107120 completed, running loss: 0.0123
Batch 60000/107120 completed, running loss: 0.0122
Batch 70000/107120 completed, running loss: 0.0120
Batch 80000/107120 completed, running loss: 0.0118
Batch 90000/107120 completed, running loss: 0.0118
Batch 100000/107120 completed, running loss: 0.0117
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 2/50
Train Loss: 0.0117, Train Acc: 0.9956
Val Loss: 3.1467, Val Acc: 0.6593, ROC-AUC: 0.6970
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0087
Batch 20000/107120 completed, running loss: 0.0083
Batch 30000/107120 completed, running loss: 0.0085
Batch 40000/107120 completed, running loss: 0.0082
Batch 50000/107120 completed, running loss: 0.0083
Batch 60000/107120 completed, running loss: 0.0085
Batch 70000/107120 completed, running loss: 0.0086
Batch 80000/107120 completed, running loss: 0.0086
Batch 90000/107120 completed, running loss: 0.0085
Batch 100000/107120 completed, running loss: 0.0085
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 3/50
Train Loss: 0.0085, Train Acc: 0.9969
Val Loss: 3.1976, Val Acc: 0.6442, ROC-AUC: 0.6895
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0062
Batch 20000/107120 completed, running loss: 0.0062
Batch 30000/107120 completed, running loss: 0.0059
Batch 40000/107120 completed, running loss: 0.0064
Batch 50000/107120 completed, running loss: 0.0063
Batch 60000/107120 completed, running loss: 0.0063
Batch 70000/107120 completed, running loss: 0.0064
Batch 80000/107120 completed, running loss: 0.0065
Batch 90000/107120 completed, running loss: 0.0064
Batch 100000/107120 completed, running loss: 0.0065
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 4/50
Train Loss: 0.0066, Train Acc: 0.9976
Val Loss: 4.9114, Val Acc: 0.6222, ROC-AUC: 0.6465
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0049
Batch 20000/107120 completed, running loss: 0.0048
Batch 30000/107120 completed, running loss: 0.0050
Batch 40000/107120 completed, running loss: 0.0050
Batch 50000/107120 completed, running loss: 0.0050
Batch 60000/107120 completed, running loss: 0.0051
Batch 70000/107120 completed, running loss: 0.0051
Batch 80000/107120 completed, running loss: 0.0052
Batch 90000/107120 completed, running loss: 0.0054
Batch 100000/107120 completed, running loss: 0.0055
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 5/50
Train Loss: 0.0054, Train Acc: 0.9980
Val Loss: 5.4862, Val Acc: 0.6216, ROC-AUC: 0.6489
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0051
Batch 20000/107120 completed, running loss: 0.0047
Batch 30000/107120 completed, running loss: 0.0048
Batch 40000/107120 completed, running loss: 0.0047
Batch 50000/107120 completed, running loss: 0.0050
Batch 60000/107120 completed, running loss: 0.0049
Batch 70000/107120 completed, running loss: 0.0049
Batch 80000/107120 completed, running loss: 0.0049
Batch 90000/107120 completed, running loss: 0.0049
Batch 100000/107120 completed, running loss: 0.0048
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 6/50
Train Loss: 0.0048, Train Acc: 0.9983
Val Loss: 6.6353, Val Acc: 0.6024, ROC-AUC: 0.6202
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0037
Batch 20000/107120 completed, running loss: 0.0042
Batch 30000/107120 completed, running loss: 0.0041
Batch 40000/107120 completed, running loss: 0.0040
Batch 50000/107120 completed, running loss: 0.0040
Batch 60000/107120 completed, running loss: 0.0042
Batch 70000/107120 completed, running loss: 0.0043
Batch 80000/107120 completed, running loss: 0.0043
Batch 90000/107120 completed, running loss: 0.0043
Batch 100000/107120 completed, running loss: 0.0043
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 7/50
Train Loss: 0.0043, Train Acc: 0.9985
Val Loss: 7.1809, Val Acc: 0.6300, ROC-AUC: 0.6517
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0038
Batch 20000/107120 completed, running loss: 0.0037
Batch 30000/107120 completed, running loss: 0.0038
Batch 40000/107120 completed, running loss: 0.0035
Batch 50000/107120 completed, running loss: 0.0037
Batch 60000/107120 completed, running loss: 0.0037
Batch 70000/107120 completed, running loss: 0.0036
Batch 80000/107120 completed, running loss: 0.0036
Batch 90000/107120 completed, running loss: 0.0035
Batch 100000/107120 completed, running loss: 0.0036
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 8/50
Train Loss: 0.0036, Train Acc: 0.9988
Val Loss: 7.3484, Val Acc: 0.6041, ROC-AUC: 0.6186
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0030
Batch 20000/107120 completed, running loss: 0.0028
Batch 30000/107120 completed, running loss: 0.0029
Batch 40000/107120 completed, running loss: 0.0031
Batch 50000/107120 completed, running loss: 0.0034
Batch 60000/107120 completed, running loss: 0.0033
Batch 70000/107120 completed, running loss: 0.0032
Batch 80000/107120 completed, running loss: 0.0033
Batch 90000/107120 completed, running loss: 0.0033
Batch 100000/107120 completed, running loss: 0.0033
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 9/50
Train Loss: 0.0033, Train Acc: 0.9988
Val Loss: 8.5586, Val Acc: 0.6028, ROC-AUC: 0.6134
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0023
Batch 20000/107120 completed, running loss: 0.0024
Batch 30000/107120 completed, running loss: 0.0025
Batch 40000/107120 completed, running loss: 0.0025
Batch 50000/107120 completed, running loss: 0.0026
Batch 60000/107120 completed, running loss: 0.0027
Batch 70000/107120 completed, running loss: 0.0027
Batch 80000/107120 completed, running loss: 0.0028
Batch 90000/107120 completed, running loss: 0.0028
Batch 100000/107120 completed, running loss: 0.0029
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 10/50
Train Loss: 0.0029, Train Acc: 0.9991
Val Loss: 10.8844, Val Acc: 0.6019, ROC-AUC: 0.6024
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0035
Batch 20000/107120 completed, running loss: 0.0026
Batch 30000/107120 completed, running loss: 0.0027
Batch 40000/107120 completed, running loss: 0.0027
Batch 50000/107120 completed, running loss: 0.0027
Batch 60000/107120 completed, running loss: 0.0027
Batch 70000/107120 completed, running loss: 0.0027
Batch 80000/107120 completed, running loss: 0.0026
Batch 90000/107120 completed, running loss: 0.0027
Batch 100000/107120 completed, running loss: 0.0028
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 11/50
Train Loss: 0.0028, Train Acc: 0.9991
Val Loss: 9.0795, Val Acc: 0.6110, ROC-AUC: 0.6122
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0023
Batch 20000/107120 completed, running loss: 0.0025
Batch 30000/107120 completed, running loss: 0.0022
Batch 40000/107120 completed, running loss: 0.0021
Batch 50000/107120 completed, running loss: 0.0022
Batch 60000/107120 completed, running loss: 0.0023
Batch 70000/107120 completed, running loss: 0.0024
Batch 80000/107120 completed, running loss: 0.0024
Batch 90000/107120 completed, running loss: 0.0024
Batch 100000/107120 completed, running loss: 0.0024
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 12/50
Train Loss: 0.0024, Train Acc: 0.9992
Val Loss: 10.2132, Val Acc: 0.6161, ROC-AUC: 0.6330
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0025
Batch 20000/107120 completed, running loss: 0.0023
Batch 30000/107120 completed, running loss: 0.0021
Batch 40000/107120 completed, running loss: 0.0019
Batch 50000/107120 completed, running loss: 0.0021
Batch 60000/107120 completed, running loss: 0.0021
Batch 70000/107120 completed, running loss: 0.0022
Batch 80000/107120 completed, running loss: 0.0022
Batch 90000/107120 completed, running loss: 0.0022
Batch 100000/107120 completed, running loss: 0.0022
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 13/50
Train Loss: 0.0022, Train Acc: 0.9994
Val Loss: 14.2015, Val Acc: 0.5673, ROC-AUC: 0.5761
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0030
Batch 20000/107120 completed, running loss: 0.0023
Batch 30000/107120 completed, running loss: 0.0021
Batch 40000/107120 completed, running loss: 0.0020
Batch 50000/107120 completed, running loss: 0.0023
Batch 60000/107120 completed, running loss: 0.0022
Batch 70000/107120 completed, running loss: 0.0022
Batch 80000/107120 completed, running loss: 0.0022
Batch 90000/107120 completed, running loss: 0.0021
Batch 100000/107120 completed, running loss: 0.0021
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 14/50
Train Loss: 0.0021, Train Acc: 0.9994
Val Loss: 14.4091, Val Acc: 0.5823, ROC-AUC: 0.5970
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0024
Batch 20000/107120 completed, running loss: 0.0023
Batch 30000/107120 completed, running loss: 0.0021
Batch 40000/107120 completed, running loss: 0.0021
Batch 50000/107120 completed, running loss: 0.0021
Batch 60000/107120 completed, running loss: 0.0021
Batch 70000/107120 completed, running loss: 0.0020
Batch 80000/107120 completed, running loss: 0.0021
Batch 90000/107120 completed, running loss: 0.0021
Batch 100000/107120 completed, running loss: 0.0021
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 15/50
Train Loss: 0.0021, Train Acc: 0.9994
Val Loss: 17.8926, Val Acc: 0.5367, ROC-AUC: 0.5077
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0016
Batch 20000/107120 completed, running loss: 0.0019
Batch 30000/107120 completed, running loss: 0.0018
Batch 40000/107120 completed, running loss: 0.0017
Batch 50000/107120 completed, running loss: 0.0018
Batch 60000/107120 completed, running loss: 0.0017
Batch 70000/107120 completed, running loss: 0.0016
Batch 80000/107120 completed, running loss: 0.0017
Batch 90000/107120 completed, running loss: 0.0018
Batch 100000/107120 completed, running loss: 0.0019
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 16/50
Train Loss: 0.0019, Train Acc: 0.9994
Val Loss: 10.6982, Val Acc: 0.5997, ROC-AUC: 0.6093
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0014
Batch 20000/107120 completed, running loss: 0.0019
Batch 30000/107120 completed, running loss: 0.0016
Batch 40000/107120 completed, running loss: 0.0015
Batch 50000/107120 completed, running loss: 0.0015
Batch 60000/107120 completed, running loss: 0.0015
Batch 70000/107120 completed, running loss: 0.0015
Batch 80000/107120 completed, running loss: 0.0016
Batch 90000/107120 completed, running loss: 0.0015
Batch 100000/107120 completed, running loss: 0.0015
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 17/50
Train Loss: 0.0016, Train Acc: 0.9995
Val Loss: 14.1734, Val Acc: 0.5662, ROC-AUC: 0.5608
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0011
Batch 20000/107120 completed, running loss: 0.0012
Batch 30000/107120 completed, running loss: 0.0013
Batch 40000/107120 completed, running loss: 0.0012
Batch 50000/107120 completed, running loss: 0.0012
Batch 60000/107120 completed, running loss: 0.0013
Batch 70000/107120 completed, running loss: 0.0014
Batch 80000/107120 completed, running loss: 0.0013
Batch 90000/107120 completed, running loss: 0.0013
Batch 100000/107120 completed, running loss: 0.0014
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 18/50
Train Loss: 0.0014, Train Acc: 0.9996
Val Loss: 16.2977, Val Acc: 0.5766, ROC-AUC: 0.5758
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0009
Batch 20000/107120 completed, running loss: 0.0008
Batch 30000/107120 completed, running loss: 0.0007
Batch 40000/107120 completed, running loss: 0.0007
Batch 50000/107120 completed, running loss: 0.0007
Batch 60000/107120 completed, running loss: 0.0006
Batch 70000/107120 completed, running loss: 0.0007
Batch 80000/107120 completed, running loss: 0.0007
Batch 90000/107120 completed, running loss: 0.0007
Batch 100000/107120 completed, running loss: 0.0007
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 19/50
Train Loss: 0.0007, Train Acc: 0.9998
Val Loss: 24.6531, Val Acc: 0.5496, ROC-AUC: 0.5347
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0006
Batch 20000/107120 completed, running loss: 0.0006
Batch 30000/107120 completed, running loss: 0.0006
Batch 40000/107120 completed, running loss: 0.0007
Batch 50000/107120 completed, running loss: 0.0006
Batch 60000/107120 completed, running loss: 0.0006
Batch 70000/107120 completed, running loss: 0.0005
Batch 80000/107120 completed, running loss: 0.0005
Batch 90000/107120 completed, running loss: 0.0006
Batch 100000/107120 completed, running loss: 0.0006
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 20/50
Train Loss: 0.0006, Train Acc: 0.9998
Val Loss: 20.4087, Val Acc: 0.5999, ROC-AUC: 0.5960
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0008
Batch 20000/107120 completed, running loss: 0.0005
Batch 30000/107120 completed, running loss: 0.0006
Batch 40000/107120 completed, running loss: 0.0007
Batch 50000/107120 completed, running loss: 0.0006
Batch 60000/107120 completed, running loss: 0.0005
Batch 70000/107120 completed, running loss: 0.0005
Batch 80000/107120 completed, running loss: 0.0005
Batch 90000/107120 completed, running loss: 0.0005
Batch 100000/107120 completed, running loss: 0.0005
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 21/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 25.5242, Val Acc: 0.5856, ROC-AUC: 0.5751
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0006
Batch 20000/107120 completed, running loss: 0.0005
Batch 30000/107120 completed, running loss: 0.0004
Batch 40000/107120 completed, running loss: 0.0006
Batch 50000/107120 completed, running loss: 0.0006
Batch 60000/107120 completed, running loss: 0.0005
Batch 70000/107120 completed, running loss: 0.0006
Batch 80000/107120 completed, running loss: 0.0006
Batch 90000/107120 completed, running loss: 0.0006
Batch 100000/107120 completed, running loss: 0.0005
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 22/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 25.6459, Val Acc: 0.5975, ROC-AUC: 0.5878
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0006
Batch 20000/107120 completed, running loss: 0.0005
Batch 30000/107120 completed, running loss: 0.0007
Batch 40000/107120 completed, running loss: 0.0006
Batch 50000/107120 completed, running loss: 0.0006
Batch 60000/107120 completed, running loss: 0.0005
Batch 70000/107120 completed, running loss: 0.0005
Batch 80000/107120 completed, running loss: 0.0004
Batch 90000/107120 completed, running loss: 0.0005
Batch 100000/107120 completed, running loss: 0.0005
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 23/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 23.2354, Val Acc: 0.6122, ROC-AUC: 0.6098
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0002
Batch 20000/107120 completed, running loss: 0.0005
Batch 30000/107120 completed, running loss: 0.0005
Batch 40000/107120 completed, running loss: 0.0005
Batch 50000/107120 completed, running loss: 0.0005
Batch 60000/107120 completed, running loss: 0.0004
Batch 70000/107120 completed, running loss: 0.0004
Batch 80000/107120 completed, running loss: 0.0004
Batch 90000/107120 completed, running loss: 0.0005
Batch 100000/107120 completed, running loss: 0.0005
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 24/50
Train Loss: 0.0005, Train Acc: 0.9999
Val Loss: 25.2365, Val Acc: 0.5988, ROC-AUC: 0.5962
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0001
Batch 20000/107120 completed, running loss: 0.0001
Batch 30000/107120 completed, running loss: 0.0002
Batch 40000/107120 completed, running loss: 0.0002
Batch 50000/107120 completed, running loss: 0.0002
Batch 60000/107120 completed, running loss: 0.0002
Batch 70000/107120 completed, running loss: 0.0002
Batch 80000/107120 completed, running loss: 0.0002
Batch 90000/107120 completed, running loss: 0.0003
Batch 100000/107120 completed, running loss: 0.0003
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 25/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 27.8546, Val Acc: 0.5931, ROC-AUC: 0.5895
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0000
Batch 20000/107120 completed, running loss: 0.0002
Batch 30000/107120 completed, running loss: 0.0002
Batch 40000/107120 completed, running loss: 0.0002
Batch 50000/107120 completed, running loss: 0.0002
Batch 60000/107120 completed, running loss: 0.0002
Batch 70000/107120 completed, running loss: 0.0002
Batch 80000/107120 completed, running loss: 0.0002
Batch 90000/107120 completed, running loss: 0.0002
Batch 100000/107120 completed, running loss: 0.0002
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 26/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 31.3948, Val Acc: 0.5930, ROC-AUC: 0.5916
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0002
Batch 20000/107120 completed, running loss: 0.0002
Batch 30000/107120 completed, running loss: 0.0003
Batch 40000/107120 completed, running loss: 0.0003
Batch 50000/107120 completed, running loss: 0.0004
Batch 60000/107120 completed, running loss: 0.0004
Batch 70000/107120 completed, running loss: 0.0004
Batch 80000/107120 completed, running loss: 0.0003
Batch 90000/107120 completed, running loss: 0.0003
Batch 100000/107120 completed, running loss: 0.0003
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 27/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 28.5191, Val Acc: 0.6107, ROC-AUC: 0.6047
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0001
Batch 20000/107120 completed, running loss: 0.0002
Batch 30000/107120 completed, running loss: 0.0002
Batch 40000/107120 completed, running loss: 0.0002
Batch 50000/107120 completed, running loss: 0.0004
Batch 60000/107120 completed, running loss: 0.0003
Batch 70000/107120 completed, running loss: 0.0003
Batch 80000/107120 completed, running loss: 0.0003
Batch 90000/107120 completed, running loss: 0.0003
Batch 100000/107120 completed, running loss: 0.0003
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 28/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 31.7643, Val Acc: 0.5950, ROC-AUC: 0.5887
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0001
Batch 20000/107120 completed, running loss: 0.0003
Batch 30000/107120 completed, running loss: 0.0002
Batch 40000/107120 completed, running loss: 0.0002
Batch 50000/107120 completed, running loss: 0.0003
Batch 60000/107120 completed, running loss: 0.0002
Batch 70000/107120 completed, running loss: 0.0002
Batch 80000/107120 completed, running loss: 0.0002
Batch 90000/107120 completed, running loss: 0.0003
Batch 100000/107120 completed, running loss: 0.0003
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 29/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 32.5927, Val Acc: 0.5878, ROC-AUC: 0.5805
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0000
Batch 20000/107120 completed, running loss: 0.0000
Batch 30000/107120 completed, running loss: 0.0001
Batch 40000/107120 completed, running loss: 0.0000
Batch 50000/107120 completed, running loss: 0.0001
Batch 60000/107120 completed, running loss: 0.0000
Batch 70000/107120 completed, running loss: 0.0001
Batch 80000/107120 completed, running loss: 0.0001
Batch 90000/107120 completed, running loss: 0.0001
Batch 100000/107120 completed, running loss: 0.0001
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 30/50
Train Loss: 0.0001, Train Acc: 1.0000
Val Loss: 36.8642, Val Acc: 0.5972, ROC-AUC: 0.5940
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0004
Batch 20000/107120 completed, running loss: 0.0004
Batch 30000/107120 completed, running loss: 0.0004
Batch 40000/107120 completed, running loss: 0.0003
Batch 50000/107120 completed, running loss: 0.0003
Batch 60000/107120 completed, running loss: 0.0003
Batch 70000/107120 completed, running loss: 0.0003
Batch 80000/107120 completed, running loss: 0.0002
Batch 90000/107120 completed, running loss: 0.0003
Batch 100000/107120 completed, running loss: 0.0003
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 31/50
Train Loss: 0.0003, Train Acc: 0.9999
Val Loss: 35.1091, Val Acc: 0.6039, ROC-AUC: 0.6027
Starting training epoch with 107120 batches
Batch 10000/107120 completed, running loss: 0.0000
Batch 20000/107120 completed, running loss: 0.0002
Batch 30000/107120 completed, running loss: 0.0003
Batch 40000/107120 completed, running loss: 0.0004
Batch 50000/107120 completed, running loss: 0.0003
Batch 60000/107120 completed, running loss: 0.0003
Batch 70000/107120 completed, running loss: 0.0003
Batch 80000/107120 completed, running loss: 0.0003
Batch 90000/107120 completed, running loss: 0.0003
Batch 100000/107120 completed, running loss: 0.0003
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Fold 7 - Epoch 32/50
Train Loss: 0.0003, Train Acc: 1.0000
Val Loss: 41.0395, Val Acc: 0.5875, ROC-AUC: 0.5841
Fold 7 - Early stopping triggered
Starting evaluation with 107120 batches
Evaluation batch 10000/107120 completed
Evaluation batch 20000/107120 completed
Evaluation batch 30000/107120 completed
Evaluation batch 40000/107120 completed
Evaluation batch 50000/107120 completed
Evaluation batch 60000/107120 completed
Evaluation batch 70000/107120 completed
Evaluation batch 80000/107120 completed
Evaluation batch 90000/107120 completed
Evaluation batch 100000/107120 completed
Starting evaluation with 17455 batches
Evaluation batch 10000/17455 completed
Starting evaluation with 11344 batches
Evaluation batch 10000/11344 completed
Fold 7 - Test Metrics:
Loss: 4.6293
Accuracy: 0.4408
Precision: 0.6014
Recall: 0.4729
Roc_auc: 0.4049
Specificity: 0.3771
F1: 0.5294

Starting Fold 8/10
Fold 8 - Training label counts: {1: 232930, 0: 197610}
Fold 8 - Validation label counts: {1: 39118, 0: 34134}
Fold 8 - Test label counts: {0: 27184, 1: 12694}
Preparing dataset with 430540 samples
Extracting features
Normalizing embeddings
Dataset ready with 430540 samples
Preparing dataset with 73252 samples
Extracting features
Normalizing embeddings
Dataset ready with 73252 samples
Preparing dataset with 39878 samples
Extracting features
Normalizing embeddings
Dataset ready with 39878 samples
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0843
Batch 20000/107635 completed, running loss: 0.0620
Batch 30000/107635 completed, running loss: 0.0519
Batch 40000/107635 completed, running loss: 0.0455
Batch 50000/107635 completed, running loss: 0.0413
Batch 60000/107635 completed, running loss: 0.0382
Batch 70000/107635 completed, running loss: 0.0360
Batch 80000/107635 completed, running loss: 0.0342
Batch 90000/107635 completed, running loss: 0.0326
Batch 100000/107635 completed, running loss: 0.0313
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 1/50
Train Loss: 0.0302, Train Acc: 0.9888
Val Loss: 2.4894, Val Acc: 0.6543, ROC-AUC: 0.6954
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0135
Batch 20000/107635 completed, running loss: 0.0135
Batch 30000/107635 completed, running loss: 0.0137
Batch 40000/107635 completed, running loss: 0.0137
Batch 50000/107635 completed, running loss: 0.0134
Batch 60000/107635 completed, running loss: 0.0134
Batch 70000/107635 completed, running loss: 0.0133
Batch 80000/107635 completed, running loss: 0.0132
Batch 90000/107635 completed, running loss: 0.0129
Batch 100000/107635 completed, running loss: 0.0128
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 2/50
Train Loss: 0.0129, Train Acc: 0.9953
Val Loss: 2.7956, Val Acc: 0.6586, ROC-AUC: 0.6895
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0094
Batch 20000/107635 completed, running loss: 0.0088
Batch 30000/107635 completed, running loss: 0.0089
Batch 40000/107635 completed, running loss: 0.0090
Batch 50000/107635 completed, running loss: 0.0090
Batch 60000/107635 completed, running loss: 0.0091
Batch 70000/107635 completed, running loss: 0.0093
Batch 80000/107635 completed, running loss: 0.0092
Batch 90000/107635 completed, running loss: 0.0091
Batch 100000/107635 completed, running loss: 0.0092
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 3/50
Train Loss: 0.0091, Train Acc: 0.9968
Val Loss: 4.0767, Val Acc: 0.6194, ROC-AUC: 0.6431
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0070
Batch 20000/107635 completed, running loss: 0.0070
Batch 30000/107635 completed, running loss: 0.0071
Batch 40000/107635 completed, running loss: 0.0072
Batch 50000/107635 completed, running loss: 0.0071
Batch 60000/107635 completed, running loss: 0.0070
Batch 70000/107635 completed, running loss: 0.0071
Batch 80000/107635 completed, running loss: 0.0071
Batch 90000/107635 completed, running loss: 0.0070
Batch 100000/107635 completed, running loss: 0.0070
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 4/50
Train Loss: 0.0071, Train Acc: 0.9975
Val Loss: 4.7509, Val Acc: 0.6251, ROC-AUC: 0.6350
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0054
Batch 20000/107635 completed, running loss: 0.0053
Batch 30000/107635 completed, running loss: 0.0053
Batch 40000/107635 completed, running loss: 0.0054
Batch 50000/107635 completed, running loss: 0.0057
Batch 60000/107635 completed, running loss: 0.0058
Batch 70000/107635 completed, running loss: 0.0058
Batch 80000/107635 completed, running loss: 0.0059
Batch 90000/107635 completed, running loss: 0.0058
Batch 100000/107635 completed, running loss: 0.0058
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 5/50
Train Loss: 0.0058, Train Acc: 0.9979
Val Loss: 4.8832, Val Acc: 0.6482, ROC-AUC: 0.6829
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0042
Batch 20000/107635 completed, running loss: 0.0044
Batch 30000/107635 completed, running loss: 0.0047
Batch 40000/107635 completed, running loss: 0.0049
Batch 50000/107635 completed, running loss: 0.0048
Batch 60000/107635 completed, running loss: 0.0049
Batch 70000/107635 completed, running loss: 0.0051
Batch 80000/107635 completed, running loss: 0.0051
Batch 90000/107635 completed, running loss: 0.0051
Batch 100000/107635 completed, running loss: 0.0051
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 6/50
Train Loss: 0.0052, Train Acc: 0.9982
Val Loss: 5.5966, Val Acc: 0.5741, ROC-AUC: 0.5579
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0047
Batch 20000/107635 completed, running loss: 0.0043
Batch 30000/107635 completed, running loss: 0.0041
Batch 40000/107635 completed, running loss: 0.0041
Batch 50000/107635 completed, running loss: 0.0041
Batch 60000/107635 completed, running loss: 0.0040
Batch 70000/107635 completed, running loss: 0.0041
Batch 80000/107635 completed, running loss: 0.0040
Batch 90000/107635 completed, running loss: 0.0041
Batch 100000/107635 completed, running loss: 0.0042
Starting evaluation with 18313 batches
Evaluation batch 10000/18313 completed
Fold 8 - Epoch 7/50
Train Loss: 0.0042, Train Acc: 0.9986
Val Loss: 5.6299, Val Acc: 0.6265, ROC-AUC: 0.6490
Starting training epoch with 107635 batches
Batch 10000/107635 completed, running loss: 0.0031
Batch 20000/107635 completed, running loss: 0.0038
Batch 30000/107635 completed, running loss: 0.0035
Batch 40000/107635 completed, running loss: 0.0035
