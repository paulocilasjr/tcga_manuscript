Setting random seed to 42
Using device: cuda
Loading CSV from ./../../../../tiles_embedding_extraction/vgg11/pytorch_format/tcga_embedding_vgg11_label.csv
Preparing 10-fold cross-validation

=== Fold 1/10 ===
Fold 1 - Training label counts: {0: 118188, 1: 111001}
Fold 1 - Validation label counts: {0: 18130, 1: 12124}
Fold 1 - Test label counts: {0: 15833, 1: 14602}
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.6384
Batch 20000/57298 completed, running loss: 0.6215
Batch 30000/57298 completed, running loss: 0.6121
Batch 40000/57298 completed, running loss: 0.6032
Batch 50000/57298 completed, running loss: 0.5967
Starting evaluation with 7564 batches
Fold 1 - Epoch 1/50
Train Loss: 0.5924, Train Acc: 0.6957
Val Loss: 0.8776, Val Acc: 0.6057, ROC-AUC: 0.6515
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.4738
Batch 20000/57298 completed, running loss: 0.4824
Batch 30000/57298 completed, running loss: 0.4856
Batch 40000/57298 completed, running loss: 0.4886
Batch 50000/57298 completed, running loss: 0.4917
Starting evaluation with 7564 batches
Fold 1 - Epoch 2/50
Train Loss: 0.4937, Train Acc: 0.7960
Val Loss: 1.1751, Val Acc: 0.5919, ROC-AUC: 0.6407
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.3686
Batch 20000/57298 completed, running loss: 0.3881
Batch 30000/57298 completed, running loss: 0.3982
Batch 40000/57298 completed, running loss: 0.4067
Batch 50000/57298 completed, running loss: 0.4148
Starting evaluation with 7564 batches
Fold 1 - Epoch 3/50
Train Loss: 0.4191, Train Acc: 0.8581
Val Loss: 1.7614, Val Acc: 0.5956, ROC-AUC: 0.6337
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2901
Batch 20000/57298 completed, running loss: 0.3083
Batch 30000/57298 completed, running loss: 0.3195
Batch 40000/57298 completed, running loss: 0.3343
Batch 50000/57298 completed, running loss: 0.3430
Starting evaluation with 7564 batches
Fold 1 - Epoch 4/50
Train Loss: 0.3484, Train Acc: 0.9000
Val Loss: 2.6338, Val Acc: 0.5884, ROC-AUC: 0.6262
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2392
Batch 20000/57298 completed, running loss: 0.2587
Batch 30000/57298 completed, running loss: 0.2703
Batch 40000/57298 completed, running loss: 0.2834
Batch 50000/57298 completed, running loss: 0.2930
Starting evaluation with 7564 batches
Fold 1 - Epoch 5/50
Train Loss: 0.2971, Train Acc: 0.9256
Val Loss: 3.6096, Val Acc: 0.5761, ROC-AUC: 0.6219
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2122
Batch 20000/57298 completed, running loss: 0.2302
Batch 30000/57298 completed, running loss: 0.2439
Batch 40000/57298 completed, running loss: 0.2524
Batch 50000/57298 completed, running loss: 0.2593
Starting evaluation with 7564 batches
Fold 1 - Epoch 6/50
Train Loss: 0.2648, Train Acc: 0.9409
Val Loss: 4.2471, Val Acc: 0.5812, ROC-AUC: 0.6226
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1888
Batch 20000/57298 completed, running loss: 0.2034
Batch 30000/57298 completed, running loss: 0.2138
Batch 40000/57298 completed, running loss: 0.2220
Batch 50000/57298 completed, running loss: 0.2318
Starting evaluation with 7564 batches
Fold 1 - Epoch 7/50
Train Loss: 0.2358, Train Acc: 0.9518
Val Loss: 5.3567, Val Acc: 0.5817, ROC-AUC: 0.6213
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1747
Batch 20000/57298 completed, running loss: 0.1882
Batch 30000/57298 completed, running loss: 0.1983
Batch 40000/57298 completed, running loss: 0.2053
Batch 50000/57298 completed, running loss: 0.2140
Starting evaluation with 7564 batches
Fold 1 - Epoch 8/50
Train Loss: 0.2207, Train Acc: 0.9579
Val Loss: 5.4142, Val Acc: 0.5756, ROC-AUC: 0.6144
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1705
Batch 20000/57298 completed, running loss: 0.1803
Batch 30000/57298 completed, running loss: 0.1871
Batch 40000/57298 completed, running loss: 0.1924
Batch 50000/57298 completed, running loss: 0.2006
Starting evaluation with 7564 batches
Fold 1 - Epoch 9/50
Train Loss: 0.2036, Train Acc: 0.9625
Val Loss: 6.1409, Val Acc: 0.5696, ROC-AUC: 0.6108
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1686
Batch 20000/57298 completed, running loss: 0.1714
Batch 30000/57298 completed, running loss: 0.1781
Batch 40000/57298 completed, running loss: 0.1834
Batch 50000/57298 completed, running loss: 0.1884
Starting evaluation with 7564 batches
Fold 1 - Epoch 10/50
Train Loss: 0.1930, Train Acc: 0.9659
Val Loss: 6.2467, Val Acc: 0.5721, ROC-AUC: 0.6121
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1440
Batch 20000/57298 completed, running loss: 0.1564
Batch 30000/57298 completed, running loss: 0.1664
Batch 40000/57298 completed, running loss: 0.1728
Batch 50000/57298 completed, running loss: 0.1751
Starting evaluation with 7564 batches
Fold 1 - Epoch 11/50
Train Loss: 0.1787, Train Acc: 0.9692
Val Loss: 6.6444, Val Acc: 0.5870, ROC-AUC: 0.6290
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1495
Batch 20000/57298 completed, running loss: 0.1561
Batch 30000/57298 completed, running loss: 0.1587
Batch 40000/57298 completed, running loss: 0.1655
Batch 50000/57298 completed, running loss: 0.1706
Starting evaluation with 7564 batches
Fold 1 - Epoch 12/50
Train Loss: 0.1728, Train Acc: 0.9712
Val Loss: 6.7936, Val Acc: 0.5880, ROC-AUC: 0.6265
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1404
Batch 20000/57298 completed, running loss: 0.1485
Batch 30000/57298 completed, running loss: 0.1493
Batch 40000/57298 completed, running loss: 0.1559
Batch 50000/57298 completed, running loss: 0.1624
Starting evaluation with 7564 batches
Fold 1 - Epoch 13/50
Train Loss: 0.1660, Train Acc: 0.9735
Val Loss: 6.8929, Val Acc: 0.5807, ROC-AUC: 0.6173
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1416
Batch 20000/57298 completed, running loss: 0.1432
Batch 30000/57298 completed, running loss: 0.1482
Batch 40000/57298 completed, running loss: 0.1506
Batch 50000/57298 completed, running loss: 0.1557
Starting evaluation with 7564 batches
Fold 1 - Epoch 14/50
Train Loss: 0.1601, Train Acc: 0.9747
Val Loss: 7.3404, Val Acc: 0.5805, ROC-AUC: 0.6241
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1231
Batch 20000/57298 completed, running loss: 0.1374
Batch 30000/57298 completed, running loss: 0.1447
Batch 40000/57298 completed, running loss: 0.1473
Batch 50000/57298 completed, running loss: 0.1516
Starting evaluation with 7564 batches
Fold 1 - Epoch 15/50
Train Loss: 0.1539, Train Acc: 0.9764
Val Loss: 7.4717, Val Acc: 0.5920, ROC-AUC: 0.6273
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1237
Batch 20000/57298 completed, running loss: 0.1247
Batch 30000/57298 completed, running loss: 0.1325
Batch 40000/57298 completed, running loss: 0.1373
Batch 50000/57298 completed, running loss: 0.1424
Starting evaluation with 7564 batches
Fold 1 - Epoch 16/50
Train Loss: 0.1440, Train Acc: 0.9776
Val Loss: 7.7520, Val Acc: 0.5833, ROC-AUC: 0.6237
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1250
Batch 20000/57298 completed, running loss: 0.1346
Batch 30000/57298 completed, running loss: 0.1399
Batch 40000/57298 completed, running loss: 0.1399
Batch 50000/57298 completed, running loss: 0.1419
Starting evaluation with 7564 batches
Fold 1 - Epoch 17/50
Train Loss: 0.1430, Train Acc: 0.9780
Val Loss: 8.2261, Val Acc: 0.5759, ROC-AUC: 0.6173
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0960
Batch 20000/57298 completed, running loss: 0.0967
Batch 30000/57298 completed, running loss: 0.0941
Batch 40000/57298 completed, running loss: 0.0939
Batch 50000/57298 completed, running loss: 0.0946
Starting evaluation with 7564 batches
Fold 1 - Epoch 18/50
Train Loss: 0.0941, Train Acc: 0.9858
Val Loss: 8.5989, Val Acc: 0.5871, ROC-AUC: 0.6239
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0797
Batch 20000/57298 completed, running loss: 0.0777
Batch 30000/57298 completed, running loss: 0.0800
Batch 40000/57298 completed, running loss: 0.0811
Batch 50000/57298 completed, running loss: 0.0790
Starting evaluation with 7564 batches
Fold 1 - Epoch 19/50
Train Loss: 0.0794, Train Acc: 0.9877
Val Loss: 9.0063, Val Acc: 0.5839, ROC-AUC: 0.6184
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0693
Batch 20000/57298 completed, running loss: 0.0715
Batch 30000/57298 completed, running loss: 0.0717
Batch 40000/57298 completed, running loss: 0.0726
Batch 50000/57298 completed, running loss: 0.0728
Starting evaluation with 7564 batches
Fold 1 - Epoch 20/50
Train Loss: 0.0731, Train Acc: 0.9891
Val Loss: 9.4910, Val Acc: 0.5777, ROC-AUC: 0.6157
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0644
Batch 20000/57298 completed, running loss: 0.0666
Batch 30000/57298 completed, running loss: 0.0651
Batch 40000/57298 completed, running loss: 0.0682
Batch 50000/57298 completed, running loss: 0.0711
Starting evaluation with 7564 batches
Fold 1 - Epoch 21/50
Train Loss: 0.0715, Train Acc: 0.9899
Val Loss: 9.3985, Val Acc: 0.5812, ROC-AUC: 0.6193
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0633
Batch 20000/57298 completed, running loss: 0.0638
Batch 30000/57298 completed, running loss: 0.0684
Batch 40000/57298 completed, running loss: 0.0697
Batch 50000/57298 completed, running loss: 0.0712
Starting evaluation with 7564 batches
Fold 1 - Epoch 22/50
Train Loss: 0.0698, Train Acc: 0.9903
Val Loss: 9.5117, Val Acc: 0.5902, ROC-AUC: 0.6228
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0632
Batch 20000/57298 completed, running loss: 0.0646
Batch 30000/57298 completed, running loss: 0.0656
Batch 40000/57298 completed, running loss: 0.0645
Batch 50000/57298 completed, running loss: 0.0650
Starting evaluation with 7564 batches
Fold 1 - Epoch 23/50
Train Loss: 0.0669, Train Acc: 0.9909
Val Loss: 10.0817, Val Acc: 0.5855, ROC-AUC: 0.6198
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0534
Batch 20000/57298 completed, running loss: 0.0577
Batch 30000/57298 completed, running loss: 0.0608
Batch 40000/57298 completed, running loss: 0.0627
Batch 50000/57298 completed, running loss: 0.0629
Starting evaluation with 7564 batches
Fold 1 - Epoch 24/50
Train Loss: 0.0632, Train Acc: 0.9910
Val Loss: 10.3585, Val Acc: 0.5861, ROC-AUC: 0.6228
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0733
Batch 20000/57298 completed, running loss: 0.0655
Batch 30000/57298 completed, running loss: 0.0650
Batch 40000/57298 completed, running loss: 0.0647
Batch 50000/57298 completed, running loss: 0.0654
Starting evaluation with 7564 batches
Fold 1 - Epoch 25/50
Train Loss: 0.0653, Train Acc: 0.9910
Val Loss: 10.3434, Val Acc: 0.5893, ROC-AUC: 0.6199
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0577
Batch 20000/57298 completed, running loss: 0.0594
Batch 30000/57298 completed, running loss: 0.0611
Batch 40000/57298 completed, running loss: 0.0604
Batch 50000/57298 completed, running loss: 0.0609
Starting evaluation with 7564 batches
Fold 1 - Epoch 26/50
Train Loss: 0.0622, Train Acc: 0.9913
Val Loss: 11.0064, Val Acc: 0.5852, ROC-AUC: 0.6211
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0564
Batch 20000/57298 completed, running loss: 0.0532
Batch 30000/57298 completed, running loss: 0.0588
Batch 40000/57298 completed, running loss: 0.0602
Batch 50000/57298 completed, running loss: 0.0618
Starting evaluation with 7564 batches
Fold 1 - Epoch 27/50
Train Loss: 0.0627, Train Acc: 0.9915
Val Loss: 10.6729, Val Acc: 0.5847, ROC-AUC: 0.6183
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0620
Batch 20000/57298 completed, running loss: 0.0581
Batch 30000/57298 completed, running loss: 0.0604
Batch 40000/57298 completed, running loss: 0.0616
Batch 50000/57298 completed, running loss: 0.0620
Starting evaluation with 7564 batches
Fold 1 - Epoch 28/50
Train Loss: 0.0622, Train Acc: 0.9918
Val Loss: 10.8620, Val Acc: 0.5804, ROC-AUC: 0.6147
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0609
Batch 20000/57298 completed, running loss: 0.0594
Batch 30000/57298 completed, running loss: 0.0588
Batch 40000/57298 completed, running loss: 0.0592
Batch 50000/57298 completed, running loss: 0.0608
Starting evaluation with 7564 batches
Fold 1 - Epoch 29/50
Train Loss: 0.0609, Train Acc: 0.9919
Val Loss: 10.7153, Val Acc: 0.5924, ROC-AUC: 0.6249
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0633
Batch 20000/57298 completed, running loss: 0.0619
Batch 30000/57298 completed, running loss: 0.0604
Batch 40000/57298 completed, running loss: 0.0604
Batch 50000/57298 completed, running loss: 0.0600
Starting evaluation with 7564 batches
Fold 1 - Epoch 30/50
Train Loss: 0.0597, Train Acc: 0.9916
Val Loss: 10.9052, Val Acc: 0.5918, ROC-AUC: 0.6251
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0618
Batch 20000/57298 completed, running loss: 0.0578
Batch 30000/57298 completed, running loss: 0.0598
Batch 40000/57298 completed, running loss: 0.0584
Batch 50000/57298 completed, running loss: 0.0594
Starting evaluation with 7564 batches
Fold 1 - Epoch 31/50
Train Loss: 0.0591, Train Acc: 0.9920
Val Loss: 10.8602, Val Acc: 0.5912, ROC-AUC: 0.6251
Fold 1 - Early stopping triggered
Starting evaluation with 57298 batches
Evaluation batch 10000/57298 completed
Evaluation batch 20000/57298 completed
Evaluation batch 30000/57298 completed
Evaluation batch 40000/57298 completed
Evaluation batch 50000/57298 completed
Starting evaluation with 7564 batches
Starting evaluation with 7609 batches

Fold 1 - Train Metrics:
  Loss: 0.3923
  Accuracy: 0.8136
  Precision: 0.8231
  Recall: 0.7837
  Roc_auc: 0.9034
  Specificity: 0.8418
  F1: 0.8029

Fold 1 - Validation Metrics:
  Loss: 0.8776
  Accuracy: 0.6057
  Precision: 0.5068
  Recall: 0.6030
  Roc_auc: 0.6515
  Specificity: 0.6075
  F1: 0.5507

Fold 1 - Test Metrics:
  Loss: 0.8505
  Accuracy: 0.5943
  Precision: 0.5792
  Recall: 0.5647
  Roc_auc: 0.6479
  Specificity: 0.6216
  F1: 0.5719

=== Fold 2/10 ===
Fold 2 - Training label counts: {0: 121495, 1: 106525}
Fold 2 - Validation label counts: {1: 17498, 0: 15987}
Fold 2 - Test label counts: {0: 14669, 1: 13704}
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.6320
Batch 20000/57005 completed, running loss: 0.6158
Batch 30000/57005 completed, running loss: 0.6062
Batch 40000/57005 completed, running loss: 0.5993
Batch 50000/57005 completed, running loss: 0.5933
Starting evaluation with 8372 batches
Fold 2 - Epoch 1/50
Train Loss: 0.5898, Train Acc: 0.6969
Val Loss: 0.9333, Val Acc: 0.5671, ROC-AUC: 0.6158
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.4679
Batch 20000/57005 completed, running loss: 0.4765
Batch 30000/57005 completed, running loss: 0.4816
Batch 40000/57005 completed, running loss: 0.4878
Batch 50000/57005 completed, running loss: 0.4915
Starting evaluation with 8372 batches
Fold 2 - Epoch 2/50
Train Loss: 0.4929, Train Acc: 0.7972
Val Loss: 1.2078, Val Acc: 0.5833, ROC-AUC: 0.6169
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.3669
Batch 20000/57005 completed, running loss: 0.3798
Batch 30000/57005 completed, running loss: 0.3903
Batch 40000/57005 completed, running loss: 0.4002
Batch 50000/57005 completed, running loss: 0.4101
Starting evaluation with 8372 batches
Fold 2 - Epoch 3/50
Train Loss: 0.4147, Train Acc: 0.8610
Val Loss: 1.8105, Val Acc: 0.5786, ROC-AUC: 0.6181
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2779
Batch 20000/57005 completed, running loss: 0.2987
Batch 30000/57005 completed, running loss: 0.3178
Batch 40000/57005 completed, running loss: 0.3304
Batch 50000/57005 completed, running loss: 0.3417
Starting evaluation with 8372 batches
Fold 2 - Epoch 4/50
Train Loss: 0.3469, Train Acc: 0.9027
Val Loss: 2.8525, Val Acc: 0.5620, ROC-AUC: 0.5921
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2372
Batch 20000/57005 completed, running loss: 0.2548
Batch 30000/57005 completed, running loss: 0.2670
Batch 40000/57005 completed, running loss: 0.2785
Batch 50000/57005 completed, running loss: 0.2897
Starting evaluation with 8372 batches
Fold 2 - Epoch 5/50
Train Loss: 0.2950, Train Acc: 0.9282
Val Loss: 3.6897, Val Acc: 0.5687, ROC-AUC: 0.5969
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2092
Batch 20000/57005 completed, running loss: 0.2261
Batch 30000/57005 completed, running loss: 0.2344
Batch 40000/57005 completed, running loss: 0.2456
Batch 50000/57005 completed, running loss: 0.2536
Starting evaluation with 8372 batches
Fold 2 - Epoch 6/50
Train Loss: 0.2590, Train Acc: 0.9428
Val Loss: 4.5169, Val Acc: 0.5716, ROC-AUC: 0.6028
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1958
Batch 20000/57005 completed, running loss: 0.2091
Batch 30000/57005 completed, running loss: 0.2169
Batch 40000/57005 completed, running loss: 0.2253
Batch 50000/57005 completed, running loss: 0.2344
Starting evaluation with 8372 batches
Fold 2 - Epoch 7/50
Train Loss: 0.2395, Train Acc: 0.9518
Val Loss: 5.1124, Val Acc: 0.5653, ROC-AUC: 0.5957
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1781
Batch 20000/57005 completed, running loss: 0.1810
Batch 30000/57005 completed, running loss: 0.1955
Batch 40000/57005 completed, running loss: 0.2016
Batch 50000/57005 completed, running loss: 0.2112
Starting evaluation with 8372 batches
Fold 2 - Epoch 8/50
Train Loss: 0.2157, Train Acc: 0.9583
Val Loss: 5.7008, Val Acc: 0.5687, ROC-AUC: 0.5981
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1722
Batch 20000/57005 completed, running loss: 0.1718
Batch 30000/57005 completed, running loss: 0.1828
Batch 40000/57005 completed, running loss: 0.1899
Batch 50000/57005 completed, running loss: 0.1950
Starting evaluation with 8372 batches
Fold 2 - Epoch 9/50
Train Loss: 0.1994, Train Acc: 0.9635
Val Loss: 6.2722, Val Acc: 0.5705, ROC-AUC: 0.6002
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1552
Batch 20000/57005 completed, running loss: 0.1681
Batch 30000/57005 completed, running loss: 0.1741
Batch 40000/57005 completed, running loss: 0.1820
Batch 50000/57005 completed, running loss: 0.1862
Starting evaluation with 8372 batches
Fold 2 - Epoch 10/50
Train Loss: 0.1919, Train Acc: 0.9672
Val Loss: 6.6405, Val Acc: 0.5797, ROC-AUC: 0.6101
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1526
Batch 20000/57005 completed, running loss: 0.1609
Batch 30000/57005 completed, running loss: 0.1683
Batch 40000/57005 completed, running loss: 0.1726
Batch 50000/57005 completed, running loss: 0.1759
Starting evaluation with 8372 batches
Fold 2 - Epoch 11/50
Train Loss: 0.1819, Train Acc: 0.9697
Val Loss: 6.9897, Val Acc: 0.5663, ROC-AUC: 0.5931
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1502
Batch 20000/57005 completed, running loss: 0.1608
Batch 30000/57005 completed, running loss: 0.1652
Batch 40000/57005 completed, running loss: 0.1696
Batch 50000/57005 completed, running loss: 0.1740
Starting evaluation with 8372 batches
Fold 2 - Epoch 12/50
Train Loss: 0.1763, Train Acc: 0.9715
Val Loss: 7.2438, Val Acc: 0.5786, ROC-AUC: 0.6109
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1360
Batch 20000/57005 completed, running loss: 0.1445
Batch 30000/57005 completed, running loss: 0.1477
Batch 40000/57005 completed, running loss: 0.1523
Batch 50000/57005 completed, running loss: 0.1551
Starting evaluation with 8372 batches
Fold 2 - Epoch 13/50
Train Loss: 0.1590, Train Acc: 0.9741
Val Loss: 7.2647, Val Acc: 0.5836, ROC-AUC: 0.6146
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1360
Batch 20000/57005 completed, running loss: 0.1421
Batch 30000/57005 completed, running loss: 0.1460
Batch 40000/57005 completed, running loss: 0.1500
Batch 50000/57005 completed, running loss: 0.1534
Starting evaluation with 8372 batches
Fold 2 - Epoch 14/50
Train Loss: 0.1566, Train Acc: 0.9754
Val Loss: 7.7135, Val Acc: 0.5725, ROC-AUC: 0.5969
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1310
Batch 20000/57005 completed, running loss: 0.1385
Batch 30000/57005 completed, running loss: 0.1447
Batch 40000/57005 completed, running loss: 0.1497
Batch 50000/57005 completed, running loss: 0.1515
Starting evaluation with 8372 batches
Fold 2 - Epoch 15/50
Train Loss: 0.1527, Train Acc: 0.9766
Val Loss: 7.8136, Val Acc: 0.5727, ROC-AUC: 0.5994
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1191
Batch 20000/57005 completed, running loss: 0.1276
Batch 30000/57005 completed, running loss: 0.1332
Batch 40000/57005 completed, running loss: 0.1368
Batch 50000/57005 completed, running loss: 0.1401
Starting evaluation with 8372 batches
Fold 2 - Epoch 16/50
Train Loss: 0.1440, Train Acc: 0.9778
Val Loss: 8.5851, Val Acc: 0.5672, ROC-AUC: 0.5918
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1245
Batch 20000/57005 completed, running loss: 0.1320
Batch 30000/57005 completed, running loss: 0.1314
Batch 40000/57005 completed, running loss: 0.1375
Batch 50000/57005 completed, running loss: 0.1398
Starting evaluation with 8372 batches
Fold 2 - Epoch 17/50
Train Loss: 0.1425, Train Acc: 0.9784
Val Loss: 8.4003, Val Acc: 0.5703, ROC-AUC: 0.5930
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1095
Batch 20000/57005 completed, running loss: 0.1214
Batch 30000/57005 completed, running loss: 0.1270
Batch 40000/57005 completed, running loss: 0.1305
Batch 50000/57005 completed, running loss: 0.1334
Starting evaluation with 8372 batches
Fold 2 - Epoch 18/50
Train Loss: 0.1345, Train Acc: 0.9798
Val Loss: 8.6292, Val Acc: 0.5654, ROC-AUC: 0.5891
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1118
Batch 20000/57005 completed, running loss: 0.1204
Batch 30000/57005 completed, running loss: 0.1223
Batch 40000/57005 completed, running loss: 0.1300
Batch 50000/57005 completed, running loss: 0.1336
Starting evaluation with 8372 batches
Fold 2 - Epoch 19/50
Train Loss: 0.1334, Train Acc: 0.9801
Val Loss: 9.6970, Val Acc: 0.5630, ROC-AUC: 0.5943
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0907
Batch 20000/57005 completed, running loss: 0.0924
Batch 30000/57005 completed, running loss: 0.0884
Batch 40000/57005 completed, running loss: 0.0903
Batch 50000/57005 completed, running loss: 0.0904
Starting evaluation with 8372 batches
Fold 2 - Epoch 20/50
Train Loss: 0.0901, Train Acc: 0.9866
Val Loss: 9.4620, Val Acc: 0.5711, ROC-AUC: 0.5943
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0734
Batch 20000/57005 completed, running loss: 0.0731
Batch 30000/57005 completed, running loss: 0.0719
Batch 40000/57005 completed, running loss: 0.0716
Batch 50000/57005 completed, running loss: 0.0740
Starting evaluation with 8372 batches
Fold 2 - Epoch 21/50
Train Loss: 0.0750, Train Acc: 0.9891
Val Loss: 9.8024, Val Acc: 0.5726, ROC-AUC: 0.5984
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0737
Batch 20000/57005 completed, running loss: 0.0736
Batch 30000/57005 completed, running loss: 0.0689
Batch 40000/57005 completed, running loss: 0.0676
Batch 50000/57005 completed, running loss: 0.0674
Starting evaluation with 8372 batches
Fold 2 - Epoch 22/50
Train Loss: 0.0693, Train Acc: 0.9898
Val Loss: 10.6008, Val Acc: 0.5674, ROC-AUC: 0.5944
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0623
Batch 20000/57005 completed, running loss: 0.0643
Batch 30000/57005 completed, running loss: 0.0654
Batch 40000/57005 completed, running loss: 0.0667
Batch 50000/57005 completed, running loss: 0.0680
Starting evaluation with 8372 batches
Fold 2 - Epoch 23/50
Train Loss: 0.0697, Train Acc: 0.9905
Val Loss: 10.5063, Val Acc: 0.5703, ROC-AUC: 0.5957
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0573
Batch 20000/57005 completed, running loss: 0.0596
Batch 30000/57005 completed, running loss: 0.0609
Batch 40000/57005 completed, running loss: 0.0602
Batch 50000/57005 completed, running loss: 0.0626
Starting evaluation with 8372 batches
Fold 2 - Epoch 24/50
Train Loss: 0.0628, Train Acc: 0.9912
Val Loss: 10.9258, Val Acc: 0.5720, ROC-AUC: 0.5967
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0654
Batch 20000/57005 completed, running loss: 0.0698
Batch 30000/57005 completed, running loss: 0.0659
Batch 40000/57005 completed, running loss: 0.0652
Batch 50000/57005 completed, running loss: 0.0642
Starting evaluation with 8372 batches
Fold 2 - Epoch 25/50
Train Loss: 0.0634, Train Acc: 0.9913
Val Loss: 11.0481, Val Acc: 0.5662, ROC-AUC: 0.5905
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0543
Batch 20000/57005 completed, running loss: 0.0568
Batch 30000/57005 completed, running loss: 0.0582
Batch 40000/57005 completed, running loss: 0.0611
Batch 50000/57005 completed, running loss: 0.0612
Starting evaluation with 8372 batches
Fold 2 - Epoch 26/50
Train Loss: 0.0612, Train Acc: 0.9917
Val Loss: 11.1993, Val Acc: 0.5698, ROC-AUC: 0.5941
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0652
Batch 20000/57005 completed, running loss: 0.0644
Batch 30000/57005 completed, running loss: 0.0657
Batch 40000/57005 completed, running loss: 0.0650
Batch 50000/57005 completed, running loss: 0.0644
Starting evaluation with 8372 batches
Fold 2 - Epoch 27/50
Train Loss: 0.0641, Train Acc: 0.9914
Val Loss: 11.0819, Val Acc: 0.5729, ROC-AUC: 0.5968
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0569
Batch 20000/57005 completed, running loss: 0.0596
Batch 30000/57005 completed, running loss: 0.0588
Batch 40000/57005 completed, running loss: 0.0589
Batch 50000/57005 completed, running loss: 0.0586
Starting evaluation with 8372 batches
Fold 2 - Epoch 28/50
Train Loss: 0.0596, Train Acc: 0.9919
Val Loss: 11.2382, Val Acc: 0.5744, ROC-AUC: 0.5977
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0565
Batch 20000/57005 completed, running loss: 0.0552
Batch 30000/57005 completed, running loss: 0.0553
Batch 40000/57005 completed, running loss: 0.0555
Batch 50000/57005 completed, running loss: 0.0560
Starting evaluation with 8372 batches
Fold 2 - Epoch 29/50
Train Loss: 0.0571, Train Acc: 0.9921
Val Loss: 11.5015, Val Acc: 0.5681, ROC-AUC: 0.5912
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0612
Batch 20000/57005 completed, running loss: 0.0612
Batch 30000/57005 completed, running loss: 0.0591
Batch 40000/57005 completed, running loss: 0.0583
Batch 50000/57005 completed, running loss: 0.0594
Starting evaluation with 8372 batches
Fold 2 - Epoch 30/50
Train Loss: 0.0614, Train Acc: 0.9922
Val Loss: 11.4340, Val Acc: 0.5785, ROC-AUC: 0.6094
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0530
Batch 20000/57005 completed, running loss: 0.0568
Batch 30000/57005 completed, running loss: 0.0557
Batch 40000/57005 completed, running loss: 0.0559
Batch 50000/57005 completed, running loss: 0.0580
Starting evaluation with 8372 batches
Fold 2 - Epoch 31/50
Train Loss: 0.0585, Train Acc: 0.9921
Val Loss: 11.6739, Val Acc: 0.5715, ROC-AUC: 0.5946
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0514
Batch 20000/57005 completed, running loss: 0.0526
Batch 30000/57005 completed, running loss: 0.0532
Batch 40000/57005 completed, running loss: 0.0537
Batch 50000/57005 completed, running loss: 0.0553
Starting evaluation with 8372 batches
Fold 2 - Epoch 32/50
Train Loss: 0.0560, Train Acc: 0.9925
Val Loss: 11.6057, Val Acc: 0.5685, ROC-AUC: 0.5936
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0464
Batch 20000/57005 completed, running loss: 0.0546
Batch 30000/57005 completed, running loss: 0.0546
Batch 40000/57005 completed, running loss: 0.0562
Batch 50000/57005 completed, running loss: 0.0589
Starting evaluation with 8372 batches
Fold 2 - Epoch 33/50
Train Loss: 0.0596, Train Acc: 0.9923
Val Loss: 11.7294, Val Acc: 0.5713, ROC-AUC: 0.5961
Fold 2 - Early stopping triggered
Starting evaluation with 57005 batches
Evaluation batch 10000/57005 completed
Evaluation batch 20000/57005 completed
Evaluation batch 30000/57005 completed
Evaluation batch 40000/57005 completed
Evaluation batch 50000/57005 completed
Starting evaluation with 8372 batches
Starting evaluation with 7094 batches

Fold 2 - Train Metrics:
  Loss: 0.1547
  Accuracy: 0.9382
  Precision: 0.9556
  Recall: 0.9099
  Roc_auc: 0.9864
  Specificity: 0.9629
  F1: 0.9322

Fold 2 - Validation Metrics:
  Loss: 1.8105
  Accuracy: 0.5786
  Precision: 0.6210
  Recall: 0.4971
  Roc_auc: 0.6181
  Specificity: 0.6679
  F1: 0.5522

Fold 2 - Test Metrics:
  Loss: 1.7426
  Accuracy: 0.5809
  Precision: 0.5711
  Recall: 0.5309
  Roc_auc: 0.6264
  Specificity: 0.6276
  F1: 0.5503

=== Fold 3/10 ===
Fold 3 - Training label counts: {0: 116663, 1: 108434}
Fold 3 - Validation label counts: {0: 15444, 1: 14378}
Fold 3 - Test label counts: {0: 20044, 1: 14915}
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.6439
Batch 20000/56275 completed, running loss: 0.6248
Batch 30000/56275 completed, running loss: 0.6135
Batch 40000/56275 completed, running loss: 0.6052
Batch 50000/56275 completed, running loss: 0.5986
Starting evaluation with 7456 batches
Fold 3 - Epoch 1/50
Train Loss: 0.5958, Train Acc: 0.6972
Val Loss: 0.8733, Val Acc: 0.6073, ROC-AUC: 0.6446
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.4701
Batch 20000/56275 completed, running loss: 0.4801
Batch 30000/56275 completed, running loss: 0.4860
Batch 40000/56275 completed, running loss: 0.4881
Batch 50000/56275 completed, running loss: 0.4913
Starting evaluation with 7456 batches
Fold 3 - Epoch 2/50
Train Loss: 0.4933, Train Acc: 0.7951
Val Loss: 1.2076, Val Acc: 0.5990, ROC-AUC: 0.6416
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.3566
Batch 20000/56275 completed, running loss: 0.3755
Batch 30000/56275 completed, running loss: 0.3901
Batch 40000/56275 completed, running loss: 0.4012
Batch 50000/56275 completed, running loss: 0.4100
Starting evaluation with 7456 batches
Fold 3 - Epoch 3/50
Train Loss: 0.4149, Train Acc: 0.8596
Val Loss: 1.8133, Val Acc: 0.6140, ROC-AUC: 0.6590
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2889
Batch 20000/56275 completed, running loss: 0.3113
Batch 30000/56275 completed, running loss: 0.3226
Batch 40000/56275 completed, running loss: 0.3331
Batch 50000/56275 completed, running loss: 0.3427
Starting evaluation with 7456 batches
Fold 3 - Epoch 4/50
Train Loss: 0.3487, Train Acc: 0.9011
Val Loss: 2.6418, Val Acc: 0.6011, ROC-AUC: 0.6419
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2418
Batch 20000/56275 completed, running loss: 0.2572
Batch 30000/56275 completed, running loss: 0.2705
Batch 40000/56275 completed, running loss: 0.2830
Batch 50000/56275 completed, running loss: 0.2931
Starting evaluation with 7456 batches
Fold 3 - Epoch 5/50
Train Loss: 0.2997, Train Acc: 0.9269
Val Loss: 3.5949, Val Acc: 0.6074, ROC-AUC: 0.6432
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2099
Batch 20000/56275 completed, running loss: 0.2242
Batch 30000/56275 completed, running loss: 0.2324
Batch 40000/56275 completed, running loss: 0.2430
Batch 50000/56275 completed, running loss: 0.2521
Starting evaluation with 7456 batches
Fold 3 - Epoch 6/50
Train Loss: 0.2580, Train Acc: 0.9429
Val Loss: 4.5479, Val Acc: 0.5963, ROC-AUC: 0.6328
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1928
Batch 20000/56275 completed, running loss: 0.2037
Batch 30000/56275 completed, running loss: 0.2166
Batch 40000/56275 completed, running loss: 0.2245
Batch 50000/56275 completed, running loss: 0.2317
Starting evaluation with 7456 batches
Fold 3 - Epoch 7/50
Train Loss: 0.2369, Train Acc: 0.9519
Val Loss: 5.0589, Val Acc: 0.5971, ROC-AUC: 0.6332
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1764
Batch 20000/56275 completed, running loss: 0.1827
Batch 30000/56275 completed, running loss: 0.1958
Batch 40000/56275 completed, running loss: 0.2037
Batch 50000/56275 completed, running loss: 0.2126
Starting evaluation with 7456 batches
Fold 3 - Epoch 8/50
Train Loss: 0.2179, Train Acc: 0.9597
Val Loss: 5.9211, Val Acc: 0.5863, ROC-AUC: 0.6170
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1723
Batch 20000/56275 completed, running loss: 0.1825
Batch 30000/56275 completed, running loss: 0.1872
Batch 40000/56275 completed, running loss: 0.1949
Batch 50000/56275 completed, running loss: 0.2006
Starting evaluation with 7456 batches
Fold 3 - Epoch 9/50
Train Loss: 0.2042, Train Acc: 0.9630
Val Loss: 6.3674, Val Acc: 0.5976, ROC-AUC: 0.6302
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1614
Batch 20000/56275 completed, running loss: 0.1653
Batch 30000/56275 completed, running loss: 0.1742
Batch 40000/56275 completed, running loss: 0.1821
Batch 50000/56275 completed, running loss: 0.1874
Starting evaluation with 7456 batches
Fold 3 - Epoch 10/50
Train Loss: 0.1904, Train Acc: 0.9670
Val Loss: 6.7276, Val Acc: 0.5853, ROC-AUC: 0.6179
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1446
Batch 20000/56275 completed, running loss: 0.1539
Batch 30000/56275 completed, running loss: 0.1675
Batch 40000/56275 completed, running loss: 0.1739
Batch 50000/56275 completed, running loss: 0.1800
Starting evaluation with 7456 batches
Fold 3 - Epoch 11/50
Train Loss: 0.1831, Train Acc: 0.9695
Val Loss: 6.8693, Val Acc: 0.6027, ROC-AUC: 0.6396
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1427
Batch 20000/56275 completed, running loss: 0.1554
Batch 30000/56275 completed, running loss: 0.1587
Batch 40000/56275 completed, running loss: 0.1641
Batch 50000/56275 completed, running loss: 0.1689
Starting evaluation with 7456 batches
Fold 3 - Epoch 12/50
Train Loss: 0.1714, Train Acc: 0.9721
Val Loss: 7.6715, Val Acc: 0.5864, ROC-AUC: 0.6169
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1357
Batch 20000/56275 completed, running loss: 0.1470
Batch 30000/56275 completed, running loss: 0.1552
Batch 40000/56275 completed, running loss: 0.1596
Batch 50000/56275 completed, running loss: 0.1630
Starting evaluation with 7456 batches
Fold 3 - Epoch 13/50
Train Loss: 0.1650, Train Acc: 0.9735
Val Loss: 7.7824, Val Acc: 0.5840, ROC-AUC: 0.6172
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1309
Batch 20000/56275 completed, running loss: 0.1425
Batch 30000/56275 completed, running loss: 0.1487
Batch 40000/56275 completed, running loss: 0.1529
Batch 50000/56275 completed, running loss: 0.1561
Starting evaluation with 7456 batches
Fold 3 - Epoch 14/50
Train Loss: 0.1572, Train Acc: 0.9753
Val Loss: 7.6872, Val Acc: 0.5960, ROC-AUC: 0.6340
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1248
Batch 20000/56275 completed, running loss: 0.1378
Batch 30000/56275 completed, running loss: 0.1421
Batch 40000/56275 completed, running loss: 0.1456
Batch 50000/56275 completed, running loss: 0.1480
Starting evaluation with 7456 batches
Fold 3 - Epoch 15/50
Train Loss: 0.1492, Train Acc: 0.9769
Val Loss: 8.1869, Val Acc: 0.5877, ROC-AUC: 0.6203
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1322
Batch 20000/56275 completed, running loss: 0.1347
Batch 30000/56275 completed, running loss: 0.1341
Batch 40000/56275 completed, running loss: 0.1387
Batch 50000/56275 completed, running loss: 0.1393
Starting evaluation with 7456 batches
Fold 3 - Epoch 16/50
Train Loss: 0.1410, Train Acc: 0.9780
Val Loss: 8.9515, Val Acc: 0.5832, ROC-AUC: 0.6118
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1264
Batch 20000/56275 completed, running loss: 0.1286
Batch 30000/56275 completed, running loss: 0.1336
Batch 40000/56275 completed, running loss: 0.1372
Batch 50000/56275 completed, running loss: 0.1395
Starting evaluation with 7456 batches
Fold 3 - Epoch 17/50
Train Loss: 0.1415, Train Acc: 0.9789
Val Loss: 8.2509, Val Acc: 0.5820, ROC-AUC: 0.6090
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1199
Batch 20000/56275 completed, running loss: 0.1257
Batch 30000/56275 completed, running loss: 0.1324
Batch 40000/56275 completed, running loss: 0.1335
Batch 50000/56275 completed, running loss: 0.1369
Starting evaluation with 7456 batches
Fold 3 - Epoch 18/50
Train Loss: 0.1371, Train Acc: 0.9796
Val Loss: 8.4091, Val Acc: 0.5943, ROC-AUC: 0.6302
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1115
Batch 20000/56275 completed, running loss: 0.1177
Batch 30000/56275 completed, running loss: 0.1206
Batch 40000/56275 completed, running loss: 0.1212
Batch 50000/56275 completed, running loss: 0.1258
Starting evaluation with 7456 batches
Fold 3 - Epoch 19/50
Train Loss: 0.1275, Train Acc: 0.9808
Val Loss: 9.0192, Val Acc: 0.5871, ROC-AUC: 0.6174
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0941
Batch 20000/56275 completed, running loss: 0.0921
Batch 30000/56275 completed, running loss: 0.0939
Batch 40000/56275 completed, running loss: 0.0941
Batch 50000/56275 completed, running loss: 0.0944
Starting evaluation with 7456 batches
Fold 3 - Epoch 20/50
Train Loss: 0.0928, Train Acc: 0.9866
Val Loss: 9.5730, Val Acc: 0.5927, ROC-AUC: 0.6279
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0652
Batch 20000/56275 completed, running loss: 0.0677
Batch 30000/56275 completed, running loss: 0.0728
Batch 40000/56275 completed, running loss: 0.0764
Batch 50000/56275 completed, running loss: 0.0766
Starting evaluation with 7456 batches
Fold 3 - Epoch 21/50
Train Loss: 0.0766, Train Acc: 0.9889
Val Loss: 10.4015, Val Acc: 0.5877, ROC-AUC: 0.6156
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0645
Batch 20000/56275 completed, running loss: 0.0651
Batch 30000/56275 completed, running loss: 0.0692
Batch 40000/56275 completed, running loss: 0.0714
Batch 50000/56275 completed, running loss: 0.0704
Starting evaluation with 7456 batches
Fold 3 - Epoch 22/50
Train Loss: 0.0716, Train Acc: 0.9898
Val Loss: 10.2999, Val Acc: 0.5955, ROC-AUC: 0.6297
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0698
Batch 20000/56275 completed, running loss: 0.0703
Batch 30000/56275 completed, running loss: 0.0687
Batch 40000/56275 completed, running loss: 0.0683
Batch 50000/56275 completed, running loss: 0.0698
Starting evaluation with 7456 batches
Fold 3 - Epoch 23/50
Train Loss: 0.0698, Train Acc: 0.9903
Val Loss: 10.6777, Val Acc: 0.5963, ROC-AUC: 0.6282
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0728
Batch 20000/56275 completed, running loss: 0.0698
Batch 30000/56275 completed, running loss: 0.0680
Batch 40000/56275 completed, running loss: 0.0660
Batch 50000/56275 completed, running loss: 0.0660
Starting evaluation with 7456 batches
Fold 3 - Epoch 24/50
Train Loss: 0.0649, Train Acc: 0.9907
Val Loss: 11.0949, Val Acc: 0.5954, ROC-AUC: 0.6283
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0610
Batch 20000/56275 completed, running loss: 0.0622
Batch 30000/56275 completed, running loss: 0.0620
Batch 40000/56275 completed, running loss: 0.0612
Batch 50000/56275 completed, running loss: 0.0602
Starting evaluation with 7456 batches
Fold 3 - Epoch 25/50
Train Loss: 0.0611, Train Acc: 0.9914
Val Loss: 11.2989, Val Acc: 0.5878, ROC-AUC: 0.6190
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0655
Batch 20000/56275 completed, running loss: 0.0621
Batch 30000/56275 completed, running loss: 0.0636
Batch 40000/56275 completed, running loss: 0.0640
Batch 50000/56275 completed, running loss: 0.0628
Starting evaluation with 7456 batches
Fold 3 - Epoch 26/50
Train Loss: 0.0620, Train Acc: 0.9918
Val Loss: 11.5570, Val Acc: 0.5916, ROC-AUC: 0.6198
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0602
Batch 20000/56275 completed, running loss: 0.0569
Batch 30000/56275 completed, running loss: 0.0610
Batch 40000/56275 completed, running loss: 0.0622
Batch 50000/56275 completed, running loss: 0.0616
Starting evaluation with 7456 batches
Fold 3 - Epoch 27/50
Train Loss: 0.0625, Train Acc: 0.9914
Val Loss: 11.4038, Val Acc: 0.5854, ROC-AUC: 0.6167
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0511
Batch 20000/56275 completed, running loss: 0.0536
Batch 30000/56275 completed, running loss: 0.0527
Batch 40000/56275 completed, running loss: 0.0557
Batch 50000/56275 completed, running loss: 0.0559
Starting evaluation with 7456 batches
Fold 3 - Epoch 28/50
Train Loss: 0.0561, Train Acc: 0.9921
Val Loss: 11.7506, Val Acc: 0.5974, ROC-AUC: 0.6313
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0562
Batch 20000/56275 completed, running loss: 0.0559
Batch 30000/56275 completed, running loss: 0.0577
Batch 40000/56275 completed, running loss: 0.0559
Batch 50000/56275 completed, running loss: 0.0585
Starting evaluation with 7456 batches
Fold 3 - Epoch 29/50
Train Loss: 0.0596, Train Acc: 0.9921
Val Loss: 11.4397, Val Acc: 0.5910, ROC-AUC: 0.6210
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0502
Batch 20000/56275 completed, running loss: 0.0516
Batch 30000/56275 completed, running loss: 0.0525
Batch 40000/56275 completed, running loss: 0.0548
Batch 50000/56275 completed, running loss: 0.0563
Starting evaluation with 7456 batches
Fold 3 - Epoch 30/50
Train Loss: 0.0575, Train Acc: 0.9926
Val Loss: 12.0668, Val Acc: 0.5862, ROC-AUC: 0.6178
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0559
Batch 20000/56275 completed, running loss: 0.0542
Batch 30000/56275 completed, running loss: 0.0555
Batch 40000/56275 completed, running loss: 0.0586
Batch 50000/56275 completed, running loss: 0.0583
Starting evaluation with 7456 batches
Fold 3 - Epoch 31/50
Train Loss: 0.0591, Train Acc: 0.9922
Val Loss: 11.6322, Val Acc: 0.5896, ROC-AUC: 0.6204
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0633
Batch 20000/56275 completed, running loss: 0.0577
Batch 30000/56275 completed, running loss: 0.0563
Batch 40000/56275 completed, running loss: 0.0556
Batch 50000/56275 completed, running loss: 0.0571
Starting evaluation with 7456 batches
Fold 3 - Epoch 32/50
Train Loss: 0.0577, Train Acc: 0.9925
Val Loss: 12.0598, Val Acc: 0.5904, ROC-AUC: 0.6225
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.0545
Batch 20000/56275 completed, running loss: 0.0561
Batch 30000/56275 completed, running loss: 0.0544
Batch 40000/56275 completed, running loss: 0.0542
Batch 50000/56275 completed, running loss: 0.0553
Starting evaluation with 7456 batches
Fold 3 - Epoch 33/50
Train Loss: 0.0558, Train Acc: 0.9924
Val Loss: 12.1299, Val Acc: 0.5888, ROC-AUC: 0.6199
Fold 3 - Early stopping triggered
Starting evaluation with 56275 batches
Evaluation batch 10000/56275 completed
Evaluation batch 20000/56275 completed
Evaluation batch 30000/56275 completed
Evaluation batch 40000/56275 completed
Evaluation batch 50000/56275 completed
Starting evaluation with 7456 batches
Starting evaluation with 8740 batches

Fold 3 - Train Metrics:
  Loss: 0.1671
  Accuracy: 0.9358
  Precision: 0.9167
  Recall: 0.9533
  Roc_auc: 0.9850
  Specificity: 0.9195
  F1: 0.9346

Fold 3 - Validation Metrics:
  Loss: 1.8133
  Accuracy: 0.6140
  Precision: 0.5956
  Recall: 0.6210
  Roc_auc: 0.6590
  Specificity: 0.6074
  F1: 0.6080

Fold 3 - Test Metrics:
  Loss: 2.0311
  Accuracy: 0.5761
  Precision: 0.5027
  Recall: 0.5991
  Roc_auc: 0.6218
  Specificity: 0.5591
  F1: 0.5467

=== Fold 4/10 ===
Fold 4 - Training label counts: {0: 119065, 1: 108862}
Fold 4 - Validation label counts: {0: 16807, 1: 13362}
Fold 4 - Test label counts: {0: 16279, 1: 15503}
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.6366
Batch 20000/56982 completed, running loss: 0.6201
Batch 30000/56982 completed, running loss: 0.6099
Batch 40000/56982 completed, running loss: 0.6011
Batch 50000/56982 completed, running loss: 0.5943
Starting evaluation with 7543 batches
Fold 4 - Epoch 1/50
Train Loss: 0.5901, Train Acc: 0.6983
Val Loss: 0.9588, Val Acc: 0.5833, ROC-AUC: 0.6227
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.4727
Batch 20000/56982 completed, running loss: 0.4819
Batch 30000/56982 completed, running loss: 0.4865
Batch 40000/56982 completed, running loss: 0.4911
Batch 50000/56982 completed, running loss: 0.4930
Starting evaluation with 7543 batches
Fold 4 - Epoch 2/50
Train Loss: 0.4954, Train Acc: 0.7932
Val Loss: 1.2465, Val Acc: 0.5803, ROC-AUC: 0.6206
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.3737
Batch 20000/56982 completed, running loss: 0.3927
Batch 30000/56982 completed, running loss: 0.4034
Batch 40000/56982 completed, running loss: 0.4103
Batch 50000/56982 completed, running loss: 0.4167
Starting evaluation with 7543 batches
Fold 4 - Epoch 3/50
Train Loss: 0.4213, Train Acc: 0.8565
Val Loss: 1.8074, Val Acc: 0.5790, ROC-AUC: 0.6171
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2878
Batch 20000/56982 completed, running loss: 0.3147
Batch 30000/56982 completed, running loss: 0.3284
Batch 40000/56982 completed, running loss: 0.3395
Batch 50000/56982 completed, running loss: 0.3493
Starting evaluation with 7543 batches
Fold 4 - Epoch 4/50
Train Loss: 0.3547, Train Acc: 0.8974
Val Loss: 2.8332, Val Acc: 0.5724, ROC-AUC: 0.5984
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2515
Batch 20000/56982 completed, running loss: 0.2649
Batch 30000/56982 completed, running loss: 0.2742
Batch 40000/56982 completed, running loss: 0.2874
Batch 50000/56982 completed, running loss: 0.2963
Starting evaluation with 7543 batches
Fold 4 - Epoch 5/50
Train Loss: 0.3017, Train Acc: 0.9234
Val Loss: 3.6390, Val Acc: 0.5753, ROC-AUC: 0.6058
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2131
Batch 20000/56982 completed, running loss: 0.2306
Batch 30000/56982 completed, running loss: 0.2366
Batch 40000/56982 completed, running loss: 0.2483
Batch 50000/56982 completed, running loss: 0.2575
Starting evaluation with 7543 batches
Fold 4 - Epoch 6/50
Train Loss: 0.2646, Train Acc: 0.9399
Val Loss: 4.5125, Val Acc: 0.5664, ROC-AUC: 0.5932
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2034
Batch 20000/56982 completed, running loss: 0.2138
Batch 30000/56982 completed, running loss: 0.2214
Batch 40000/56982 completed, running loss: 0.2301
Batch 50000/56982 completed, running loss: 0.2348
Starting evaluation with 7543 batches
Fold 4 - Epoch 7/50
Train Loss: 0.2401, Train Acc: 0.9501
Val Loss: 5.3360, Val Acc: 0.5714, ROC-AUC: 0.6011
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1819
Batch 20000/56982 completed, running loss: 0.1930
Batch 30000/56982 completed, running loss: 0.2017
Batch 40000/56982 completed, running loss: 0.2099
Batch 50000/56982 completed, running loss: 0.2132
Starting evaluation with 7543 batches
Fold 4 - Epoch 8/50
Train Loss: 0.2151, Train Acc: 0.9575
Val Loss: 5.7306, Val Acc: 0.5803, ROC-AUC: 0.6077
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1804
Batch 20000/56982 completed, running loss: 0.1864
Batch 30000/56982 completed, running loss: 0.1977
Batch 40000/56982 completed, running loss: 0.2002
Batch 50000/56982 completed, running loss: 0.2048
Starting evaluation with 7543 batches
Fold 4 - Epoch 9/50
Train Loss: 0.2062, Train Acc: 0.9619
Val Loss: 6.1909, Val Acc: 0.5686, ROC-AUC: 0.5972
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1692
Batch 20000/56982 completed, running loss: 0.1760
Batch 30000/56982 completed, running loss: 0.1812
Batch 40000/56982 completed, running loss: 0.1857
Batch 50000/56982 completed, running loss: 0.1906
Starting evaluation with 7543 batches
Fold 4 - Epoch 10/50
Train Loss: 0.1947, Train Acc: 0.9651
Val Loss: 6.6153, Val Acc: 0.5694, ROC-AUC: 0.5976
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1569
Batch 20000/56982 completed, running loss: 0.1679
Batch 30000/56982 completed, running loss: 0.1722
Batch 40000/56982 completed, running loss: 0.1750
Batch 50000/56982 completed, running loss: 0.1802
Starting evaluation with 7543 batches
Fold 4 - Epoch 11/50
Train Loss: 0.1821, Train Acc: 0.9686
Val Loss: 6.7039, Val Acc: 0.5739, ROC-AUC: 0.6087
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1535
Batch 20000/56982 completed, running loss: 0.1520
Batch 30000/56982 completed, running loss: 0.1593
Batch 40000/56982 completed, running loss: 0.1645
Batch 50000/56982 completed, running loss: 0.1712
Starting evaluation with 7543 batches
Fold 4 - Epoch 12/50
Train Loss: 0.1732, Train Acc: 0.9713
Val Loss: 7.0141, Val Acc: 0.5703, ROC-AUC: 0.6052
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1460
Batch 20000/56982 completed, running loss: 0.1476
Batch 30000/56982 completed, running loss: 0.1515
Batch 40000/56982 completed, running loss: 0.1576
Batch 50000/56982 completed, running loss: 0.1611
Starting evaluation with 7543 batches
Fold 4 - Epoch 13/50
Train Loss: 0.1639, Train Acc: 0.9732
Val Loss: 7.4490, Val Acc: 0.5786, ROC-AUC: 0.6099
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1438
Batch 20000/56982 completed, running loss: 0.1431
Batch 30000/56982 completed, running loss: 0.1471
Batch 40000/56982 completed, running loss: 0.1529
Batch 50000/56982 completed, running loss: 0.1576
Starting evaluation with 7543 batches
Fold 4 - Epoch 14/50
Train Loss: 0.1577, Train Acc: 0.9744
Val Loss: 7.3589, Val Acc: 0.5759, ROC-AUC: 0.6075
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1283
Batch 20000/56982 completed, running loss: 0.1344
Batch 30000/56982 completed, running loss: 0.1453
Batch 40000/56982 completed, running loss: 0.1480
Batch 50000/56982 completed, running loss: 0.1501
Starting evaluation with 7543 batches
Fold 4 - Epoch 15/50
Train Loss: 0.1513, Train Acc: 0.9761
Val Loss: 7.9387, Val Acc: 0.5763, ROC-AUC: 0.6058
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1351
Batch 20000/56982 completed, running loss: 0.1396
Batch 30000/56982 completed, running loss: 0.1451
Batch 40000/56982 completed, running loss: 0.1481
Batch 50000/56982 completed, running loss: 0.1508
Starting evaluation with 7543 batches
Fold 4 - Epoch 16/50
Train Loss: 0.1527, Train Acc: 0.9770
Val Loss: 7.7463, Val Acc: 0.5747, ROC-AUC: 0.6066
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1201
Batch 20000/56982 completed, running loss: 0.1269
Batch 30000/56982 completed, running loss: 0.1319
Batch 40000/56982 completed, running loss: 0.1377
Batch 50000/56982 completed, running loss: 0.1411
Starting evaluation with 7543 batches
Fold 4 - Epoch 17/50
Train Loss: 0.1413, Train Acc: 0.9781
Val Loss: 8.5691, Val Acc: 0.5752, ROC-AUC: 0.6065
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1022
Batch 20000/56982 completed, running loss: 0.1004
Batch 30000/56982 completed, running loss: 0.0946
Batch 40000/56982 completed, running loss: 0.0932
Batch 50000/56982 completed, running loss: 0.0941
Starting evaluation with 7543 batches
Fold 4 - Epoch 18/50
Train Loss: 0.0948, Train Acc: 0.9852
Val Loss: 8.8611, Val Acc: 0.5670, ROC-AUC: 0.5970
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0752
Batch 20000/56982 completed, running loss: 0.0793
Batch 30000/56982 completed, running loss: 0.0803
Batch 40000/56982 completed, running loss: 0.0803
Batch 50000/56982 completed, running loss: 0.0812
Starting evaluation with 7543 batches
Fold 4 - Epoch 19/50
Train Loss: 0.0811, Train Acc: 0.9878
Val Loss: 9.3434, Val Acc: 0.5709, ROC-AUC: 0.6029
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0681
Batch 20000/56982 completed, running loss: 0.0703
Batch 30000/56982 completed, running loss: 0.0735
Batch 40000/56982 completed, running loss: 0.0740
Batch 50000/56982 completed, running loss: 0.0727
Starting evaluation with 7543 batches
Fold 4 - Epoch 20/50
Train Loss: 0.0737, Train Acc: 0.9890
Val Loss: 10.1192, Val Acc: 0.5711, ROC-AUC: 0.6010
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0704
Batch 20000/56982 completed, running loss: 0.0714
Batch 30000/56982 completed, running loss: 0.0683
Batch 40000/56982 completed, running loss: 0.0674
Batch 50000/56982 completed, running loss: 0.0677
Starting evaluation with 7543 batches
Fold 4 - Epoch 21/50
Train Loss: 0.0689, Train Acc: 0.9896
Val Loss: 10.0546, Val Acc: 0.5678, ROC-AUC: 0.5980
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0620
Batch 20000/56982 completed, running loss: 0.0624
Batch 30000/56982 completed, running loss: 0.0656
Batch 40000/56982 completed, running loss: 0.0673
Batch 50000/56982 completed, running loss: 0.0671
Starting evaluation with 7543 batches
Fold 4 - Epoch 22/50
Train Loss: 0.0684, Train Acc: 0.9901
Val Loss: 10.0588, Val Acc: 0.5687, ROC-AUC: 0.5996
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0574
Batch 20000/56982 completed, running loss: 0.0589
Batch 30000/56982 completed, running loss: 0.0616
Batch 40000/56982 completed, running loss: 0.0634
Batch 50000/56982 completed, running loss: 0.0642
Starting evaluation with 7543 batches
Fold 4 - Epoch 23/50
Train Loss: 0.0635, Train Acc: 0.9907
Val Loss: 10.5670, Val Acc: 0.5749, ROC-AUC: 0.6055
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0570
Batch 20000/56982 completed, running loss: 0.0594
Batch 30000/56982 completed, running loss: 0.0601
Batch 40000/56982 completed, running loss: 0.0619
Batch 50000/56982 completed, running loss: 0.0628
Starting evaluation with 7543 batches
Fold 4 - Epoch 24/50
Train Loss: 0.0643, Train Acc: 0.9909
Val Loss: 10.4374, Val Acc: 0.5687, ROC-AUC: 0.6002
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0518
Batch 20000/56982 completed, running loss: 0.0570
Batch 30000/56982 completed, running loss: 0.0605
Batch 40000/56982 completed, running loss: 0.0604
Batch 50000/56982 completed, running loss: 0.0624
Starting evaluation with 7543 batches
Fold 4 - Epoch 25/50
Train Loss: 0.0629, Train Acc: 0.9915
Val Loss: 11.0255, Val Acc: 0.5665, ROC-AUC: 0.5979
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0620
Batch 20000/56982 completed, running loss: 0.0649
Batch 30000/56982 completed, running loss: 0.0647
Batch 40000/56982 completed, running loss: 0.0634
Batch 50000/56982 completed, running loss: 0.0640
Starting evaluation with 7543 batches
Fold 4 - Epoch 26/50
Train Loss: 0.0642, Train Acc: 0.9911
Val Loss: 10.8593, Val Acc: 0.5758, ROC-AUC: 0.6079
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0553
Batch 20000/56982 completed, running loss: 0.0596
Batch 30000/56982 completed, running loss: 0.0607
Batch 40000/56982 completed, running loss: 0.0621
Batch 50000/56982 completed, running loss: 0.0643
Starting evaluation with 7543 batches
Fold 4 - Epoch 27/50
Train Loss: 0.0650, Train Acc: 0.9913
Val Loss: 10.7780, Val Acc: 0.5785, ROC-AUC: 0.6090
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0519
Batch 20000/56982 completed, running loss: 0.0590
Batch 30000/56982 completed, running loss: 0.0599
Batch 40000/56982 completed, running loss: 0.0599
Batch 50000/56982 completed, running loss: 0.0591
Starting evaluation with 7543 batches
Fold 4 - Epoch 28/50
Train Loss: 0.0610, Train Acc: 0.9917
Val Loss: 11.1987, Val Acc: 0.5700, ROC-AUC: 0.6029
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0596
Batch 20000/56982 completed, running loss: 0.0582
Batch 30000/56982 completed, running loss: 0.0597
Batch 40000/56982 completed, running loss: 0.0599
Batch 50000/56982 completed, running loss: 0.0596
Starting evaluation with 7543 batches
Fold 4 - Epoch 29/50
Train Loss: 0.0607, Train Acc: 0.9917
Val Loss: 11.0761, Val Acc: 0.5761, ROC-AUC: 0.6062
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0504
Batch 20000/56982 completed, running loss: 0.0572
Batch 30000/56982 completed, running loss: 0.0565
Batch 40000/56982 completed, running loss: 0.0572
Batch 50000/56982 completed, running loss: 0.0584
Starting evaluation with 7543 batches
Fold 4 - Epoch 30/50
Train Loss: 0.0575, Train Acc: 0.9922
Val Loss: 11.8540, Val Acc: 0.5724, ROC-AUC: 0.6014
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.0479
Batch 20000/56982 completed, running loss: 0.0548
Batch 30000/56982 completed, running loss: 0.0562
Batch 40000/56982 completed, running loss: 0.0578
Batch 50000/56982 completed, running loss: 0.0589
Starting evaluation with 7543 batches
Fold 4 - Epoch 31/50
Train Loss: 0.0592, Train Acc: 0.9923
Val Loss: 12.0316, Val Acc: 0.5706, ROC-AUC: 0.6024
Fold 4 - Early stopping triggered
Starting evaluation with 56982 batches
Evaluation batch 10000/56982 completed
Evaluation batch 20000/56982 completed
Evaluation batch 30000/56982 completed
Evaluation batch 40000/56982 completed
Evaluation batch 50000/56982 completed
Starting evaluation with 7543 batches
Starting evaluation with 7946 batches

Fold 4 - Train Metrics:
  Loss: 0.4061
  Accuracy: 0.8054
  Precision: 0.7938
  Recall: 0.8006
  Roc_auc: 0.8960
  Specificity: 0.8098
  F1: 0.7972

Fold 4 - Validation Metrics:
  Loss: 0.9588
  Accuracy: 0.5833
  Precision: 0.5291
  Recall: 0.5383
  Roc_auc: 0.6227
  Specificity: 0.6191
  F1: 0.5337

Fold 4 - Test Metrics:
  Loss: 0.9792
  Accuracy: 0.5818
  Precision: 0.5757
  Recall: 0.5426
  Roc_auc: 0.6111
  Specificity: 0.6191
  F1: 0.5586

=== Fold 5/10 ===
Fold 5 - Training label counts: {0: 121048, 1: 110209}
Fold 5 - Validation label counts: {0: 16911, 1: 13561}
Fold 5 - Test label counts: {0: 14192, 1: 13957}
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.6417
Batch 20000/57815 completed, running loss: 0.6247
Batch 30000/57815 completed, running loss: 0.6137
Batch 40000/57815 completed, running loss: 0.6046
Batch 50000/57815 completed, running loss: 0.5975
Starting evaluation with 7618 batches
Fold 5 - Epoch 1/50
Train Loss: 0.5921, Train Acc: 0.6984
Val Loss: 0.9048, Val Acc: 0.5819, ROC-AUC: 0.6205
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.4677
Batch 20000/57815 completed, running loss: 0.4766
Batch 30000/57815 completed, running loss: 0.4805
Batch 40000/57815 completed, running loss: 0.4843
Batch 50000/57815 completed, running loss: 0.4890
Starting evaluation with 7618 batches
Fold 5 - Epoch 2/50
Train Loss: 0.4910, Train Acc: 0.7981
Val Loss: 1.2114, Val Acc: 0.5819, ROC-AUC: 0.6209
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.3649
Batch 20000/57815 completed, running loss: 0.3827
Batch 30000/57815 completed, running loss: 0.3929
Batch 40000/57815 completed, running loss: 0.4037
Batch 50000/57815 completed, running loss: 0.4120
Starting evaluation with 7618 batches
Fold 5 - Epoch 3/50
Train Loss: 0.4160, Train Acc: 0.8598
Val Loss: 1.9025, Val Acc: 0.5765, ROC-AUC: 0.6180
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.2862
Batch 20000/57815 completed, running loss: 0.3084
Batch 30000/57815 completed, running loss: 0.3205
Batch 40000/57815 completed, running loss: 0.3321
Batch 50000/57815 completed, running loss: 0.3419
Starting evaluation with 7618 batches
Fold 5 - Epoch 4/50
Train Loss: 0.3473, Train Acc: 0.8999
Val Loss: 2.6745, Val Acc: 0.5852, ROC-AUC: 0.6209
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.2456
Batch 20000/57815 completed, running loss: 0.2643
Batch 30000/57815 completed, running loss: 0.2718
Batch 40000/57815 completed, running loss: 0.2836
Batch 50000/57815 completed, running loss: 0.2934
Starting evaluation with 7618 batches
Fold 5 - Epoch 5/50
Train Loss: 0.3010, Train Acc: 0.9258
Val Loss: 3.3423, Val Acc: 0.5856, ROC-AUC: 0.6239
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.2072
Batch 20000/57815 completed, running loss: 0.2230
Batch 30000/57815 completed, running loss: 0.2357
Batch 40000/57815 completed, running loss: 0.2449
Batch 50000/57815 completed, running loss: 0.2549
Starting evaluation with 7618 batches
Fold 5 - Epoch 6/50
Train Loss: 0.2624, Train Acc: 0.9411
Val Loss: 4.2745, Val Acc: 0.5829, ROC-AUC: 0.6194
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.2044
Batch 20000/57815 completed, running loss: 0.2141
Batch 30000/57815 completed, running loss: 0.2228
Batch 40000/57815 completed, running loss: 0.2280
Batch 50000/57815 completed, running loss: 0.2326
Starting evaluation with 7618 batches
Fold 5 - Epoch 7/50
Train Loss: 0.2390, Train Acc: 0.9510
Val Loss: 5.0052, Val Acc: 0.5769, ROC-AUC: 0.6120
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1801
Batch 20000/57815 completed, running loss: 0.1905
Batch 30000/57815 completed, running loss: 0.2020
Batch 40000/57815 completed, running loss: 0.2086
Batch 50000/57815 completed, running loss: 0.2143
Starting evaluation with 7618 batches
Fold 5 - Epoch 8/50
Train Loss: 0.2207, Train Acc: 0.9571
Val Loss: 5.5354, Val Acc: 0.5776, ROC-AUC: 0.6079
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1590
Batch 20000/57815 completed, running loss: 0.1741
Batch 30000/57815 completed, running loss: 0.1833
Batch 40000/57815 completed, running loss: 0.1899
Batch 50000/57815 completed, running loss: 0.1964
Starting evaluation with 7618 batches
Fold 5 - Epoch 9/50
Train Loss: 0.2007, Train Acc: 0.9631
Val Loss: 5.5598, Val Acc: 0.5819, ROC-AUC: 0.6141
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1581
Batch 20000/57815 completed, running loss: 0.1717
Batch 30000/57815 completed, running loss: 0.1787
Batch 40000/57815 completed, running loss: 0.1821
Batch 50000/57815 completed, running loss: 0.1859
Starting evaluation with 7618 batches
Fold 5 - Epoch 10/50
Train Loss: 0.1902, Train Acc: 0.9658
Val Loss: 6.0420, Val Acc: 0.5838, ROC-AUC: 0.6146
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1622
Batch 20000/57815 completed, running loss: 0.1694
Batch 30000/57815 completed, running loss: 0.1756
Batch 40000/57815 completed, running loss: 0.1756
Batch 50000/57815 completed, running loss: 0.1782
Starting evaluation with 7618 batches
Fold 5 - Epoch 11/50
Train Loss: 0.1805, Train Acc: 0.9688
Val Loss: 6.5345, Val Acc: 0.5837, ROC-AUC: 0.6128
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1438
Batch 20000/57815 completed, running loss: 0.1556
Batch 30000/57815 completed, running loss: 0.1646
Batch 40000/57815 completed, running loss: 0.1679
Batch 50000/57815 completed, running loss: 0.1729
Starting evaluation with 7618 batches
Fold 5 - Epoch 12/50
Train Loss: 0.1772, Train Acc: 0.9712
Val Loss: 6.8359, Val Acc: 0.5856, ROC-AUC: 0.6145
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1301
Batch 20000/57815 completed, running loss: 0.1408
Batch 30000/57815 completed, running loss: 0.1506
Batch 40000/57815 completed, running loss: 0.1573
Batch 50000/57815 completed, running loss: 0.1586
Starting evaluation with 7618 batches
Fold 5 - Epoch 13/50
Train Loss: 0.1626, Train Acc: 0.9736
Val Loss: 7.1970, Val Acc: 0.5694, ROC-AUC: 0.6032
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1432
Batch 20000/57815 completed, running loss: 0.1448
Batch 30000/57815 completed, running loss: 0.1496
Batch 40000/57815 completed, running loss: 0.1564
Batch 50000/57815 completed, running loss: 0.1611
Starting evaluation with 7618 batches
Fold 5 - Epoch 14/50
Train Loss: 0.1627, Train Acc: 0.9742
Val Loss: 7.3252, Val Acc: 0.5798, ROC-AUC: 0.6109
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1396
Batch 20000/57815 completed, running loss: 0.1377
Batch 30000/57815 completed, running loss: 0.1438
Batch 40000/57815 completed, running loss: 0.1433
Batch 50000/57815 completed, running loss: 0.1499
Starting evaluation with 7618 batches
Fold 5 - Epoch 15/50
Train Loss: 0.1499, Train Acc: 0.9764
Val Loss: 7.7476, Val Acc: 0.5716, ROC-AUC: 0.6010
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1320
Batch 20000/57815 completed, running loss: 0.1356
Batch 30000/57815 completed, running loss: 0.1411
Batch 40000/57815 completed, running loss: 0.1425
Batch 50000/57815 completed, running loss: 0.1454
Starting evaluation with 7618 batches
Fold 5 - Epoch 16/50
Train Loss: 0.1470, Train Acc: 0.9776
Val Loss: 7.9851, Val Acc: 0.5753, ROC-AUC: 0.6069
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1263
Batch 20000/57815 completed, running loss: 0.1317
Batch 30000/57815 completed, running loss: 0.1338
Batch 40000/57815 completed, running loss: 0.1361
Batch 50000/57815 completed, running loss: 0.1388
Starting evaluation with 7618 batches
Fold 5 - Epoch 17/50
Train Loss: 0.1402, Train Acc: 0.9783
Val Loss: 8.3049, Val Acc: 0.5717, ROC-AUC: 0.6045
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1211
Batch 20000/57815 completed, running loss: 0.1304
Batch 30000/57815 completed, running loss: 0.1288
Batch 40000/57815 completed, running loss: 0.1320
Batch 50000/57815 completed, running loss: 0.1344
Starting evaluation with 7618 batches
Fold 5 - Epoch 18/50
Train Loss: 0.1369, Train Acc: 0.9793
Val Loss: 8.1063, Val Acc: 0.5853, ROC-AUC: 0.6168
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1130
Batch 20000/57815 completed, running loss: 0.1210
Batch 30000/57815 completed, running loss: 0.1263
Batch 40000/57815 completed, running loss: 0.1295
Batch 50000/57815 completed, running loss: 0.1305
Starting evaluation with 7618 batches
Fold 5 - Epoch 19/50
Train Loss: 0.1322, Train Acc: 0.9800
Val Loss: 8.3515, Val Acc: 0.5779, ROC-AUC: 0.6060
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1149
Batch 20000/57815 completed, running loss: 0.1137
Batch 30000/57815 completed, running loss: 0.1204
Batch 40000/57815 completed, running loss: 0.1236
Batch 50000/57815 completed, running loss: 0.1269
Starting evaluation with 7618 batches
Fold 5 - Epoch 20/50
Train Loss: 0.1284, Train Acc: 0.9811
Val Loss: 8.7230, Val Acc: 0.5777, ROC-AUC: 0.6098
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.1118
Batch 20000/57815 completed, running loss: 0.1163
Batch 30000/57815 completed, running loss: 0.1160
Batch 40000/57815 completed, running loss: 0.1214
Batch 50000/57815 completed, running loss: 0.1226
Starting evaluation with 7618 batches
Fold 5 - Epoch 21/50
Train Loss: 0.1235, Train Acc: 0.9818
Val Loss: 8.6025, Val Acc: 0.5726, ROC-AUC: 0.6038
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0965
Batch 20000/57815 completed, running loss: 0.0890
Batch 30000/57815 completed, running loss: 0.0875
Batch 40000/57815 completed, running loss: 0.0919
Batch 50000/57815 completed, running loss: 0.0928
Starting evaluation with 7618 batches
Fold 5 - Epoch 22/50
Train Loss: 0.0917, Train Acc: 0.9873
Val Loss: 9.2218, Val Acc: 0.5693, ROC-AUC: 0.6033
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0695
Batch 20000/57815 completed, running loss: 0.0735
Batch 30000/57815 completed, running loss: 0.0769
Batch 40000/57815 completed, running loss: 0.0771
Batch 50000/57815 completed, running loss: 0.0775
Starting evaluation with 7618 batches
Fold 5 - Epoch 23/50
Train Loss: 0.0772, Train Acc: 0.9891
Val Loss: 9.3288, Val Acc: 0.5786, ROC-AUC: 0.6078
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0630
Batch 20000/57815 completed, running loss: 0.0618
Batch 30000/57815 completed, running loss: 0.0663
Batch 40000/57815 completed, running loss: 0.0654
Batch 50000/57815 completed, running loss: 0.0668
Starting evaluation with 7618 batches
Fold 5 - Epoch 24/50
Train Loss: 0.0670, Train Acc: 0.9901
Val Loss: 10.0830, Val Acc: 0.5697, ROC-AUC: 0.6028
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0607
Batch 20000/57815 completed, running loss: 0.0599
Batch 30000/57815 completed, running loss: 0.0588
Batch 40000/57815 completed, running loss: 0.0633
Batch 50000/57815 completed, running loss: 0.0636
Starting evaluation with 7618 batches
Fold 5 - Epoch 25/50
Train Loss: 0.0639, Train Acc: 0.9908
Val Loss: 10.0494, Val Acc: 0.5777, ROC-AUC: 0.6089
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0650
Batch 20000/57815 completed, running loss: 0.0643
Batch 30000/57815 completed, running loss: 0.0683
Batch 40000/57815 completed, running loss: 0.0668
Batch 50000/57815 completed, running loss: 0.0656
Starting evaluation with 7618 batches
Fold 5 - Epoch 26/50
Train Loss: 0.0662, Train Acc: 0.9910
Val Loss: 10.8145, Val Acc: 0.5718, ROC-AUC: 0.5981
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0603
Batch 20000/57815 completed, running loss: 0.0614
Batch 30000/57815 completed, running loss: 0.0647
Batch 40000/57815 completed, running loss: 0.0631
Batch 50000/57815 completed, running loss: 0.0630
Starting evaluation with 7618 batches
Fold 5 - Epoch 27/50
Train Loss: 0.0625, Train Acc: 0.9915
Val Loss: 10.9379, Val Acc: 0.5736, ROC-AUC: 0.6017
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0574
Batch 20000/57815 completed, running loss: 0.0551
Batch 30000/57815 completed, running loss: 0.0572
Batch 40000/57815 completed, running loss: 0.0593
Batch 50000/57815 completed, running loss: 0.0627
Starting evaluation with 7618 batches
Fold 5 - Epoch 28/50
Train Loss: 0.0625, Train Acc: 0.9913
Val Loss: 11.0400, Val Acc: 0.5738, ROC-AUC: 0.6014
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0559
Batch 20000/57815 completed, running loss: 0.0628
Batch 30000/57815 completed, running loss: 0.0624
Batch 40000/57815 completed, running loss: 0.0602
Batch 50000/57815 completed, running loss: 0.0607
Starting evaluation with 7618 batches
Fold 5 - Epoch 29/50
Train Loss: 0.0605, Train Acc: 0.9919
Val Loss: 10.9095, Val Acc: 0.5734, ROC-AUC: 0.6012
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0478
Batch 20000/57815 completed, running loss: 0.0532
Batch 30000/57815 completed, running loss: 0.0534
Batch 40000/57815 completed, running loss: 0.0541
Batch 50000/57815 completed, running loss: 0.0552
Starting evaluation with 7618 batches
Fold 5 - Epoch 30/50
Train Loss: 0.0570, Train Acc: 0.9922
Val Loss: 11.5267, Val Acc: 0.5718, ROC-AUC: 0.6028
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0545
Batch 20000/57815 completed, running loss: 0.0596
Batch 30000/57815 completed, running loss: 0.0589
Batch 40000/57815 completed, running loss: 0.0596
Batch 50000/57815 completed, running loss: 0.0614
Starting evaluation with 7618 batches
Fold 5 - Epoch 31/50
Train Loss: 0.0602, Train Acc: 0.9923
Val Loss: 11.3802, Val Acc: 0.5736, ROC-AUC: 0.6038
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0529
Batch 20000/57815 completed, running loss: 0.0578
Batch 30000/57815 completed, running loss: 0.0571
Batch 40000/57815 completed, running loss: 0.0581
Batch 50000/57815 completed, running loss: 0.0574
Starting evaluation with 7618 batches
Fold 5 - Epoch 32/50
Train Loss: 0.0587, Train Acc: 0.9923
Val Loss: 11.4205, Val Acc: 0.5714, ROC-AUC: 0.6015
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0528
Batch 20000/57815 completed, running loss: 0.0512
Batch 30000/57815 completed, running loss: 0.0508
Batch 40000/57815 completed, running loss: 0.0528
Batch 50000/57815 completed, running loss: 0.0535
Starting evaluation with 7618 batches
Fold 5 - Epoch 33/50
Train Loss: 0.0553, Train Acc: 0.9927
Val Loss: 11.5866, Val Acc: 0.5746, ROC-AUC: 0.6042
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0570
Batch 20000/57815 completed, running loss: 0.0550
Batch 30000/57815 completed, running loss: 0.0538
Batch 40000/57815 completed, running loss: 0.0554
Batch 50000/57815 completed, running loss: 0.0550
Starting evaluation with 7618 batches
Fold 5 - Epoch 34/50
Train Loss: 0.0562, Train Acc: 0.9925
Val Loss: 11.4770, Val Acc: 0.5766, ROC-AUC: 0.6040
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.0460
Batch 20000/57815 completed, running loss: 0.0507
Batch 30000/57815 completed, running loss: 0.0537
Batch 40000/57815 completed, running loss: 0.0557
Batch 50000/57815 completed, running loss: 0.0566
Starting evaluation with 7618 batches
Fold 5 - Epoch 35/50
Train Loss: 0.0565, Train Acc: 0.9927
Val Loss: 11.7957, Val Acc: 0.5698, ROC-AUC: 0.5961
Fold 5 - Early stopping triggered
Starting evaluation with 57815 batches
Evaluation batch 10000/57815 completed
Evaluation batch 20000/57815 completed
Evaluation batch 30000/57815 completed
Evaluation batch 40000/57815 completed
Evaluation batch 50000/57815 completed
Starting evaluation with 7618 batches
Starting evaluation with 7038 batches

Fold 5 - Train Metrics:
  Loss: 0.0801
  Accuracy: 0.9726
  Precision: 0.9665
  Recall: 0.9765
  Roc_auc: 0.9967
  Specificity: 0.9691
  F1: 0.9714

Fold 5 - Validation Metrics:
  Loss: 3.3423
  Accuracy: 0.5856
  Precision: 0.5319
  Recall: 0.5745
  Roc_auc: 0.6239
  Specificity: 0.5945
  F1: 0.5524

Fold 5 - Test Metrics:
  Loss: 3.2461
  Accuracy: 0.5882
  Precision: 0.5907
  Recall: 0.5519
  Roc_auc: 0.6335
  Specificity: 0.6239
  F1: 0.5706

=== Fold 6/10 ===
Fold 6 - Training label counts: {0: 120477, 1: 113053}
Fold 6 - Validation label counts: {0: 14475, 1: 11870}
Fold 6 - Test label counts: {0: 17199, 1: 12804}
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.6414
Batch 20000/58383 completed, running loss: 0.6245
Batch 30000/58383 completed, running loss: 0.6132
Batch 40000/58383 completed, running loss: 0.6050
Batch 50000/58383 completed, running loss: 0.5989
Starting evaluation with 6587 batches
Fold 6 - Epoch 1/50
Train Loss: 0.5944, Train Acc: 0.6916
Val Loss: 0.8181, Val Acc: 0.6143, ROC-AUC: 0.6727
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.4704
Batch 20000/58383 completed, running loss: 0.4810
Batch 30000/58383 completed, running loss: 0.4889
Batch 40000/58383 completed, running loss: 0.4914
Batch 50000/58383 completed, running loss: 0.4954
Starting evaluation with 6587 batches
Fold 6 - Epoch 2/50
Train Loss: 0.4966, Train Acc: 0.7892
Val Loss: 1.2295, Val Acc: 0.5973, ROC-AUC: 0.6513
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.3810
Batch 20000/58383 completed, running loss: 0.3923
Batch 30000/58383 completed, running loss: 0.4008
Batch 40000/58383 completed, running loss: 0.4101
Batch 50000/58383 completed, running loss: 0.4180
Starting evaluation with 6587 batches
Fold 6 - Epoch 3/50
Train Loss: 0.4233, Train Acc: 0.8522
Val Loss: 1.8185, Val Acc: 0.5989, ROC-AUC: 0.6491
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.3028
Batch 20000/58383 completed, running loss: 0.3191
Batch 30000/58383 completed, running loss: 0.3307
Batch 40000/58383 completed, running loss: 0.3411
Batch 50000/58383 completed, running loss: 0.3503
Starting evaluation with 6587 batches
Fold 6 - Epoch 4/50
Train Loss: 0.3573, Train Acc: 0.8949
Val Loss: 2.5259, Val Acc: 0.6036, ROC-AUC: 0.6516
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.2594
Batch 20000/58383 completed, running loss: 0.2734
Batch 30000/58383 completed, running loss: 0.2794
Batch 40000/58383 completed, running loss: 0.2900
Batch 50000/58383 completed, running loss: 0.2990
Starting evaluation with 6587 batches
Fold 6 - Epoch 5/50
Train Loss: 0.3058, Train Acc: 0.9221
Val Loss: 3.4931, Val Acc: 0.6125, ROC-AUC: 0.6628
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.2234
Batch 20000/58383 completed, running loss: 0.2369
Batch 30000/58383 completed, running loss: 0.2496
Batch 40000/58383 completed, running loss: 0.2567
Batch 50000/58383 completed, running loss: 0.2643
Starting evaluation with 6587 batches
Fold 6 - Epoch 6/50
Train Loss: 0.2716, Train Acc: 0.9378
Val Loss: 4.2909, Val Acc: 0.5945, ROC-AUC: 0.6350
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.2011
Batch 20000/58383 completed, running loss: 0.2164
Batch 30000/58383 completed, running loss: 0.2235
Batch 40000/58383 completed, running loss: 0.2339
Batch 50000/58383 completed, running loss: 0.2395
Starting evaluation with 6587 batches
Fold 6 - Epoch 7/50
Train Loss: 0.2442, Train Acc: 0.9482
Val Loss: 4.8216, Val Acc: 0.6008, ROC-AUC: 0.6472
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1904
Batch 20000/58383 completed, running loss: 0.1991
Batch 30000/58383 completed, running loss: 0.2081
Batch 40000/58383 completed, running loss: 0.2145
Batch 50000/58383 completed, running loss: 0.2222
Starting evaluation with 6587 batches
Fold 6 - Epoch 8/50
Train Loss: 0.2270, Train Acc: 0.9554
Val Loss: 5.1360, Val Acc: 0.6013, ROC-AUC: 0.6425
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1775
Batch 20000/58383 completed, running loss: 0.1818
Batch 30000/58383 completed, running loss: 0.1862
Batch 40000/58383 completed, running loss: 0.1925
Batch 50000/58383 completed, running loss: 0.2040
Starting evaluation with 6587 batches
Fold 6 - Epoch 9/50
Train Loss: 0.2115, Train Acc: 0.9604
Val Loss: 6.0058, Val Acc: 0.5998, ROC-AUC: 0.6383
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1681
Batch 20000/58383 completed, running loss: 0.1784
Batch 30000/58383 completed, running loss: 0.1835
Batch 40000/58383 completed, running loss: 0.1902
Batch 50000/58383 completed, running loss: 0.1931
Starting evaluation with 6587 batches
Fold 6 - Epoch 10/50
Train Loss: 0.1966, Train Acc: 0.9644
Val Loss: 6.1246, Val Acc: 0.6130, ROC-AUC: 0.6530
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1574
Batch 20000/58383 completed, running loss: 0.1672
Batch 30000/58383 completed, running loss: 0.1766
Batch 40000/58383 completed, running loss: 0.1816
Batch 50000/58383 completed, running loss: 0.1842
Starting evaluation with 6587 batches
Fold 6 - Epoch 11/50
Train Loss: 0.1878, Train Acc: 0.9674
Val Loss: 6.1287, Val Acc: 0.6091, ROC-AUC: 0.6515
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1500
Batch 20000/58383 completed, running loss: 0.1545
Batch 30000/58383 completed, running loss: 0.1631
Batch 40000/58383 completed, running loss: 0.1689
Batch 50000/58383 completed, running loss: 0.1728
Starting evaluation with 6587 batches
Fold 6 - Epoch 12/50
Train Loss: 0.1753, Train Acc: 0.9696
Val Loss: 6.6596, Val Acc: 0.6104, ROC-AUC: 0.6501
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1441
Batch 20000/58383 completed, running loss: 0.1507
Batch 30000/58383 completed, running loss: 0.1565
Batch 40000/58383 completed, running loss: 0.1593
Batch 50000/58383 completed, running loss: 0.1644
Starting evaluation with 6587 batches
Fold 6 - Epoch 13/50
Train Loss: 0.1671, Train Acc: 0.9719
Val Loss: 7.2554, Val Acc: 0.6089, ROC-AUC: 0.6483
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1475
Batch 20000/58383 completed, running loss: 0.1476
Batch 30000/58383 completed, running loss: 0.1546
Batch 40000/58383 completed, running loss: 0.1567
Batch 50000/58383 completed, running loss: 0.1600
Starting evaluation with 6587 batches
Fold 6 - Epoch 14/50
Train Loss: 0.1636, Train Acc: 0.9735
Val Loss: 7.4476, Val Acc: 0.6117, ROC-AUC: 0.6499
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1422
Batch 20000/58383 completed, running loss: 0.1426
Batch 30000/58383 completed, running loss: 0.1435
Batch 40000/58383 completed, running loss: 0.1474
Batch 50000/58383 completed, running loss: 0.1498
Starting evaluation with 6587 batches
Fold 6 - Epoch 15/50
Train Loss: 0.1535, Train Acc: 0.9756
Val Loss: 7.5050, Val Acc: 0.6039, ROC-AUC: 0.6403
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1252
Batch 20000/58383 completed, running loss: 0.1337
Batch 30000/58383 completed, running loss: 0.1378
Batch 40000/58383 completed, running loss: 0.1408
Batch 50000/58383 completed, running loss: 0.1429
Starting evaluation with 6587 batches
Fold 6 - Epoch 16/50
Train Loss: 0.1473, Train Acc: 0.9767
Val Loss: 7.9621, Val Acc: 0.5997, ROC-AUC: 0.6366
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.1249
Batch 20000/58383 completed, running loss: 0.1257
Batch 30000/58383 completed, running loss: 0.1316
Batch 40000/58383 completed, running loss: 0.1368
Batch 50000/58383 completed, running loss: 0.1417
Starting evaluation with 6587 batches
Fold 6 - Epoch 17/50
Train Loss: 0.1444, Train Acc: 0.9778
Val Loss: 7.9522, Val Acc: 0.6076, ROC-AUC: 0.6444
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0951
Batch 20000/58383 completed, running loss: 0.0961
Batch 30000/58383 completed, running loss: 0.0950
Batch 40000/58383 completed, running loss: 0.0956
Batch 50000/58383 completed, running loss: 0.0971
Starting evaluation with 6587 batches
Fold 6 - Epoch 18/50
Train Loss: 0.0990, Train Acc: 0.9848
Val Loss: 8.3586, Val Acc: 0.6042, ROC-AUC: 0.6462
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0782
Batch 20000/58383 completed, running loss: 0.0815
Batch 30000/58383 completed, running loss: 0.0824
Batch 40000/58383 completed, running loss: 0.0831
Batch 50000/58383 completed, running loss: 0.0829
Starting evaluation with 6587 batches
Fold 6 - Epoch 19/50
Train Loss: 0.0839, Train Acc: 0.9872
Val Loss: 9.1202, Val Acc: 0.6068, ROC-AUC: 0.6429
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0742
Batch 20000/58383 completed, running loss: 0.0788
Batch 30000/58383 completed, running loss: 0.0809
Batch 40000/58383 completed, running loss: 0.0806
Batch 50000/58383 completed, running loss: 0.0785
Starting evaluation with 6587 batches
Fold 6 - Epoch 20/50
Train Loss: 0.0779, Train Acc: 0.9884
Val Loss: 9.4003, Val Acc: 0.6068, ROC-AUC: 0.6453
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0692
Batch 20000/58383 completed, running loss: 0.0666
Batch 30000/58383 completed, running loss: 0.0679
Batch 40000/58383 completed, running loss: 0.0708
Batch 50000/58383 completed, running loss: 0.0721
Starting evaluation with 6587 batches
Fold 6 - Epoch 21/50
Train Loss: 0.0725, Train Acc: 0.9892
Val Loss: 9.7722, Val Acc: 0.6060, ROC-AUC: 0.6419
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0628
Batch 20000/58383 completed, running loss: 0.0627
Batch 30000/58383 completed, running loss: 0.0631
Batch 40000/58383 completed, running loss: 0.0650
Batch 50000/58383 completed, running loss: 0.0668
Starting evaluation with 6587 batches
Fold 6 - Epoch 22/50
Train Loss: 0.0677, Train Acc: 0.9898
Val Loss: 10.3011, Val Acc: 0.6051, ROC-AUC: 0.6419
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0621
Batch 20000/58383 completed, running loss: 0.0630
Batch 30000/58383 completed, running loss: 0.0652
Batch 40000/58383 completed, running loss: 0.0658
Batch 50000/58383 completed, running loss: 0.0671
Starting evaluation with 6587 batches
Fold 6 - Epoch 23/50
Train Loss: 0.0687, Train Acc: 0.9905
Val Loss: 10.4365, Val Acc: 0.6000, ROC-AUC: 0.6347
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0622
Batch 20000/58383 completed, running loss: 0.0628
Batch 30000/58383 completed, running loss: 0.0635
Batch 40000/58383 completed, running loss: 0.0641
Batch 50000/58383 completed, running loss: 0.0655
Starting evaluation with 6587 batches
Fold 6 - Epoch 24/50
Train Loss: 0.0648, Train Acc: 0.9909
Val Loss: 10.4717, Val Acc: 0.6028, ROC-AUC: 0.6386
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0565
Batch 20000/58383 completed, running loss: 0.0626
Batch 30000/58383 completed, running loss: 0.0655
Batch 40000/58383 completed, running loss: 0.0649
Batch 50000/58383 completed, running loss: 0.0654
Starting evaluation with 6587 batches
Fold 6 - Epoch 25/50
Train Loss: 0.0654, Train Acc: 0.9906
Val Loss: 10.4463, Val Acc: 0.6088, ROC-AUC: 0.6410
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0566
Batch 20000/58383 completed, running loss: 0.0609
Batch 30000/58383 completed, running loss: 0.0638
Batch 40000/58383 completed, running loss: 0.0647
Batch 50000/58383 completed, running loss: 0.0671
Starting evaluation with 6587 batches
Fold 6 - Epoch 26/50
Train Loss: 0.0686, Train Acc: 0.9908
Val Loss: 10.8393, Val Acc: 0.6006, ROC-AUC: 0.6378
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0579
Batch 20000/58383 completed, running loss: 0.0636
Batch 30000/58383 completed, running loss: 0.0645
Batch 40000/58383 completed, running loss: 0.0620
Batch 50000/58383 completed, running loss: 0.0632
Starting evaluation with 6587 batches
Fold 6 - Epoch 27/50
Train Loss: 0.0656, Train Acc: 0.9911
Val Loss: 10.9176, Val Acc: 0.6022, ROC-AUC: 0.6380
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0567
Batch 20000/58383 completed, running loss: 0.0647
Batch 30000/58383 completed, running loss: 0.0633
Batch 40000/58383 completed, running loss: 0.0632
Batch 50000/58383 completed, running loss: 0.0624
Starting evaluation with 6587 batches
Fold 6 - Epoch 28/50
Train Loss: 0.0637, Train Acc: 0.9913
Val Loss: 10.9821, Val Acc: 0.6009, ROC-AUC: 0.6334
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0565
Batch 20000/58383 completed, running loss: 0.0606
Batch 30000/58383 completed, running loss: 0.0598
Batch 40000/58383 completed, running loss: 0.0586
Batch 50000/58383 completed, running loss: 0.0596
Starting evaluation with 6587 batches
Fold 6 - Epoch 29/50
Train Loss: 0.0612, Train Acc: 0.9918
Val Loss: 11.2724, Val Acc: 0.6032, ROC-AUC: 0.6386
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0723
Batch 20000/58383 completed, running loss: 0.0682
Batch 30000/58383 completed, running loss: 0.0664
Batch 40000/58383 completed, running loss: 0.0640
Batch 50000/58383 completed, running loss: 0.0622
Starting evaluation with 6587 batches
Fold 6 - Epoch 30/50
Train Loss: 0.0623, Train Acc: 0.9917
Val Loss: 11.0622, Val Acc: 0.6043, ROC-AUC: 0.6404
Starting training epoch with 58383 batches
Batch 10000/58383 completed, running loss: 0.0515
Batch 20000/58383 completed, running loss: 0.0557
Batch 30000/58383 completed, running loss: 0.0585
Batch 40000/58383 completed, running loss: 0.0597
Batch 50000/58383 completed, running loss: 0.0598
Starting evaluation with 6587 batches
Fold 6 - Epoch 31/50
Train Loss: 0.0623, Train Acc: 0.9915
Val Loss: 11.5296, Val Acc: 0.6025, ROC-AUC: 0.6389
Fold 6 - Early stopping triggered
Starting evaluation with 58383 batches
Evaluation batch 10000/58383 completed
Evaluation batch 20000/58383 completed
Evaluation batch 30000/58383 completed
Evaluation batch 40000/58383 completed
Evaluation batch 50000/58383 completed
Starting evaluation with 6587 batches
Starting evaluation with 7501 batches

Fold 6 - Train Metrics:
  Loss: 0.4008
  Accuracy: 0.8051
  Precision: 0.7951
  Recall: 0.8048
  Roc_auc: 0.8981
  Specificity: 0.8054
  F1: 0.7999

Fold 6 - Validation Metrics:
  Loss: 0.8181
  Accuracy: 0.6143
  Precision: 0.5690
  Recall: 0.5936
  Roc_auc: 0.6727
  Specificity: 0.6312
  F1: 0.5810

Fold 6 - Test Metrics:
  Loss: 0.8685
  Accuracy: 0.5868
  Precision: 0.5151
  Recall: 0.5407
  Roc_auc: 0.6210
  Specificity: 0.6211
  F1: 0.5276

=== Fold 7/10 ===
Fold 7 - Training label counts: {0: 123489, 1: 113593}
Fold 7 - Validation label counts: {1: 15367, 0: 14984}
Fold 7 - Test label counts: {0: 13678, 1: 8767}
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.6430
Batch 20000/59271 completed, running loss: 0.6242
Batch 30000/59271 completed, running loss: 0.6117
Batch 40000/59271 completed, running loss: 0.6036
Batch 50000/59271 completed, running loss: 0.5969
Starting evaluation with 7588 batches
Fold 7 - Epoch 1/50
Train Loss: 0.5921, Train Acc: 0.6955
Val Loss: 0.8543, Val Acc: 0.6331, ROC-AUC: 0.6736
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.4715
Batch 20000/59271 completed, running loss: 0.4807
Batch 30000/59271 completed, running loss: 0.4871
Batch 40000/59271 completed, running loss: 0.4908
Batch 50000/59271 completed, running loss: 0.4943
Starting evaluation with 7588 batches
Fold 7 - Epoch 2/50
Train Loss: 0.4966, Train Acc: 0.7941
Val Loss: 1.1075, Val Acc: 0.6337, ROC-AUC: 0.6776
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.3694
Batch 20000/59271 completed, running loss: 0.3877
Batch 30000/59271 completed, running loss: 0.3973
Batch 40000/59271 completed, running loss: 0.4065
Batch 50000/59271 completed, running loss: 0.4142
Starting evaluation with 7588 batches
Fold 7 - Epoch 3/50
Train Loss: 0.4205, Train Acc: 0.8571
Val Loss: 1.6418, Val Acc: 0.6046, ROC-AUC: 0.6385
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.2916
Batch 20000/59271 completed, running loss: 0.3114
Batch 30000/59271 completed, running loss: 0.3272
Batch 40000/59271 completed, running loss: 0.3383
Batch 50000/59271 completed, running loss: 0.3455
Starting evaluation with 7588 batches
Fold 7 - Epoch 4/50
Train Loss: 0.3546, Train Acc: 0.8984
Val Loss: 2.4023, Val Acc: 0.6140, ROC-AUC: 0.6527
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.2380
Batch 20000/59271 completed, running loss: 0.2570
Batch 30000/59271 completed, running loss: 0.2713
Batch 40000/59271 completed, running loss: 0.2820
Batch 50000/59271 completed, running loss: 0.2886
Starting evaluation with 7588 batches
Fold 7 - Epoch 5/50
Train Loss: 0.2975, Train Acc: 0.9247
Val Loss: 3.3336, Val Acc: 0.6121, ROC-AUC: 0.6475
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.2105
Batch 20000/59271 completed, running loss: 0.2255
Batch 30000/59271 completed, running loss: 0.2373
Batch 40000/59271 completed, running loss: 0.2473
Batch 50000/59271 completed, running loss: 0.2589
Starting evaluation with 7588 batches
Fold 7 - Epoch 6/50
Train Loss: 0.2670, Train Acc: 0.9409
Val Loss: 4.0658, Val Acc: 0.6014, ROC-AUC: 0.6363
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1941
Batch 20000/59271 completed, running loss: 0.2073
Batch 30000/59271 completed, running loss: 0.2152
Batch 40000/59271 completed, running loss: 0.2258
Batch 50000/59271 completed, running loss: 0.2319
Starting evaluation with 7588 batches
Fold 7 - Epoch 7/50
Train Loss: 0.2370, Train Acc: 0.9505
Val Loss: 4.7683, Val Acc: 0.5965, ROC-AUC: 0.6277
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1860
Batch 20000/59271 completed, running loss: 0.1909
Batch 30000/59271 completed, running loss: 0.1977
Batch 40000/59271 completed, running loss: 0.2052
Batch 50000/59271 completed, running loss: 0.2128
Starting evaluation with 7588 batches
Fold 7 - Epoch 8/50
Train Loss: 0.2188, Train Acc: 0.9572
Val Loss: 5.2308, Val Acc: 0.5992, ROC-AUC: 0.6379
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1794
Batch 20000/59271 completed, running loss: 0.1874
Batch 30000/59271 completed, running loss: 0.1949
Batch 40000/59271 completed, running loss: 0.2019
Batch 50000/59271 completed, running loss: 0.2044
Starting evaluation with 7588 batches
Fold 7 - Epoch 9/50
Train Loss: 0.2088, Train Acc: 0.9618
Val Loss: 5.7492, Val Acc: 0.5997, ROC-AUC: 0.6330
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1697
Batch 20000/59271 completed, running loss: 0.1748
Batch 30000/59271 completed, running loss: 0.1778
Batch 40000/59271 completed, running loss: 0.1822
Batch 50000/59271 completed, running loss: 0.1900
Starting evaluation with 7588 batches
Fold 7 - Epoch 10/50
Train Loss: 0.1922, Train Acc: 0.9660
Val Loss: 5.9080, Val Acc: 0.6074, ROC-AUC: 0.6398
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1556
Batch 20000/59271 completed, running loss: 0.1637
Batch 30000/59271 completed, running loss: 0.1710
Batch 40000/59271 completed, running loss: 0.1758
Batch 50000/59271 completed, running loss: 0.1812
Starting evaluation with 7588 batches
Fold 7 - Epoch 11/50
Train Loss: 0.1834, Train Acc: 0.9687
Val Loss: 6.2662, Val Acc: 0.6050, ROC-AUC: 0.6408
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1485
Batch 20000/59271 completed, running loss: 0.1560
Batch 30000/59271 completed, running loss: 0.1650
Batch 40000/59271 completed, running loss: 0.1690
Batch 50000/59271 completed, running loss: 0.1717
Starting evaluation with 7588 batches
Fold 7 - Epoch 12/50
Train Loss: 0.1751, Train Acc: 0.9709
Val Loss: 7.0002, Val Acc: 0.6042, ROC-AUC: 0.6412
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1456
Batch 20000/59271 completed, running loss: 0.1463
Batch 30000/59271 completed, running loss: 0.1519
Batch 40000/59271 completed, running loss: 0.1571
Batch 50000/59271 completed, running loss: 0.1623
Starting evaluation with 7588 batches
Fold 7 - Epoch 13/50
Train Loss: 0.1656, Train Acc: 0.9732
Val Loss: 7.0227, Val Acc: 0.6012, ROC-AUC: 0.6324
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1408
Batch 20000/59271 completed, running loss: 0.1409
Batch 30000/59271 completed, running loss: 0.1465
Batch 40000/59271 completed, running loss: 0.1490
Batch 50000/59271 completed, running loss: 0.1523
Starting evaluation with 7588 batches
Fold 7 - Epoch 14/50
Train Loss: 0.1563, Train Acc: 0.9746
Val Loss: 6.7413, Val Acc: 0.6078, ROC-AUC: 0.6424
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1262
Batch 20000/59271 completed, running loss: 0.1358
Batch 30000/59271 completed, running loss: 0.1361
Batch 40000/59271 completed, running loss: 0.1423
Batch 50000/59271 completed, running loss: 0.1471
Starting evaluation with 7588 batches
Fold 7 - Epoch 15/50
Train Loss: 0.1491, Train Acc: 0.9762
Val Loss: 7.4900, Val Acc: 0.5997, ROC-AUC: 0.6316
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1425
Batch 20000/59271 completed, running loss: 0.1319
Batch 30000/59271 completed, running loss: 0.1359
Batch 40000/59271 completed, running loss: 0.1369
Batch 50000/59271 completed, running loss: 0.1400
Starting evaluation with 7588 batches
Fold 7 - Epoch 16/50
Train Loss: 0.1425, Train Acc: 0.9768
Val Loss: 7.5355, Val Acc: 0.6014, ROC-AUC: 0.6343
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1251
Batch 20000/59271 completed, running loss: 0.1323
Batch 30000/59271 completed, running loss: 0.1299
Batch 40000/59271 completed, running loss: 0.1323
Batch 50000/59271 completed, running loss: 0.1360
Starting evaluation with 7588 batches
Fold 7 - Epoch 17/50
Train Loss: 0.1373, Train Acc: 0.9787
Val Loss: 7.9732, Val Acc: 0.6064, ROC-AUC: 0.6400
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.1261
Batch 20000/59271 completed, running loss: 0.1280
Batch 30000/59271 completed, running loss: 0.1322
Batch 40000/59271 completed, running loss: 0.1330
Batch 50000/59271 completed, running loss: 0.1343
Starting evaluation with 7588 batches
Fold 7 - Epoch 18/50
Train Loss: 0.1365, Train Acc: 0.9793
Val Loss: 7.5061, Val Acc: 0.6030, ROC-AUC: 0.6352
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0979
Batch 20000/59271 completed, running loss: 0.0934
Batch 30000/59271 completed, running loss: 0.0946
Batch 40000/59271 completed, running loss: 0.0947
Batch 50000/59271 completed, running loss: 0.0944
Starting evaluation with 7588 batches
Fold 7 - Epoch 19/50
Train Loss: 0.0936, Train Acc: 0.9855
Val Loss: 8.4119, Val Acc: 0.6031, ROC-AUC: 0.6354
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0819
Batch 20000/59271 completed, running loss: 0.0781
Batch 30000/59271 completed, running loss: 0.0803
Batch 40000/59271 completed, running loss: 0.0826
Batch 50000/59271 completed, running loss: 0.0831
Starting evaluation with 7588 batches
Fold 7 - Epoch 20/50
Train Loss: 0.0836, Train Acc: 0.9877
Val Loss: 8.6622, Val Acc: 0.6050, ROC-AUC: 0.6384
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0687
Batch 20000/59271 completed, running loss: 0.0679
Batch 30000/59271 completed, running loss: 0.0681
Batch 40000/59271 completed, running loss: 0.0701
Batch 50000/59271 completed, running loss: 0.0717
Starting evaluation with 7588 batches
Fold 7 - Epoch 21/50
Train Loss: 0.0723, Train Acc: 0.9894
Val Loss: 9.1054, Val Acc: 0.6047, ROC-AUC: 0.6390
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0721
Batch 20000/59271 completed, running loss: 0.0682
Batch 30000/59271 completed, running loss: 0.0685
Batch 40000/59271 completed, running loss: 0.0694
Batch 50000/59271 completed, running loss: 0.0701
Starting evaluation with 7588 batches
Fold 7 - Epoch 22/50
Train Loss: 0.0692, Train Acc: 0.9898
Val Loss: 9.7173, Val Acc: 0.6096, ROC-AUC: 0.6398
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0629
Batch 20000/59271 completed, running loss: 0.0633
Batch 30000/59271 completed, running loss: 0.0654
Batch 40000/59271 completed, running loss: 0.0661
Batch 50000/59271 completed, running loss: 0.0668
Starting evaluation with 7588 batches
Fold 7 - Epoch 23/50
Train Loss: 0.0691, Train Acc: 0.9901
Val Loss: 9.6906, Val Acc: 0.6032, ROC-AUC: 0.6356
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0650
Batch 20000/59271 completed, running loss: 0.0703
Batch 30000/59271 completed, running loss: 0.0714
Batch 40000/59271 completed, running loss: 0.0710
Batch 50000/59271 completed, running loss: 0.0678
Starting evaluation with 7588 batches
Fold 7 - Epoch 24/50
Train Loss: 0.0685, Train Acc: 0.9907
Val Loss: 10.0543, Val Acc: 0.6075, ROC-AUC: 0.6370
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0534
Batch 20000/59271 completed, running loss: 0.0569
Batch 30000/59271 completed, running loss: 0.0589
Batch 40000/59271 completed, running loss: 0.0611
Batch 50000/59271 completed, running loss: 0.0627
Starting evaluation with 7588 batches
Fold 7 - Epoch 25/50
Train Loss: 0.0627, Train Acc: 0.9910
Val Loss: 9.7953, Val Acc: 0.6076, ROC-AUC: 0.6409
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0469
Batch 20000/59271 completed, running loss: 0.0524
Batch 30000/59271 completed, running loss: 0.0547
Batch 40000/59271 completed, running loss: 0.0582
Batch 50000/59271 completed, running loss: 0.0619
Starting evaluation with 7588 batches
Fold 7 - Epoch 26/50
Train Loss: 0.0623, Train Acc: 0.9913
Val Loss: 10.1246, Val Acc: 0.6085, ROC-AUC: 0.6403
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0551
Batch 20000/59271 completed, running loss: 0.0526
Batch 30000/59271 completed, running loss: 0.0563
Batch 40000/59271 completed, running loss: 0.0586
Batch 50000/59271 completed, running loss: 0.0583
Starting evaluation with 7588 batches
Fold 7 - Epoch 27/50
Train Loss: 0.0594, Train Acc: 0.9915
Val Loss: 10.2276, Val Acc: 0.6022, ROC-AUC: 0.6333
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0584
Batch 20000/59271 completed, running loss: 0.0630
Batch 30000/59271 completed, running loss: 0.0602
Batch 40000/59271 completed, running loss: 0.0605
Batch 50000/59271 completed, running loss: 0.0596
Starting evaluation with 7588 batches
Fold 7 - Epoch 28/50
Train Loss: 0.0603, Train Acc: 0.9918
Val Loss: 11.0210, Val Acc: 0.6029, ROC-AUC: 0.6330
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0557
Batch 20000/59271 completed, running loss: 0.0560
Batch 30000/59271 completed, running loss: 0.0582
Batch 40000/59271 completed, running loss: 0.0577
Batch 50000/59271 completed, running loss: 0.0598
Starting evaluation with 7588 batches
Fold 7 - Epoch 29/50
Train Loss: 0.0610, Train Acc: 0.9918
Val Loss: 10.7346, Val Acc: 0.6087, ROC-AUC: 0.6395
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0587
Batch 20000/59271 completed, running loss: 0.0596
Batch 30000/59271 completed, running loss: 0.0617
Batch 40000/59271 completed, running loss: 0.0593
Batch 50000/59271 completed, running loss: 0.0569
Starting evaluation with 7588 batches
Fold 7 - Epoch 30/50
Train Loss: 0.0584, Train Acc: 0.9921
Val Loss: 11.2501, Val Acc: 0.5991, ROC-AUC: 0.6254
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0561
Batch 20000/59271 completed, running loss: 0.0586
Batch 30000/59271 completed, running loss: 0.0614
Batch 40000/59271 completed, running loss: 0.0628
Batch 50000/59271 completed, running loss: 0.0621
Starting evaluation with 7588 batches
Fold 7 - Epoch 31/50
Train Loss: 0.0615, Train Acc: 0.9920
Val Loss: 10.7060, Val Acc: 0.6075, ROC-AUC: 0.6396
Starting training epoch with 59271 batches
Batch 10000/59271 completed, running loss: 0.0474
Batch 20000/59271 completed, running loss: 0.0486
Batch 30000/59271 completed, running loss: 0.0532
Batch 40000/59271 completed, running loss: 0.0554
Batch 50000/59271 completed, running loss: 0.0550
Starting evaluation with 7588 batches
Fold 7 - Epoch 32/50
Train Loss: 0.0560, Train Acc: 0.9922
Val Loss: 11.2928, Val Acc: 0.6069, ROC-AUC: 0.6358
Fold 7 - Early stopping triggered
Starting evaluation with 59271 batches
Evaluation batch 10000/59271 completed
Evaluation batch 20000/59271 completed
Evaluation batch 30000/59271 completed
Evaluation batch 40000/59271 completed
Evaluation batch 50000/59271 completed
Starting evaluation with 7588 batches
Starting evaluation with 5612 batches

Fold 7 - Train Metrics:
  Loss: 0.2679
  Accuracy: 0.8847
  Precision: 0.8762
  Recall: 0.8842
  Roc_auc: 0.9569
  Specificity: 0.8851
  F1: 0.8802

Fold 7 - Validation Metrics:
  Loss: 1.1075
  Accuracy: 0.6337
  Precision: 0.6485
  Recall: 0.6038
  Roc_auc: 0.6776
  Specificity: 0.6644
  F1: 0.6253

Fold 7 - Test Metrics:
  Loss: 1.3302
  Accuracy: 0.5604
  Precision: 0.4486
  Recall: 0.5474
  Roc_auc: 0.5951
  Specificity: 0.5687
  F1: 0.4931

=== Fold 8/10 ===
Fold 8 - Training label counts: {0: 122332, 1: 107548}
Fold 8 - Validation label counts: {0: 16257, 1: 13675}
Fold 8 - Test label counts: {1: 16504, 0: 13562}
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.6443
Batch 20000/57470 completed, running loss: 0.6212
Batch 30000/57470 completed, running loss: 0.6087
Batch 40000/57470 completed, running loss: 0.5993
Batch 50000/57470 completed, running loss: 0.5916
Starting evaluation with 7483 batches
Fold 8 - Epoch 1/50
Train Loss: 0.5871, Train Acc: 0.6983
Val Loss: 0.9023, Val Acc: 0.5925, ROC-AUC: 0.6310
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.4589
Batch 20000/57470 completed, running loss: 0.4709
Batch 30000/57470 completed, running loss: 0.4766
Batch 40000/57470 completed, running loss: 0.4811
Batch 50000/57470 completed, running loss: 0.4848
Starting evaluation with 7483 batches
Fold 8 - Epoch 2/50
Train Loss: 0.4874, Train Acc: 0.8002
Val Loss: 1.3226, Val Acc: 0.5614, ROC-AUC: 0.5939
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.3598
Batch 20000/57470 completed, running loss: 0.3786
Batch 30000/57470 completed, running loss: 0.3895
Batch 40000/57470 completed, running loss: 0.3996
Batch 50000/57470 completed, running loss: 0.4077
Starting evaluation with 7483 batches
Fold 8 - Epoch 3/50
Train Loss: 0.4134, Train Acc: 0.8626
Val Loss: 1.9104, Val Acc: 0.5594, ROC-AUC: 0.5926
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.2854
Batch 20000/57470 completed, running loss: 0.3048
Batch 30000/57470 completed, running loss: 0.3189
Batch 40000/57470 completed, running loss: 0.3300
Batch 50000/57470 completed, running loss: 0.3419
Starting evaluation with 7483 batches
Fold 8 - Epoch 4/50
Train Loss: 0.3478, Train Acc: 0.9031
Val Loss: 2.8548, Val Acc: 0.5568, ROC-AUC: 0.5851
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.2363
Batch 20000/57470 completed, running loss: 0.2547
Batch 30000/57470 completed, running loss: 0.2699
Batch 40000/57470 completed, running loss: 0.2789
Batch 50000/57470 completed, running loss: 0.2889
Starting evaluation with 7483 batches
Fold 8 - Epoch 5/50
Train Loss: 0.2947, Train Acc: 0.9285
Val Loss: 4.0878, Val Acc: 0.5539, ROC-AUC: 0.5881
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.2012
Batch 20000/57470 completed, running loss: 0.2190
Batch 30000/57470 completed, running loss: 0.2335
Batch 40000/57470 completed, running loss: 0.2438
Batch 50000/57470 completed, running loss: 0.2508
Starting evaluation with 7483 batches
Fold 8 - Epoch 6/50
Train Loss: 0.2572, Train Acc: 0.9437
Val Loss: 4.6539, Val Acc: 0.5752, ROC-AUC: 0.6107
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1917
Batch 20000/57470 completed, running loss: 0.2078
Batch 30000/57470 completed, running loss: 0.2169
Batch 40000/57470 completed, running loss: 0.2267
Batch 50000/57470 completed, running loss: 0.2335
Starting evaluation with 7483 batches
Fold 8 - Epoch 7/50
Train Loss: 0.2391, Train Acc: 0.9528
Val Loss: 5.6709, Val Acc: 0.5641, ROC-AUC: 0.5963
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1817
Batch 20000/57470 completed, running loss: 0.1968
Batch 30000/57470 completed, running loss: 0.2020
Batch 40000/57470 completed, running loss: 0.2101
Batch 50000/57470 completed, running loss: 0.2166
Starting evaluation with 7483 batches
Fold 8 - Epoch 8/50
Train Loss: 0.2228, Train Acc: 0.9588
Val Loss: 5.8704, Val Acc: 0.5678, ROC-AUC: 0.5951
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1638
Batch 20000/57470 completed, running loss: 0.1756
Batch 30000/57470 completed, running loss: 0.1844
Batch 40000/57470 completed, running loss: 0.1921
Batch 50000/57470 completed, running loss: 0.1985
Starting evaluation with 7483 batches
Fold 8 - Epoch 9/50
Train Loss: 0.2014, Train Acc: 0.9640
Val Loss: 6.7184, Val Acc: 0.5639, ROC-AUC: 0.5916
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1606
Batch 20000/57470 completed, running loss: 0.1678
Batch 30000/57470 completed, running loss: 0.1741
Batch 40000/57470 completed, running loss: 0.1815
Batch 50000/57470 completed, running loss: 0.1838
Starting evaluation with 7483 batches
Fold 8 - Epoch 10/50
Train Loss: 0.1886, Train Acc: 0.9673
Val Loss: 6.5484, Val Acc: 0.5717, ROC-AUC: 0.6005
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1442
Batch 20000/57470 completed, running loss: 0.1589
Batch 30000/57470 completed, running loss: 0.1623
Batch 40000/57470 completed, running loss: 0.1688
Batch 50000/57470 completed, running loss: 0.1751
Starting evaluation with 7483 batches
Fold 8 - Epoch 11/50
Train Loss: 0.1771, Train Acc: 0.9705
Val Loss: 7.8379, Val Acc: 0.5591, ROC-AUC: 0.5859
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1523
Batch 20000/57470 completed, running loss: 0.1573
Batch 30000/57470 completed, running loss: 0.1640
Batch 40000/57470 completed, running loss: 0.1699
Batch 50000/57470 completed, running loss: 0.1738
Starting evaluation with 7483 batches
Fold 8 - Epoch 12/50
Train Loss: 0.1767, Train Acc: 0.9715
Val Loss: 7.6085, Val Acc: 0.5594, ROC-AUC: 0.5890
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1381
Batch 20000/57470 completed, running loss: 0.1451
Batch 30000/57470 completed, running loss: 0.1508
Batch 40000/57470 completed, running loss: 0.1562
Batch 50000/57470 completed, running loss: 0.1591
Starting evaluation with 7483 batches
Fold 8 - Epoch 13/50
Train Loss: 0.1611, Train Acc: 0.9737
Val Loss: 8.0930, Val Acc: 0.5589, ROC-AUC: 0.5858
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1321
Batch 20000/57470 completed, running loss: 0.1336
Batch 30000/57470 completed, running loss: 0.1445
Batch 40000/57470 completed, running loss: 0.1519
Batch 50000/57470 completed, running loss: 0.1527
Starting evaluation with 7483 batches
Fold 8 - Epoch 14/50
Train Loss: 0.1559, Train Acc: 0.9755
Val Loss: 8.0201, Val Acc: 0.5705, ROC-AUC: 0.6014
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1271
Batch 20000/57470 completed, running loss: 0.1346
Batch 30000/57470 completed, running loss: 0.1436
Batch 40000/57470 completed, running loss: 0.1464
Batch 50000/57470 completed, running loss: 0.1503
Starting evaluation with 7483 batches
Fold 8 - Epoch 15/50
Train Loss: 0.1526, Train Acc: 0.9767
Val Loss: 8.5184, Val Acc: 0.5610, ROC-AUC: 0.5872
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1269
Batch 20000/57470 completed, running loss: 0.1318
Batch 30000/57470 completed, running loss: 0.1365
Batch 40000/57470 completed, running loss: 0.1411
Batch 50000/57470 completed, running loss: 0.1437
Starting evaluation with 7483 batches
Fold 8 - Epoch 16/50
Train Loss: 0.1465, Train Acc: 0.9782
Val Loss: 9.5278, Val Acc: 0.5520, ROC-AUC: 0.5796
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1266
Batch 20000/57470 completed, running loss: 0.1323
Batch 30000/57470 completed, running loss: 0.1354
Batch 40000/57470 completed, running loss: 0.1367
Batch 50000/57470 completed, running loss: 0.1422
Starting evaluation with 7483 batches
Fold 8 - Epoch 17/50
Train Loss: 0.1443, Train Acc: 0.9786
Val Loss: 8.5653, Val Acc: 0.5695, ROC-AUC: 0.5999
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.1004
Batch 20000/57470 completed, running loss: 0.1004
Batch 30000/57470 completed, running loss: 0.1003
Batch 40000/57470 completed, running loss: 0.0984
Batch 50000/57470 completed, running loss: 0.0982
Starting evaluation with 7483 batches
Fold 8 - Epoch 18/50
Train Loss: 0.0973, Train Acc: 0.9855
Val Loss: 9.6735, Val Acc: 0.5611, ROC-AUC: 0.5876
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0765
Batch 20000/57470 completed, running loss: 0.0789
Batch 30000/57470 completed, running loss: 0.0774
Batch 40000/57470 completed, running loss: 0.0804
Batch 50000/57470 completed, running loss: 0.0828
Starting evaluation with 7483 batches
Fold 8 - Epoch 19/50
Train Loss: 0.0820, Train Acc: 0.9880
Val Loss: 10.0421, Val Acc: 0.5698, ROC-AUC: 0.5944
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0711
Batch 20000/57470 completed, running loss: 0.0701
Batch 30000/57470 completed, running loss: 0.0729
Batch 40000/57470 completed, running loss: 0.0746
Batch 50000/57470 completed, running loss: 0.0752
Starting evaluation with 7483 batches
Fold 8 - Epoch 20/50
Train Loss: 0.0748, Train Acc: 0.9892
Val Loss: 10.5507, Val Acc: 0.5679, ROC-AUC: 0.5938
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0669
Batch 20000/57470 completed, running loss: 0.0681
Batch 30000/57470 completed, running loss: 0.0706
Batch 40000/57470 completed, running loss: 0.0719
Batch 50000/57470 completed, running loss: 0.0715
Starting evaluation with 7483 batches
Fold 8 - Epoch 21/50
Train Loss: 0.0723, Train Acc: 0.9901
Val Loss: 10.9775, Val Acc: 0.5698, ROC-AUC: 0.5939
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0619
Batch 20000/57470 completed, running loss: 0.0643
Batch 30000/57470 completed, running loss: 0.0656
Batch 40000/57470 completed, running loss: 0.0676
Batch 50000/57470 completed, running loss: 0.0685
Starting evaluation with 7483 batches
Fold 8 - Epoch 22/50
Train Loss: 0.0690, Train Acc: 0.9903
Val Loss: 11.0708, Val Acc: 0.5624, ROC-AUC: 0.5866
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0687
Batch 20000/57470 completed, running loss: 0.0667
Batch 30000/57470 completed, running loss: 0.0666
Batch 40000/57470 completed, running loss: 0.0673
Batch 50000/57470 completed, running loss: 0.0658
Starting evaluation with 7483 batches
Fold 8 - Epoch 23/50
Train Loss: 0.0667, Train Acc: 0.9905
Val Loss: 11.3438, Val Acc: 0.5698, ROC-AUC: 0.5917
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0645
Batch 20000/57470 completed, running loss: 0.0640
Batch 30000/57470 completed, running loss: 0.0682
Batch 40000/57470 completed, running loss: 0.0673
Batch 50000/57470 completed, running loss: 0.0670
Starting evaluation with 7483 batches
Fold 8 - Epoch 24/50
Train Loss: 0.0673, Train Acc: 0.9909
Val Loss: 11.6133, Val Acc: 0.5678, ROC-AUC: 0.5898
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0533
Batch 20000/57470 completed, running loss: 0.0580
Batch 30000/57470 completed, running loss: 0.0603
Batch 40000/57470 completed, running loss: 0.0600
Batch 50000/57470 completed, running loss: 0.0608
Starting evaluation with 7483 batches
Fold 8 - Epoch 25/50
Train Loss: 0.0626, Train Acc: 0.9913
Val Loss: 11.8490, Val Acc: 0.5666, ROC-AUC: 0.5924
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0529
Batch 20000/57470 completed, running loss: 0.0558
Batch 30000/57470 completed, running loss: 0.0579
Batch 40000/57470 completed, running loss: 0.0588
Batch 50000/57470 completed, running loss: 0.0602
Starting evaluation with 7483 batches
Fold 8 - Epoch 26/50
Train Loss: 0.0608, Train Acc: 0.9917
Val Loss: 11.8948, Val Acc: 0.5724, ROC-AUC: 0.5967
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0578
Batch 20000/57470 completed, running loss: 0.0583
Batch 30000/57470 completed, running loss: 0.0580
Batch 40000/57470 completed, running loss: 0.0592
Batch 50000/57470 completed, running loss: 0.0604
Starting evaluation with 7483 batches
Fold 8 - Epoch 27/50
Train Loss: 0.0611, Train Acc: 0.9918
Val Loss: 12.2345, Val Acc: 0.5632, ROC-AUC: 0.5867
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0619
Batch 20000/57470 completed, running loss: 0.0591
Batch 30000/57470 completed, running loss: 0.0621
Batch 40000/57470 completed, running loss: 0.0626
Batch 50000/57470 completed, running loss: 0.0647
Starting evaluation with 7483 batches
Fold 8 - Epoch 28/50
Train Loss: 0.0644, Train Acc: 0.9917
Val Loss: 11.9178, Val Acc: 0.5686, ROC-AUC: 0.5904
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0523
Batch 20000/57470 completed, running loss: 0.0569
Batch 30000/57470 completed, running loss: 0.0589
Batch 40000/57470 completed, running loss: 0.0640
Batch 50000/57470 completed, running loss: 0.0629
Starting evaluation with 7483 batches
Fold 8 - Epoch 29/50
Train Loss: 0.0631, Train Acc: 0.9922
Val Loss: 12.3540, Val Acc: 0.5681, ROC-AUC: 0.5948
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0520
Batch 20000/57470 completed, running loss: 0.0565
Batch 30000/57470 completed, running loss: 0.0572
Batch 40000/57470 completed, running loss: 0.0567
Batch 50000/57470 completed, running loss: 0.0570
Starting evaluation with 7483 batches
Fold 8 - Epoch 30/50
Train Loss: 0.0573, Train Acc: 0.9920
Val Loss: 12.8217, Val Acc: 0.5723, ROC-AUC: 0.5966
Starting training epoch with 57470 batches
Batch 10000/57470 completed, running loss: 0.0632
Batch 20000/57470 completed, running loss: 0.0641
Batch 30000/57470 completed, running loss: 0.0618
Batch 40000/57470 completed, running loss: 0.0638
Batch 50000/57470 completed, running loss: 0.0630
Starting evaluation with 7483 batches
Fold 8 - Epoch 31/50
Train Loss: 0.0627, Train Acc: 0.9923
Val Loss: 12.8325, Val Acc: 0.5636, ROC-AUC: 0.5889
Fold 8 - Early stopping triggered
Starting evaluation with 57470 batches
Evaluation batch 10000/57470 completed
Evaluation batch 20000/57470 completed
Evaluation batch 30000/57470 completed
Evaluation batch 40000/57470 completed
Evaluation batch 50000/57470 completed
Starting evaluation with 7483 batches
Starting evaluation with 7517 batches

Fold 8 - Train Metrics:
  Loss: 0.3836
  Accuracy: 0.8187
  Precision: 0.8135
  Recall: 0.7947
  Roc_auc: 0.9070
  Specificity: 0.8398
  F1: 0.8040

Fold 8 - Validation Metrics:
  Loss: 0.9023
  Accuracy: 0.5925
  Precision: 0.5511
  Recall: 0.5828
  Roc_auc: 0.6310
  Specificity: 0.6007
  F1: 0.5665

Fold 8 - Test Metrics:
  Loss: 0.8837
  Accuracy: 0.5794
  Precision: 0.6455
  Recall: 0.5185
  Roc_auc: 0.6429
  Specificity: 0.6535
  F1: 0.5751

=== Fold 9/10 ===
Fold 9 - Training label counts: {0: 120812, 1: 110265}
Fold 9 - Validation label counts: {0: 16109, 1: 14338}
Fold 9 - Test label counts: {0: 15230, 1: 13124}
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.6346
Batch 20000/57770 completed, running loss: 0.6170
Batch 30000/57770 completed, running loss: 0.6053
Batch 40000/57770 completed, running loss: 0.5979
Batch 50000/57770 completed, running loss: 0.5916
Starting evaluation with 7612 batches
Fold 9 - Epoch 1/50
Train Loss: 0.5873, Train Acc: 0.7027
Val Loss: 0.8847, Val Acc: 0.5877, ROC-AUC: 0.6308
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.4657
Batch 20000/57770 completed, running loss: 0.4737
Batch 30000/57770 completed, running loss: 0.4813
Batch 40000/57770 completed, running loss: 0.4847
Batch 50000/57770 completed, running loss: 0.4868
Starting evaluation with 7612 batches
Fold 9 - Epoch 2/50
Train Loss: 0.4889, Train Acc: 0.7996
Val Loss: 1.2427, Val Acc: 0.5905, ROC-AUC: 0.6298
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.3606
Batch 20000/57770 completed, running loss: 0.3799
Batch 30000/57770 completed, running loss: 0.3925
Batch 40000/57770 completed, running loss: 0.4025
Batch 50000/57770 completed, running loss: 0.4087
Starting evaluation with 7612 batches
Fold 9 - Epoch 3/50
Train Loss: 0.4132, Train Acc: 0.8609
Val Loss: 2.0003, Val Acc: 0.5780, ROC-AUC: 0.6187
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.2894
Batch 20000/57770 completed, running loss: 0.3114
Batch 30000/57770 completed, running loss: 0.3236
Batch 40000/57770 completed, running loss: 0.3328
Batch 50000/57770 completed, running loss: 0.3408
Starting evaluation with 7612 batches
Fold 9 - Epoch 4/50
Train Loss: 0.3461, Train Acc: 0.9025
Val Loss: 2.7927, Val Acc: 0.5765, ROC-AUC: 0.6134
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.2334
Batch 20000/57770 completed, running loss: 0.2554
Batch 30000/57770 completed, running loss: 0.2680
Batch 40000/57770 completed, running loss: 0.2779
Batch 50000/57770 completed, running loss: 0.2885
Starting evaluation with 7612 batches
Fold 9 - Epoch 5/50
Train Loss: 0.2940, Train Acc: 0.9270
Val Loss: 3.7159, Val Acc: 0.5769, ROC-AUC: 0.6083
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.2081
Batch 20000/57770 completed, running loss: 0.2221
Batch 30000/57770 completed, running loss: 0.2323
Batch 40000/57770 completed, running loss: 0.2462
Batch 50000/57770 completed, running loss: 0.2533
Starting evaluation with 7612 batches
Fold 9 - Epoch 6/50
Train Loss: 0.2584, Train Acc: 0.9433
Val Loss: 4.5471, Val Acc: 0.5715, ROC-AUC: 0.6044
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1938
Batch 20000/57770 completed, running loss: 0.1998
Batch 30000/57770 completed, running loss: 0.2102
Batch 40000/57770 completed, running loss: 0.2205
Batch 50000/57770 completed, running loss: 0.2269
Starting evaluation with 7612 batches
Fold 9 - Epoch 7/50
Train Loss: 0.2325, Train Acc: 0.9526
Val Loss: 5.2244, Val Acc: 0.5777, ROC-AUC: 0.6095
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1731
Batch 20000/57770 completed, running loss: 0.1887
Batch 30000/57770 completed, running loss: 0.1969
Batch 40000/57770 completed, running loss: 0.2044
Batch 50000/57770 completed, running loss: 0.2112
Starting evaluation with 7612 batches
Fold 9 - Epoch 8/50
Train Loss: 0.2174, Train Acc: 0.9588
Val Loss: 6.0064, Val Acc: 0.5738, ROC-AUC: 0.6035
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1741
Batch 20000/57770 completed, running loss: 0.1821
Batch 30000/57770 completed, running loss: 0.1870
Batch 40000/57770 completed, running loss: 0.1933
Batch 50000/57770 completed, running loss: 0.2003
Starting evaluation with 7612 batches
Fold 9 - Epoch 9/50
Train Loss: 0.2039, Train Acc: 0.9630
Val Loss: 6.3588, Val Acc: 0.5802, ROC-AUC: 0.6081
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1529
Batch 20000/57770 completed, running loss: 0.1650
Batch 30000/57770 completed, running loss: 0.1793
Batch 40000/57770 completed, running loss: 0.1840
Batch 50000/57770 completed, running loss: 0.1871
Starting evaluation with 7612 batches
Fold 9 - Epoch 10/50
Train Loss: 0.1898, Train Acc: 0.9670
Val Loss: 6.7075, Val Acc: 0.5803, ROC-AUC: 0.6106
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1562
Batch 20000/57770 completed, running loss: 0.1608
Batch 30000/57770 completed, running loss: 0.1653
Batch 40000/57770 completed, running loss: 0.1708
Batch 50000/57770 completed, running loss: 0.1762
Starting evaluation with 7612 batches
Fold 9 - Epoch 11/50
Train Loss: 0.1773, Train Acc: 0.9701
Val Loss: 7.0220, Val Acc: 0.5797, ROC-AUC: 0.6087
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1444
Batch 20000/57770 completed, running loss: 0.1568
Batch 30000/57770 completed, running loss: 0.1620
Batch 40000/57770 completed, running loss: 0.1654
Batch 50000/57770 completed, running loss: 0.1701
Starting evaluation with 7612 batches
Fold 9 - Epoch 12/50
Train Loss: 0.1711, Train Acc: 0.9716
Val Loss: 7.5264, Val Acc: 0.5622, ROC-AUC: 0.5873
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1391
Batch 20000/57770 completed, running loss: 0.1485
Batch 30000/57770 completed, running loss: 0.1491
Batch 40000/57770 completed, running loss: 0.1541
Batch 50000/57770 completed, running loss: 0.1604
Starting evaluation with 7612 batches
Fold 9 - Epoch 13/50
Train Loss: 0.1631, Train Acc: 0.9738
Val Loss: 7.2855, Val Acc: 0.5786, ROC-AUC: 0.6072
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1388
Batch 20000/57770 completed, running loss: 0.1379
Batch 30000/57770 completed, running loss: 0.1453
Batch 40000/57770 completed, running loss: 0.1515
Batch 50000/57770 completed, running loss: 0.1540
Starting evaluation with 7612 batches
Fold 9 - Epoch 14/50
Train Loss: 0.1565, Train Acc: 0.9754
Val Loss: 8.0729, Val Acc: 0.5693, ROC-AUC: 0.5962
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1316
Batch 20000/57770 completed, running loss: 0.1360
Batch 30000/57770 completed, running loss: 0.1427
Batch 40000/57770 completed, running loss: 0.1438
Batch 50000/57770 completed, running loss: 0.1466
Starting evaluation with 7612 batches
Fold 9 - Epoch 15/50
Train Loss: 0.1491, Train Acc: 0.9775
Val Loss: 8.3306, Val Acc: 0.5710, ROC-AUC: 0.5980
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1329
Batch 20000/57770 completed, running loss: 0.1363
Batch 30000/57770 completed, running loss: 0.1425
Batch 40000/57770 completed, running loss: 0.1473
Batch 50000/57770 completed, running loss: 0.1470
Starting evaluation with 7612 batches
Fold 9 - Epoch 16/50
Train Loss: 0.1494, Train Acc: 0.9774
Val Loss: 8.6708, Val Acc: 0.5762, ROC-AUC: 0.6005
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.1297
Batch 20000/57770 completed, running loss: 0.1307
Batch 30000/57770 completed, running loss: 0.1318
Batch 40000/57770 completed, running loss: 0.1325
Batch 50000/57770 completed, running loss: 0.1338
Starting evaluation with 7612 batches
Fold 9 - Epoch 17/50
Train Loss: 0.1347, Train Acc: 0.9792
Val Loss: 8.5992, Val Acc: 0.5785, ROC-AUC: 0.6060
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0990
Batch 20000/57770 completed, running loss: 0.0926
Batch 30000/57770 completed, running loss: 0.0936
Batch 40000/57770 completed, running loss: 0.0918
Batch 50000/57770 completed, running loss: 0.0925
Starting evaluation with 7612 batches
Fold 9 - Epoch 18/50
Train Loss: 0.0922, Train Acc: 0.9863
Val Loss: 9.3903, Val Acc: 0.5762, ROC-AUC: 0.6030
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0726
Batch 20000/57770 completed, running loss: 0.0744
Batch 30000/57770 completed, running loss: 0.0742
Batch 40000/57770 completed, running loss: 0.0788
Batch 50000/57770 completed, running loss: 0.0786
Starting evaluation with 7612 batches
Fold 9 - Epoch 19/50
Train Loss: 0.0801, Train Acc: 0.9882
Val Loss: 9.8034, Val Acc: 0.5782, ROC-AUC: 0.6031
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0708
Batch 20000/57770 completed, running loss: 0.0750
Batch 30000/57770 completed, running loss: 0.0757
Batch 40000/57770 completed, running loss: 0.0742
Batch 50000/57770 completed, running loss: 0.0746
Starting evaluation with 7612 batches
Fold 9 - Epoch 20/50
Train Loss: 0.0747, Train Acc: 0.9891
Val Loss: 9.9170, Val Acc: 0.5775, ROC-AUC: 0.6049
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0668
Batch 20000/57770 completed, running loss: 0.0666
Batch 30000/57770 completed, running loss: 0.0660
Batch 40000/57770 completed, running loss: 0.0701
Batch 50000/57770 completed, running loss: 0.0704
Starting evaluation with 7612 batches
Fold 9 - Epoch 21/50
Train Loss: 0.0717, Train Acc: 0.9899
Val Loss: 10.3270, Val Acc: 0.5758, ROC-AUC: 0.5964
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0578
Batch 20000/57770 completed, running loss: 0.0637
Batch 30000/57770 completed, running loss: 0.0666
Batch 40000/57770 completed, running loss: 0.0654
Batch 50000/57770 completed, running loss: 0.0661
Starting evaluation with 7612 batches
Fold 9 - Epoch 22/50
Train Loss: 0.0680, Train Acc: 0.9904
Val Loss: 10.7783, Val Acc: 0.5735, ROC-AUC: 0.5943
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0620
Batch 20000/57770 completed, running loss: 0.0614
Batch 30000/57770 completed, running loss: 0.0633
Batch 40000/57770 completed, running loss: 0.0608
Batch 50000/57770 completed, running loss: 0.0629
Starting evaluation with 7612 batches
Fold 9 - Epoch 23/50
Train Loss: 0.0640, Train Acc: 0.9907
Val Loss: 10.5599, Val Acc: 0.5794, ROC-AUC: 0.6060
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0604
Batch 20000/57770 completed, running loss: 0.0641
Batch 30000/57770 completed, running loss: 0.0651
Batch 40000/57770 completed, running loss: 0.0638
Batch 50000/57770 completed, running loss: 0.0643
Starting evaluation with 7612 batches
Fold 9 - Epoch 24/50
Train Loss: 0.0639, Train Acc: 0.9911
Val Loss: 11.1301, Val Acc: 0.5769, ROC-AUC: 0.6043
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0631
Batch 20000/57770 completed, running loss: 0.0600
Batch 30000/57770 completed, running loss: 0.0621
Batch 40000/57770 completed, running loss: 0.0631
Batch 50000/57770 completed, running loss: 0.0626
Starting evaluation with 7612 batches
Fold 9 - Epoch 25/50
Train Loss: 0.0624, Train Acc: 0.9913
Val Loss: 11.2820, Val Acc: 0.5771, ROC-AUC: 0.5994
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0600
Batch 20000/57770 completed, running loss: 0.0585
Batch 30000/57770 completed, running loss: 0.0587
Batch 40000/57770 completed, running loss: 0.0620
Batch 50000/57770 completed, running loss: 0.0612
Starting evaluation with 7612 batches
Fold 9 - Epoch 26/50
Train Loss: 0.0623, Train Acc: 0.9917
Val Loss: 11.5418, Val Acc: 0.5792, ROC-AUC: 0.6022
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0601
Batch 20000/57770 completed, running loss: 0.0607
Batch 30000/57770 completed, running loss: 0.0572
Batch 40000/57770 completed, running loss: 0.0575
Batch 50000/57770 completed, running loss: 0.0605
Starting evaluation with 7612 batches
Fold 9 - Epoch 27/50
Train Loss: 0.0597, Train Acc: 0.9919
Val Loss: 11.6274, Val Acc: 0.5779, ROC-AUC: 0.6016
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0592
Batch 20000/57770 completed, running loss: 0.0587
Batch 30000/57770 completed, running loss: 0.0568
Batch 40000/57770 completed, running loss: 0.0569
Batch 50000/57770 completed, running loss: 0.0596
Starting evaluation with 7612 batches
Fold 9 - Epoch 28/50
Train Loss: 0.0626, Train Acc: 0.9919
Val Loss: 11.4158, Val Acc: 0.5795, ROC-AUC: 0.6028
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0540
Batch 20000/57770 completed, running loss: 0.0552
Batch 30000/57770 completed, running loss: 0.0582
Batch 40000/57770 completed, running loss: 0.0574
Batch 50000/57770 completed, running loss: 0.0609
Starting evaluation with 7612 batches
Fold 9 - Epoch 29/50
Train Loss: 0.0615, Train Acc: 0.9918
Val Loss: 11.7067, Val Acc: 0.5786, ROC-AUC: 0.6022
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0561
Batch 20000/57770 completed, running loss: 0.0586
Batch 30000/57770 completed, running loss: 0.0580
Batch 40000/57770 completed, running loss: 0.0600
Batch 50000/57770 completed, running loss: 0.0592
Starting evaluation with 7612 batches
Fold 9 - Epoch 30/50
Train Loss: 0.0589, Train Acc: 0.9924
Val Loss: 11.9350, Val Acc: 0.5759, ROC-AUC: 0.6004
Starting training epoch with 57770 batches
Batch 10000/57770 completed, running loss: 0.0652
Batch 20000/57770 completed, running loss: 0.0625
Batch 30000/57770 completed, running loss: 0.0578
Batch 40000/57770 completed, running loss: 0.0566
Batch 50000/57770 completed, running loss: 0.0572
Starting evaluation with 7612 batches
Fold 9 - Epoch 31/50
Train Loss: 0.0582, Train Acc: 0.9922
Val Loss: 11.9705, Val Acc: 0.5800, ROC-AUC: 0.6037
Fold 9 - Early stopping triggered
Starting evaluation with 57770 batches
Evaluation batch 10000/57770 completed
Evaluation batch 20000/57770 completed
Evaluation batch 30000/57770 completed
Evaluation batch 40000/57770 completed
Evaluation batch 50000/57770 completed
Starting evaluation with 7612 batches
Starting evaluation with 7089 batches

Fold 9 - Train Metrics:
  Loss: 0.3776
  Accuracy: 0.8215
  Precision: 0.8377
  Recall: 0.7765
  Roc_auc: 0.9105
  Specificity: 0.8627
  F1: 0.8059

Fold 9 - Validation Metrics:
  Loss: 0.8847
  Accuracy: 0.5877
  Precision: 0.5669
  Recall: 0.5280
  Roc_auc: 0.6308
  Specificity: 0.6409
  F1: 0.5468

Fold 9 - Test Metrics:
  Loss: 0.9453
  Accuracy: 0.5534
  Precision: 0.5198
  Recall: 0.4610
  Roc_auc: 0.5899
  Specificity: 0.6330
  F1: 0.4886

=== Fold 10/10 ===
Fold 10 - Training label counts: {0: 123520, 1: 110759}
Fold 10 - Validation label counts: {0: 17166, 1: 13121}
Fold 10 - Test label counts: {1: 13847, 0: 11465}
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.6463
Batch 20000/58570 completed, running loss: 0.6273
Batch 30000/58570 completed, running loss: 0.6164
Batch 40000/58570 completed, running loss: 0.6073
Batch 50000/58570 completed, running loss: 0.5991
Starting evaluation with 7572 batches
Fold 10 - Epoch 1/50
Train Loss: 0.5938, Train Acc: 0.6915
Val Loss: 0.8623, Val Acc: 0.5896, ROC-AUC: 0.6313
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.4801
Batch 20000/58570 completed, running loss: 0.4828
Batch 30000/58570 completed, running loss: 0.4881
Batch 40000/58570 completed, running loss: 0.4905
Batch 50000/58570 completed, running loss: 0.4935
Starting evaluation with 7572 batches
Fold 10 - Epoch 2/50
Train Loss: 0.4957, Train Acc: 0.7938
Val Loss: 1.2819, Val Acc: 0.5706, ROC-AUC: 0.6096
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.3742
Batch 20000/58570 completed, running loss: 0.3895
Batch 30000/58570 completed, running loss: 0.4022
Batch 40000/58570 completed, running loss: 0.4117
Batch 50000/58570 completed, running loss: 0.4175
Starting evaluation with 7572 batches
Fold 10 - Epoch 3/50
Train Loss: 0.4226, Train Acc: 0.8552
Val Loss: 1.9171, Val Acc: 0.5611, ROC-AUC: 0.5976
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.2999
Batch 20000/58570 completed, running loss: 0.3175
Batch 30000/58570 completed, running loss: 0.3296
Batch 40000/58570 completed, running loss: 0.3414
Batch 50000/58570 completed, running loss: 0.3487
Starting evaluation with 7572 batches
Fold 10 - Epoch 4/50
Train Loss: 0.3564, Train Acc: 0.8976
Val Loss: 2.7074, Val Acc: 0.5524, ROC-AUC: 0.5822
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.2457
Batch 20000/58570 completed, running loss: 0.2597
Batch 30000/58570 completed, running loss: 0.2787
Batch 40000/58570 completed, running loss: 0.2881
Batch 50000/58570 completed, running loss: 0.2985
Starting evaluation with 7572 batches
Fold 10 - Epoch 5/50
Train Loss: 0.3062, Train Acc: 0.9229
Val Loss: 3.4110, Val Acc: 0.5747, ROC-AUC: 0.6034
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.2054
Batch 20000/58570 completed, running loss: 0.2277
Batch 30000/58570 completed, running loss: 0.2392
Batch 40000/58570 completed, running loss: 0.2518
Batch 50000/58570 completed, running loss: 0.2601
Starting evaluation with 7572 batches
Fold 10 - Epoch 6/50
Train Loss: 0.2679, Train Acc: 0.9393
Val Loss: 4.0702, Val Acc: 0.5690, ROC-AUC: 0.6019
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.2035
Batch 20000/58570 completed, running loss: 0.2148
Batch 30000/58570 completed, running loss: 0.2273
Batch 40000/58570 completed, running loss: 0.2354
Batch 50000/58570 completed, running loss: 0.2450
Starting evaluation with 7572 batches
Fold 10 - Epoch 7/50
Train Loss: 0.2500, Train Acc: 0.9484
Val Loss: 4.8509, Val Acc: 0.5623, ROC-AUC: 0.5937
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1783
Batch 20000/58570 completed, running loss: 0.1940
Batch 30000/58570 completed, running loss: 0.2038
Batch 40000/58570 completed, running loss: 0.2125
Batch 50000/58570 completed, running loss: 0.2184
Starting evaluation with 7572 batches
Fold 10 - Epoch 8/50
Train Loss: 0.2237, Train Acc: 0.9565
Val Loss: 5.3535, Val Acc: 0.5703, ROC-AUC: 0.5995
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1672
Batch 20000/58570 completed, running loss: 0.1815
Batch 30000/58570 completed, running loss: 0.1954
Batch 40000/58570 completed, running loss: 0.2019
Batch 50000/58570 completed, running loss: 0.2069
Starting evaluation with 7572 batches
Fold 10 - Epoch 9/50
Train Loss: 0.2116, Train Acc: 0.9605
Val Loss: 5.7507, Val Acc: 0.5789, ROC-AUC: 0.6099
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1606
Batch 20000/58570 completed, running loss: 0.1698
Batch 30000/58570 completed, running loss: 0.1763
Batch 40000/58570 completed, running loss: 0.1836
Batch 50000/58570 completed, running loss: 0.1901
Starting evaluation with 7572 batches
Fold 10 - Epoch 10/50
Train Loss: 0.1961, Train Acc: 0.9651
Val Loss: 6.4576, Val Acc: 0.5673, ROC-AUC: 0.5958
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1646
Batch 20000/58570 completed, running loss: 0.1713
Batch 30000/58570 completed, running loss: 0.1720
Batch 40000/58570 completed, running loss: 0.1765
Batch 50000/58570 completed, running loss: 0.1834
Starting evaluation with 7572 batches
Fold 10 - Epoch 11/50
Train Loss: 0.1868, Train Acc: 0.9679
Val Loss: 7.0822, Val Acc: 0.5669, ROC-AUC: 0.5941
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1618
Batch 20000/58570 completed, running loss: 0.1678
Batch 30000/58570 completed, running loss: 0.1758
Batch 40000/58570 completed, running loss: 0.1793
Batch 50000/58570 completed, running loss: 0.1824
Starting evaluation with 7572 batches
Fold 10 - Epoch 12/50
Train Loss: 0.1857, Train Acc: 0.9698
Val Loss: 7.3219, Val Acc: 0.5637, ROC-AUC: 0.5911
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1487
Batch 20000/58570 completed, running loss: 0.1547
Batch 30000/58570 completed, running loss: 0.1575
Batch 40000/58570 completed, running loss: 0.1628
Batch 50000/58570 completed, running loss: 0.1706
Starting evaluation with 7572 batches
Fold 10 - Epoch 13/50
Train Loss: 0.1744, Train Acc: 0.9716
Val Loss: 7.2314, Val Acc: 0.5637, ROC-AUC: 0.5918
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1506
Batch 20000/58570 completed, running loss: 0.1534
Batch 30000/58570 completed, running loss: 0.1523
Batch 40000/58570 completed, running loss: 0.1572
Batch 50000/58570 completed, running loss: 0.1640
Starting evaluation with 7572 batches
Fold 10 - Epoch 14/50
Train Loss: 0.1662, Train Acc: 0.9733
Val Loss: 7.5660, Val Acc: 0.5743, ROC-AUC: 0.6058
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1434
Batch 20000/58570 completed, running loss: 0.1485
Batch 30000/58570 completed, running loss: 0.1494
Batch 40000/58570 completed, running loss: 0.1491
Batch 50000/58570 completed, running loss: 0.1526
Starting evaluation with 7572 batches
Fold 10 - Epoch 15/50
Train Loss: 0.1581, Train Acc: 0.9751
Val Loss: 7.6400, Val Acc: 0.5739, ROC-AUC: 0.6034
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1418
Batch 20000/58570 completed, running loss: 0.1436
Batch 30000/58570 completed, running loss: 0.1473
Batch 40000/58570 completed, running loss: 0.1508
Batch 50000/58570 completed, running loss: 0.1512
Starting evaluation with 7572 batches
Fold 10 - Epoch 16/50
Train Loss: 0.1534, Train Acc: 0.9757
Val Loss: 8.4312, Val Acc: 0.5618, ROC-AUC: 0.5895
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1251
Batch 20000/58570 completed, running loss: 0.1321
Batch 30000/58570 completed, running loss: 0.1388
Batch 40000/58570 completed, running loss: 0.1419
Batch 50000/58570 completed, running loss: 0.1437
Starting evaluation with 7572 batches
Fold 10 - Epoch 17/50
Train Loss: 0.1455, Train Acc: 0.9778
Val Loss: 8.0468, Val Acc: 0.5742, ROC-AUC: 0.5964
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.1081
Batch 20000/58570 completed, running loss: 0.1090
Batch 30000/58570 completed, running loss: 0.1052
Batch 40000/58570 completed, running loss: 0.1050
Batch 50000/58570 completed, running loss: 0.1064
Starting evaluation with 7572 batches
Fold 10 - Epoch 18/50
Train Loss: 0.1058, Train Acc: 0.9844
Val Loss: 9.1147, Val Acc: 0.5635, ROC-AUC: 0.5849
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0755
Batch 20000/58570 completed, running loss: 0.0774
Batch 30000/58570 completed, running loss: 0.0790
Batch 40000/58570 completed, running loss: 0.0807
Batch 50000/58570 completed, running loss: 0.0812
Starting evaluation with 7572 batches
Fold 10 - Epoch 19/50
Train Loss: 0.0812, Train Acc: 0.9874
Val Loss: 9.2759, Val Acc: 0.5713, ROC-AUC: 0.5974
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0727
Batch 20000/58570 completed, running loss: 0.0774
Batch 30000/58570 completed, running loss: 0.0775
Batch 40000/58570 completed, running loss: 0.0795
Batch 50000/58570 completed, running loss: 0.0792
Starting evaluation with 7572 batches
Fold 10 - Epoch 20/50
Train Loss: 0.0789, Train Acc: 0.9883
Val Loss: 9.6266, Val Acc: 0.5730, ROC-AUC: 0.6011
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0739
Batch 20000/58570 completed, running loss: 0.0763
Batch 30000/58570 completed, running loss: 0.0757
Batch 40000/58570 completed, running loss: 0.0753
Batch 50000/58570 completed, running loss: 0.0760
Starting evaluation with 7572 batches
Fold 10 - Epoch 21/50
Train Loss: 0.0759, Train Acc: 0.9887
Val Loss: 9.9961, Val Acc: 0.5690, ROC-AUC: 0.5953
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0719
Batch 20000/58570 completed, running loss: 0.0706
Batch 30000/58570 completed, running loss: 0.0689
Batch 40000/58570 completed, running loss: 0.0712
Batch 50000/58570 completed, running loss: 0.0721
Starting evaluation with 7572 batches
Fold 10 - Epoch 22/50
Train Loss: 0.0738, Train Acc: 0.9896
Val Loss: 10.3854, Val Acc: 0.5701, ROC-AUC: 0.5949
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0627
Batch 20000/58570 completed, running loss: 0.0678
Batch 30000/58570 completed, running loss: 0.0699
Batch 40000/58570 completed, running loss: 0.0691
Batch 50000/58570 completed, running loss: 0.0690
Starting evaluation with 7572 batches
Fold 10 - Epoch 23/50
Train Loss: 0.0688, Train Acc: 0.9901
Val Loss: 10.2496, Val Acc: 0.5711, ROC-AUC: 0.5938
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0643
Batch 20000/58570 completed, running loss: 0.0647
Batch 30000/58570 completed, running loss: 0.0672
Batch 40000/58570 completed, running loss: 0.0676
Batch 50000/58570 completed, running loss: 0.0687
Starting evaluation with 7572 batches
Fold 10 - Epoch 24/50
Train Loss: 0.0681, Train Acc: 0.9905
Val Loss: 10.7033, Val Acc: 0.5644, ROC-AUC: 0.5903
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0661
Batch 20000/58570 completed, running loss: 0.0617
Batch 30000/58570 completed, running loss: 0.0633
Batch 40000/58570 completed, running loss: 0.0666
Batch 50000/58570 completed, running loss: 0.0699
Starting evaluation with 7572 batches
Fold 10 - Epoch 25/50
Train Loss: 0.0696, Train Acc: 0.9904
Val Loss: 10.6159, Val Acc: 0.5737, ROC-AUC: 0.5975
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0707
Batch 20000/58570 completed, running loss: 0.0697
Batch 30000/58570 completed, running loss: 0.0696
Batch 40000/58570 completed, running loss: 0.0681
Batch 50000/58570 completed, running loss: 0.0674
Starting evaluation with 7572 batches
Fold 10 - Epoch 26/50
Train Loss: 0.0675, Train Acc: 0.9908
Val Loss: 11.1430, Val Acc: 0.5697, ROC-AUC: 0.5960
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0615
Batch 20000/58570 completed, running loss: 0.0628
Batch 30000/58570 completed, running loss: 0.0648
Batch 40000/58570 completed, running loss: 0.0665
Batch 50000/58570 completed, running loss: 0.0659
Starting evaluation with 7572 batches
Fold 10 - Epoch 27/50
Train Loss: 0.0677, Train Acc: 0.9907
Val Loss: 10.8893, Val Acc: 0.5655, ROC-AUC: 0.5920
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0619
Batch 20000/58570 completed, running loss: 0.0608
Batch 30000/58570 completed, running loss: 0.0604
Batch 40000/58570 completed, running loss: 0.0626
Batch 50000/58570 completed, running loss: 0.0631
Starting evaluation with 7572 batches
Fold 10 - Epoch 28/50
Train Loss: 0.0647, Train Acc: 0.9912
Val Loss: 11.2025, Val Acc: 0.5753, ROC-AUC: 0.6016
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0620
Batch 20000/58570 completed, running loss: 0.0588
Batch 30000/58570 completed, running loss: 0.0613
Batch 40000/58570 completed, running loss: 0.0620
Batch 50000/58570 completed, running loss: 0.0626
Starting evaluation with 7572 batches
Fold 10 - Epoch 29/50
Train Loss: 0.0629, Train Acc: 0.9915
Val Loss: 11.6738, Val Acc: 0.5676, ROC-AUC: 0.5924
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0636
Batch 20000/58570 completed, running loss: 0.0616
Batch 30000/58570 completed, running loss: 0.0623
Batch 40000/58570 completed, running loss: 0.0610
Batch 50000/58570 completed, running loss: 0.0621
Starting evaluation with 7572 batches
Fold 10 - Epoch 30/50
Train Loss: 0.0631, Train Acc: 0.9914
Val Loss: 11.1989, Val Acc: 0.5774, ROC-AUC: 0.5974
Starting training epoch with 58570 batches
Batch 10000/58570 completed, running loss: 0.0670
Batch 20000/58570 completed, running loss: 0.0664
Batch 30000/58570 completed, running loss: 0.0641
Batch 40000/58570 completed, running loss: 0.0645
Batch 50000/58570 completed, running loss: 0.0648
Starting evaluation with 7572 batches
Fold 10 - Epoch 31/50
Train Loss: 0.0665, Train Acc: 0.9914
Val Loss: 11.5932, Val Acc: 0.5716, ROC-AUC: 0.5946
Fold 10 - Early stopping triggered
Starting evaluation with 58570 batches
Evaluation batch 10000/58570 completed
Evaluation batch 20000/58570 completed
Evaluation batch 30000/58570 completed
Evaluation batch 40000/58570 completed
Evaluation batch 50000/58570 completed
Starting evaluation with 7572 batches
Starting evaluation with 6328 batches

Fold 10 - Train Metrics:
  Loss: 0.3859
  Accuracy: 0.8165
  Precision: 0.8345
  Recall: 0.7633
  Roc_auc: 0.9062
  Specificity: 0.8642
  F1: 0.7973

Fold 10 - Validation Metrics:
  Loss: 0.8623
  Accuracy: 0.5896
  Precision: 0.5275
  Recall: 0.5042
  Roc_auc: 0.6313
  Specificity: 0.6548
  F1: 0.5156

Fold 10 - Test Metrics:
  Loss: 0.7072
  Accuracy: 0.6379
  Precision: 0.7080
  Recall: 0.5754
  Roc_auc: 0.7286
  Specificity: 0.7133
  F1: 0.6348

=== Final 10-Fold Cross-Validation Summary ===

10-Fold Cross-Validation Results (Train Set):
Average Metrics:
  Loss: 0.3016 (95% CI: 0.0969 - 0.4049)
  Accuracy: 0.8612 (95% CI: 0.8052 - 0.9649)
  Precision: 0.8613 (95% CI: 0.7941 - 0.9640)
  Recall: 0.8447 (95% CI: 0.7662 - 0.9713)
  Roc_auc: 0.9346 (95% CI: 0.8965 - 0.9944)
  Specificity: 0.8760 (95% CI: 0.8064 - 0.9677)
  F1: 0.8526 (95% CI: 0.7972 - 0.9632)
