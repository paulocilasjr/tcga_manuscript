Setting random seed to 42
Using device: cuda
Loading CSV from ./../../../tiles_embedding_extraction/resnet/vector_column_format/tcga_embeddings_resnet50_label.csv
Preparing 10-fold cross-validation

Starting Fold 1/10
Fold 1 - Training label counts: {0: 118188, 1: 111001}
Fold 1 - Validation label counts: {0: 18130, 1: 12124}
Fold 1 - Test label counts: {0: 15833, 1: 14602}
Preparing dataset with 229189 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 229189 samples
Preparing dataset with 30254 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 30254 samples
Preparing dataset with 30435 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 30435 samples
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.6131
Batch 20000/57298 completed, running loss: 0.5943
Batch 30000/57298 completed, running loss: 0.5794
Batch 40000/57298 completed, running loss: 0.5697
Batch 50000/57298 completed, running loss: 0.5612
Starting evaluation with 7564 batches
Fold 1 - Epoch 1/50
Train Loss: 0.5561, Train Acc: 0.7022
Val Loss: 0.6818, Val Acc: 0.6090, ROC-AUC: 0.6534
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.4832
Batch 20000/57298 completed, running loss: 0.4817
Batch 30000/57298 completed, running loss: 0.4793
Batch 40000/57298 completed, running loss: 0.4779
Batch 50000/57298 completed, running loss: 0.4766
Starting evaluation with 7564 batches
Fold 1 - Epoch 2/50
Train Loss: 0.4750, Train Acc: 0.7617
Val Loss: 0.7398, Val Acc: 0.6043, ROC-AUC: 0.6568
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.4251
Batch 20000/57298 completed, running loss: 0.4233
Batch 30000/57298 completed, running loss: 0.4247
Batch 40000/57298 completed, running loss: 0.4260
Batch 50000/57298 completed, running loss: 0.4259
Starting evaluation with 7564 batches
Fold 1 - Epoch 3/50
Train Loss: 0.4265, Train Acc: 0.7921
Val Loss: 0.7613, Val Acc: 0.5977, ROC-AUC: 0.6504
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.3813
Batch 20000/57298 completed, running loss: 0.3863
Batch 30000/57298 completed, running loss: 0.3874
Batch 40000/57298 completed, running loss: 0.3892
Batch 50000/57298 completed, running loss: 0.3895
Starting evaluation with 7564 batches
Fold 1 - Epoch 4/50
Train Loss: 0.3899, Train Acc: 0.8144
Val Loss: 0.8252, Val Acc: 0.5861, ROC-AUC: 0.6410
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.3486
Batch 20000/57298 completed, running loss: 0.3538
Batch 30000/57298 completed, running loss: 0.3561
Batch 40000/57298 completed, running loss: 0.3568
Batch 50000/57298 completed, running loss: 0.3593
Starting evaluation with 7564 batches
Fold 1 - Epoch 5/50
Train Loss: 0.3603, Train Acc: 0.8315
Val Loss: 0.8456, Val Acc: 0.6033, ROC-AUC: 0.6457
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.3250
Batch 20000/57298 completed, running loss: 0.3257
Batch 30000/57298 completed, running loss: 0.3295
Batch 40000/57298 completed, running loss: 0.3323
Batch 50000/57298 completed, running loss: 0.3350
Starting evaluation with 7564 batches
Fold 1 - Epoch 6/50
Train Loss: 0.3355, Train Acc: 0.8450
Val Loss: 0.9286, Val Acc: 0.5912, ROC-AUC: 0.6437
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.3034
Batch 20000/57298 completed, running loss: 0.3043
Batch 30000/57298 completed, running loss: 0.3077
Batch 40000/57298 completed, running loss: 0.3103
Batch 50000/57298 completed, running loss: 0.3116
Starting evaluation with 7564 batches
Fold 1 - Epoch 7/50
Train Loss: 0.3130, Train Acc: 0.8570
Val Loss: 0.8986, Val Acc: 0.6097, ROC-AUC: 0.6601
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2857
Batch 20000/57298 completed, running loss: 0.2879
Batch 30000/57298 completed, running loss: 0.2893
Batch 40000/57298 completed, running loss: 0.2923
Batch 50000/57298 completed, running loss: 0.2938
Starting evaluation with 7564 batches
Fold 1 - Epoch 8/50
Train Loss: 0.2955, Train Acc: 0.8671
Val Loss: 1.0193, Val Acc: 0.5777, ROC-AUC: 0.6273
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2661
Batch 20000/57298 completed, running loss: 0.2703
Batch 30000/57298 completed, running loss: 0.2740
Batch 40000/57298 completed, running loss: 0.2768
Batch 50000/57298 completed, running loss: 0.2799
Starting evaluation with 7564 batches
Fold 1 - Epoch 9/50
Train Loss: 0.2814, Train Acc: 0.8749
Val Loss: 0.9902, Val Acc: 0.5944, ROC-AUC: 0.6384
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2512
Batch 20000/57298 completed, running loss: 0.2578
Batch 30000/57298 completed, running loss: 0.2606
Batch 40000/57298 completed, running loss: 0.2636
Batch 50000/57298 completed, running loss: 0.2655
Starting evaluation with 7564 batches
Fold 1 - Epoch 10/50
Train Loss: 0.2671, Train Acc: 0.8821
Val Loss: 1.0692, Val Acc: 0.5810, ROC-AUC: 0.6292
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2349
Batch 20000/57298 completed, running loss: 0.2425
Batch 30000/57298 completed, running loss: 0.2463
Batch 40000/57298 completed, running loss: 0.2496
Batch 50000/57298 completed, running loss: 0.2523
Starting evaluation with 7564 batches
Fold 1 - Epoch 11/50
Train Loss: 0.2537, Train Acc: 0.8888
Val Loss: 1.0914, Val Acc: 0.5804, ROC-AUC: 0.6323
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2263
Batch 20000/57298 completed, running loss: 0.2317
Batch 30000/57298 completed, running loss: 0.2352
Batch 40000/57298 completed, running loss: 0.2388
Batch 50000/57298 completed, running loss: 0.2414
Starting evaluation with 7564 batches
Fold 1 - Epoch 12/50
Train Loss: 0.2425, Train Acc: 0.8933
Val Loss: 1.1156, Val Acc: 0.5957, ROC-AUC: 0.6450
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2171
Batch 20000/57298 completed, running loss: 0.2240
Batch 30000/57298 completed, running loss: 0.2257
Batch 40000/57298 completed, running loss: 0.2277
Batch 50000/57298 completed, running loss: 0.2306
Starting evaluation with 7564 batches
Fold 1 - Epoch 13/50
Train Loss: 0.2321, Train Acc: 0.8994
Val Loss: 1.0967, Val Acc: 0.6004, ROC-AUC: 0.6494
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.2093
Batch 20000/57298 completed, running loss: 0.2124
Batch 30000/57298 completed, running loss: 0.2145
Batch 40000/57298 completed, running loss: 0.2173
Batch 50000/57298 completed, running loss: 0.2203
Starting evaluation with 7564 batches
Fold 1 - Epoch 14/50
Train Loss: 0.2217, Train Acc: 0.9052
Val Loss: 1.1521, Val Acc: 0.5915, ROC-AUC: 0.6372
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1988
Batch 20000/57298 completed, running loss: 0.2040
Batch 30000/57298 completed, running loss: 0.2072
Batch 40000/57298 completed, running loss: 0.2110
Batch 50000/57298 completed, running loss: 0.2127
Starting evaluation with 7564 batches
Fold 1 - Epoch 15/50
Train Loss: 0.2145, Train Acc: 0.9094
Val Loss: 1.2112, Val Acc: 0.5923, ROC-AUC: 0.6536
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1944
Batch 20000/57298 completed, running loss: 0.1982
Batch 30000/57298 completed, running loss: 0.2009
Batch 40000/57298 completed, running loss: 0.2043
Batch 50000/57298 completed, running loss: 0.2051
Starting evaluation with 7564 batches
Fold 1 - Epoch 16/50
Train Loss: 0.2075, Train Acc: 0.9127
Val Loss: 1.1759, Val Acc: 0.6007, ROC-AUC: 0.6494
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1844
Batch 20000/57298 completed, running loss: 0.1895
Batch 30000/57298 completed, running loss: 0.1949
Batch 40000/57298 completed, running loss: 0.1972
Batch 50000/57298 completed, running loss: 0.1989
Starting evaluation with 7564 batches
Fold 1 - Epoch 17/50
Train Loss: 0.1998, Train Acc: 0.9162
Val Loss: 1.2231, Val Acc: 0.5980, ROC-AUC: 0.6485
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1776
Batch 20000/57298 completed, running loss: 0.1817
Batch 30000/57298 completed, running loss: 0.1849
Batch 40000/57298 completed, running loss: 0.1872
Batch 50000/57298 completed, running loss: 0.1897
Starting evaluation with 7564 batches
Fold 1 - Epoch 18/50
Train Loss: 0.1917, Train Acc: 0.9204
Val Loss: 1.2600, Val Acc: 0.5901, ROC-AUC: 0.6403
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1752
Batch 20000/57298 completed, running loss: 0.1781
Batch 30000/57298 completed, running loss: 0.1798
Batch 40000/57298 completed, running loss: 0.1825
Batch 50000/57298 completed, running loss: 0.1842
Starting evaluation with 7564 batches
Fold 1 - Epoch 19/50
Train Loss: 0.1856, Train Acc: 0.9233
Val Loss: 1.3053, Val Acc: 0.5988, ROC-AUC: 0.6503
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1678
Batch 20000/57298 completed, running loss: 0.1718
Batch 30000/57298 completed, running loss: 0.1748
Batch 40000/57298 completed, running loss: 0.1772
Batch 50000/57298 completed, running loss: 0.1792
Starting evaluation with 7564 batches
Fold 1 - Epoch 20/50
Train Loss: 0.1808, Train Acc: 0.9254
Val Loss: 1.3802, Val Acc: 0.5870, ROC-AUC: 0.6362
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1619
Batch 20000/57298 completed, running loss: 0.1656
Batch 30000/57298 completed, running loss: 0.1681
Batch 40000/57298 completed, running loss: 0.1713
Batch 50000/57298 completed, running loss: 0.1733
Starting evaluation with 7564 batches
Fold 1 - Epoch 21/50
Train Loss: 0.1747, Train Acc: 0.9282
Val Loss: 1.2906, Val Acc: 0.6034, ROC-AUC: 0.6497
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1564
Batch 20000/57298 completed, running loss: 0.1634
Batch 30000/57298 completed, running loss: 0.1651
Batch 40000/57298 completed, running loss: 0.1666
Batch 50000/57298 completed, running loss: 0.1688
Starting evaluation with 7564 batches
Fold 1 - Epoch 22/50
Train Loss: 0.1708, Train Acc: 0.9304
Val Loss: 1.3549, Val Acc: 0.5873, ROC-AUC: 0.6447
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1536
Batch 20000/57298 completed, running loss: 0.1591
Batch 30000/57298 completed, running loss: 0.1615
Batch 40000/57298 completed, running loss: 0.1629
Batch 50000/57298 completed, running loss: 0.1655
Starting evaluation with 7564 batches
Fold 1 - Epoch 23/50
Train Loss: 0.1671, Train Acc: 0.9324
Val Loss: 1.3736, Val Acc: 0.5902, ROC-AUC: 0.6385
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1393
Batch 20000/57298 completed, running loss: 0.1374
Batch 30000/57298 completed, running loss: 0.1381
Batch 40000/57298 completed, running loss: 0.1369
Batch 50000/57298 completed, running loss: 0.1367
Starting evaluation with 7564 batches
Fold 1 - Epoch 24/50
Train Loss: 0.1367, Train Acc: 0.9453
Val Loss: 1.5430, Val Acc: 0.5872, ROC-AUC: 0.6400
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1217
Batch 20000/57298 completed, running loss: 0.1231
Batch 30000/57298 completed, running loss: 0.1243
Batch 40000/57298 completed, running loss: 0.1248
Batch 50000/57298 completed, running loss: 0.1255
Starting evaluation with 7564 batches
Fold 1 - Epoch 25/50
Train Loss: 0.1262, Train Acc: 0.9501
Val Loss: 1.6258, Val Acc: 0.5858, ROC-AUC: 0.6411
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1165
Batch 20000/57298 completed, running loss: 0.1160
Batch 30000/57298 completed, running loss: 0.1181
Batch 40000/57298 completed, running loss: 0.1195
Batch 50000/57298 completed, running loss: 0.1203
Starting evaluation with 7564 batches
Fold 1 - Epoch 26/50
Train Loss: 0.1209, Train Acc: 0.9521
Val Loss: 1.6732, Val Acc: 0.5909, ROC-AUC: 0.6412
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1138
Batch 20000/57298 completed, running loss: 0.1132
Batch 30000/57298 completed, running loss: 0.1149
Batch 40000/57298 completed, running loss: 0.1163
Batch 50000/57298 completed, running loss: 0.1167
Starting evaluation with 7564 batches
Fold 1 - Epoch 27/50
Train Loss: 0.1173, Train Acc: 0.9545
Val Loss: 1.6929, Val Acc: 0.5890, ROC-AUC: 0.6448
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1077
Batch 20000/57298 completed, running loss: 0.1126
Batch 30000/57298 completed, running loss: 0.1138
Batch 40000/57298 completed, running loss: 0.1145
Batch 50000/57298 completed, running loss: 0.1142
Starting evaluation with 7564 batches
Fold 1 - Epoch 28/50
Train Loss: 0.1147, Train Acc: 0.9559
Val Loss: 1.6933, Val Acc: 0.5948, ROC-AUC: 0.6474
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1078
Batch 20000/57298 completed, running loss: 0.1083
Batch 30000/57298 completed, running loss: 0.1097
Batch 40000/57298 completed, running loss: 0.1099
Batch 50000/57298 completed, running loss: 0.1118
Starting evaluation with 7564 batches
Fold 1 - Epoch 29/50
Train Loss: 0.1123, Train Acc: 0.9568
Val Loss: 1.7217, Val Acc: 0.5950, ROC-AUC: 0.6512
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1048
Batch 20000/57298 completed, running loss: 0.1073
Batch 30000/57298 completed, running loss: 0.1087
Batch 40000/57298 completed, running loss: 0.1091
Batch 50000/57298 completed, running loss: 0.1100
Starting evaluation with 7564 batches
Fold 1 - Epoch 30/50
Train Loss: 0.1105, Train Acc: 0.9579
Val Loss: 1.8699, Val Acc: 0.5796, ROC-AUC: 0.6352
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1012
Batch 20000/57298 completed, running loss: 0.1031
Batch 30000/57298 completed, running loss: 0.1035
Batch 40000/57298 completed, running loss: 0.1053
Batch 50000/57298 completed, running loss: 0.1058
Starting evaluation with 7564 batches
Fold 1 - Epoch 31/50
Train Loss: 0.1069, Train Acc: 0.9593
Val Loss: 1.7381, Val Acc: 0.5988, ROC-AUC: 0.6510
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1013
Batch 20000/57298 completed, running loss: 0.1020
Batch 30000/57298 completed, running loss: 0.1036
Batch 40000/57298 completed, running loss: 0.1054
Batch 50000/57298 completed, running loss: 0.1056
Starting evaluation with 7564 batches
Fold 1 - Epoch 32/50
Train Loss: 0.1064, Train Acc: 0.9598
Val Loss: 1.7592, Val Acc: 0.5932, ROC-AUC: 0.6440
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0970
Batch 20000/57298 completed, running loss: 0.0974
Batch 30000/57298 completed, running loss: 0.0988
Batch 40000/57298 completed, running loss: 0.0997
Batch 50000/57298 completed, running loss: 0.1008
Starting evaluation with 7564 batches
Fold 1 - Epoch 33/50
Train Loss: 0.1017, Train Acc: 0.9615
Val Loss: 1.8226, Val Acc: 0.5881, ROC-AUC: 0.6412
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.1009
Batch 20000/57298 completed, running loss: 0.0988
Batch 30000/57298 completed, running loss: 0.0999
Batch 40000/57298 completed, running loss: 0.1010
Batch 50000/57298 completed, running loss: 0.1010
Starting evaluation with 7564 batches
Fold 1 - Epoch 34/50
Train Loss: 0.1016, Train Acc: 0.9614
Val Loss: 1.8055, Val Acc: 0.5963, ROC-AUC: 0.6470
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0960
Batch 20000/57298 completed, running loss: 0.0956
Batch 30000/57298 completed, running loss: 0.0973
Batch 40000/57298 completed, running loss: 0.0979
Batch 50000/57298 completed, running loss: 0.0993
Starting evaluation with 7564 batches
Fold 1 - Epoch 35/50
Train Loss: 0.1001, Train Acc: 0.9626
Val Loss: 1.8485, Val Acc: 0.5871, ROC-AUC: 0.6414
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0932
Batch 20000/57298 completed, running loss: 0.0930
Batch 30000/57298 completed, running loss: 0.0944
Batch 40000/57298 completed, running loss: 0.0964
Batch 50000/57298 completed, running loss: 0.0971
Starting evaluation with 7564 batches
Fold 1 - Epoch 36/50
Train Loss: 0.0984, Train Acc: 0.9631
Val Loss: 1.7845, Val Acc: 0.5985, ROC-AUC: 0.6498
Starting training epoch with 57298 batches
Batch 10000/57298 completed, running loss: 0.0928
Batch 20000/57298 completed, running loss: 0.0919
Batch 30000/57298 completed, running loss: 0.0928
Batch 40000/57298 completed, running loss: 0.0940
Batch 50000/57298 completed, running loss: 0.0953
Starting evaluation with 7564 batches
Fold 1 - Epoch 37/50
Train Loss: 0.0958, Train Acc: 0.9642
Val Loss: 1.8702, Val Acc: 0.5893, ROC-AUC: 0.6423
Fold 1 - Early stopping triggered
Starting evaluation with 57298 batches
Evaluation batch 10000/57298 completed
Evaluation batch 20000/57298 completed
Evaluation batch 30000/57298 completed
Evaluation batch 40000/57298 completed
Evaluation batch 50000/57298 completed
Starting evaluation with 7564 batches
Starting evaluation with 7609 batches
Fold 1 - Test Metrics:
Loss: 0.9765
Accuracy: 0.6001
Precision: 0.5855
Recall: 0.5699
Roc_auc: 0.6474
Specificity: 0.6280
F1: 0.5776

Starting Fold 2/10
Fold 2 - Training label counts: {0: 121495, 1: 106525}
Fold 2 - Validation label counts: {1: 17498, 0: 15987}
Fold 2 - Test label counts: {0: 14669, 1: 13704}
Preparing dataset with 228020 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 228020 samples
Preparing dataset with 33485 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 33485 samples
Preparing dataset with 28373 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 28373 samples
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.6060
Batch 20000/57005 completed, running loss: 0.5882
Batch 30000/57005 completed, running loss: 0.5772
Batch 40000/57005 completed, running loss: 0.5675
Batch 50000/57005 completed, running loss: 0.5590
Starting evaluation with 8372 batches
Fold 2 - Epoch 1/50
Train Loss: 0.5531, Train Acc: 0.7040
Val Loss: 0.7819, Val Acc: 0.5723, ROC-AUC: 0.6098
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.4845
Batch 20000/57005 completed, running loss: 0.4830
Batch 30000/57005 completed, running loss: 0.4822
Batch 40000/57005 completed, running loss: 0.4802
Batch 50000/57005 completed, running loss: 0.4782
Starting evaluation with 8372 batches
Fold 2 - Epoch 2/50
Train Loss: 0.4764, Train Acc: 0.7601
Val Loss: 0.8034, Val Acc: 0.5794, ROC-AUC: 0.6241
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.4324
Batch 20000/57005 completed, running loss: 0.4319
Batch 30000/57005 completed, running loss: 0.4316
Batch 40000/57005 completed, running loss: 0.4319
Batch 50000/57005 completed, running loss: 0.4321
Starting evaluation with 8372 batches
Fold 2 - Epoch 3/50
Train Loss: 0.4317, Train Acc: 0.7885
Val Loss: 0.8382, Val Acc: 0.5754, ROC-AUC: 0.6161
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.3910
Batch 20000/57005 completed, running loss: 0.3929
Batch 30000/57005 completed, running loss: 0.3942
Batch 40000/57005 completed, running loss: 0.3944
Batch 50000/57005 completed, running loss: 0.3953
Starting evaluation with 8372 batches
Fold 2 - Epoch 4/50
Train Loss: 0.3957, Train Acc: 0.8099
Val Loss: 0.8836, Val Acc: 0.5898, ROC-AUC: 0.6301
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.3574
Batch 20000/57005 completed, running loss: 0.3602
Batch 30000/57005 completed, running loss: 0.3642
Batch 40000/57005 completed, running loss: 0.3654
Batch 50000/57005 completed, running loss: 0.3679
Starting evaluation with 8372 batches
Fold 2 - Epoch 5/50
Train Loss: 0.3685, Train Acc: 0.8261
Val Loss: 0.9030, Val Acc: 0.5813, ROC-AUC: 0.6276
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.3328
Batch 20000/57005 completed, running loss: 0.3362
Batch 30000/57005 completed, running loss: 0.3374
Batch 40000/57005 completed, running loss: 0.3401
Batch 50000/57005 completed, running loss: 0.3419
Starting evaluation with 8372 batches
Fold 2 - Epoch 6/50
Train Loss: 0.3424, Train Acc: 0.8416
Val Loss: 0.9711, Val Acc: 0.5704, ROC-AUC: 0.6125
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.3090
Batch 20000/57005 completed, running loss: 0.3129
Batch 30000/57005 completed, running loss: 0.3157
Batch 40000/57005 completed, running loss: 0.3186
Batch 50000/57005 completed, running loss: 0.3201
Starting evaluation with 8372 batches
Fold 2 - Epoch 7/50
Train Loss: 0.3214, Train Acc: 0.8536
Val Loss: 1.0300, Val Acc: 0.5651, ROC-AUC: 0.6043
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2865
Batch 20000/57005 completed, running loss: 0.2937
Batch 30000/57005 completed, running loss: 0.2954
Batch 40000/57005 completed, running loss: 0.2976
Batch 50000/57005 completed, running loss: 0.3010
Starting evaluation with 8372 batches
Fold 2 - Epoch 8/50
Train Loss: 0.3020, Train Acc: 0.8634
Val Loss: 1.0411, Val Acc: 0.5688, ROC-AUC: 0.6139
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2698
Batch 20000/57005 completed, running loss: 0.2742
Batch 30000/57005 completed, running loss: 0.2770
Batch 40000/57005 completed, running loss: 0.2797
Batch 50000/57005 completed, running loss: 0.2824
Starting evaluation with 8372 batches
Fold 2 - Epoch 9/50
Train Loss: 0.2835, Train Acc: 0.8736
Val Loss: 1.0272, Val Acc: 0.5778, ROC-AUC: 0.6252
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2552
Batch 20000/57005 completed, running loss: 0.2579
Batch 30000/57005 completed, running loss: 0.2619
Batch 40000/57005 completed, running loss: 0.2654
Batch 50000/57005 completed, running loss: 0.2684
Starting evaluation with 8372 batches
Fold 2 - Epoch 10/50
Train Loss: 0.2695, Train Acc: 0.8805
Val Loss: 1.1237, Val Acc: 0.5799, ROC-AUC: 0.6239
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2428
Batch 20000/57005 completed, running loss: 0.2459
Batch 30000/57005 completed, running loss: 0.2488
Batch 40000/57005 completed, running loss: 0.2521
Batch 50000/57005 completed, running loss: 0.2554
Starting evaluation with 8372 batches
Fold 2 - Epoch 11/50
Train Loss: 0.2573, Train Acc: 0.8874
Val Loss: 1.1283, Val Acc: 0.5734, ROC-AUC: 0.6174
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2333
Batch 20000/57005 completed, running loss: 0.2366
Batch 30000/57005 completed, running loss: 0.2393
Batch 40000/57005 completed, running loss: 0.2411
Batch 50000/57005 completed, running loss: 0.2441
Starting evaluation with 8372 batches
Fold 2 - Epoch 12/50
Train Loss: 0.2460, Train Acc: 0.8930
Val Loss: 1.1990, Val Acc: 0.5673, ROC-AUC: 0.6085
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2232
Batch 20000/57005 completed, running loss: 0.2246
Batch 30000/57005 completed, running loss: 0.2281
Batch 40000/57005 completed, running loss: 0.2307
Batch 50000/57005 completed, running loss: 0.2331
Starting evaluation with 8372 batches
Fold 2 - Epoch 13/50
Train Loss: 0.2346, Train Acc: 0.8988
Val Loss: 1.2221, Val Acc: 0.5771, ROC-AUC: 0.6206
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2105
Batch 20000/57005 completed, running loss: 0.2147
Batch 30000/57005 completed, running loss: 0.2186
Batch 40000/57005 completed, running loss: 0.2200
Batch 50000/57005 completed, running loss: 0.2222
Starting evaluation with 8372 batches
Fold 2 - Epoch 14/50
Train Loss: 0.2246, Train Acc: 0.9039
Val Loss: 1.2556, Val Acc: 0.5705, ROC-AUC: 0.6081
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.2015
Batch 20000/57005 completed, running loss: 0.2077
Batch 30000/57005 completed, running loss: 0.2099
Batch 40000/57005 completed, running loss: 0.2111
Batch 50000/57005 completed, running loss: 0.2135
Starting evaluation with 8372 batches
Fold 2 - Epoch 15/50
Train Loss: 0.2152, Train Acc: 0.9089
Val Loss: 1.2652, Val Acc: 0.5713, ROC-AUC: 0.6128
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1920
Batch 20000/57005 completed, running loss: 0.1972
Batch 30000/57005 completed, running loss: 0.2005
Batch 40000/57005 completed, running loss: 0.2025
Batch 50000/57005 completed, running loss: 0.2051
Starting evaluation with 8372 batches
Fold 2 - Epoch 16/50
Train Loss: 0.2070, Train Acc: 0.9131
Val Loss: 1.3145, Val Acc: 0.5630, ROC-AUC: 0.6051
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1877
Batch 20000/57005 completed, running loss: 0.1915
Batch 30000/57005 completed, running loss: 0.1936
Batch 40000/57005 completed, running loss: 0.1975
Batch 50000/57005 completed, running loss: 0.1998
Starting evaluation with 8372 batches
Fold 2 - Epoch 17/50
Train Loss: 0.2012, Train Acc: 0.9160
Val Loss: 1.3316, Val Acc: 0.5744, ROC-AUC: 0.6182
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1799
Batch 20000/57005 completed, running loss: 0.1847
Batch 30000/57005 completed, running loss: 0.1887
Batch 40000/57005 completed, running loss: 0.1911
Batch 50000/57005 completed, running loss: 0.1933
Starting evaluation with 8372 batches
Fold 2 - Epoch 18/50
Train Loss: 0.1942, Train Acc: 0.9192
Val Loss: 1.3763, Val Acc: 0.5689, ROC-AUC: 0.6127
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1760
Batch 20000/57005 completed, running loss: 0.1794
Batch 30000/57005 completed, running loss: 0.1828
Batch 40000/57005 completed, running loss: 0.1839
Batch 50000/57005 completed, running loss: 0.1860
Starting evaluation with 8372 batches
Fold 2 - Epoch 19/50
Train Loss: 0.1870, Train Acc: 0.9228
Val Loss: 1.3841, Val Acc: 0.5794, ROC-AUC: 0.6243
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1664
Batch 20000/57005 completed, running loss: 0.1706
Batch 30000/57005 completed, running loss: 0.1741
Batch 40000/57005 completed, running loss: 0.1762
Batch 50000/57005 completed, running loss: 0.1777
Starting evaluation with 8372 batches
Fold 2 - Epoch 20/50
Train Loss: 0.1794, Train Acc: 0.9267
Val Loss: 1.5096, Val Acc: 0.5580, ROC-AUC: 0.5997
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1493
Batch 20000/57005 completed, running loss: 0.1495
Batch 30000/57005 completed, running loss: 0.1495
Batch 40000/57005 completed, running loss: 0.1491
Batch 50000/57005 completed, running loss: 0.1500
Starting evaluation with 8372 batches
Fold 2 - Epoch 21/50
Train Loss: 0.1503, Train Acc: 0.9399
Val Loss: 1.5933, Val Acc: 0.5725, ROC-AUC: 0.6170
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1342
Batch 20000/57005 completed, running loss: 0.1348
Batch 30000/57005 completed, running loss: 0.1362
Batch 40000/57005 completed, running loss: 0.1372
Batch 50000/57005 completed, running loss: 0.1380
Starting evaluation with 8372 batches
Fold 2 - Epoch 22/50
Train Loss: 0.1385, Train Acc: 0.9451
Val Loss: 1.7243, Val Acc: 0.5683, ROC-AUC: 0.6099
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1282
Batch 20000/57005 completed, running loss: 0.1282
Batch 30000/57005 completed, running loss: 0.1289
Batch 40000/57005 completed, running loss: 0.1299
Batch 50000/57005 completed, running loss: 0.1304
Starting evaluation with 8372 batches
Fold 2 - Epoch 23/50
Train Loss: 0.1312, Train Acc: 0.9483
Val Loss: 1.7268, Val Acc: 0.5680, ROC-AUC: 0.6074
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1225
Batch 20000/57005 completed, running loss: 0.1217
Batch 30000/57005 completed, running loss: 0.1245
Batch 40000/57005 completed, running loss: 0.1253
Batch 50000/57005 completed, running loss: 0.1265
Starting evaluation with 8372 batches
Fold 2 - Epoch 24/50
Train Loss: 0.1277, Train Acc: 0.9502
Val Loss: 1.7265, Val Acc: 0.5670, ROC-AUC: 0.6115
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1183
Batch 20000/57005 completed, running loss: 0.1197
Batch 30000/57005 completed, running loss: 0.1203
Batch 40000/57005 completed, running loss: 0.1215
Batch 50000/57005 completed, running loss: 0.1221
Starting evaluation with 8372 batches
Fold 2 - Epoch 25/50
Train Loss: 0.1237, Train Acc: 0.9516
Val Loss: 1.7407, Val Acc: 0.5743, ROC-AUC: 0.6173
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1145
Batch 20000/57005 completed, running loss: 0.1160
Batch 30000/57005 completed, running loss: 0.1166
Batch 40000/57005 completed, running loss: 0.1183
Batch 50000/57005 completed, running loss: 0.1203
Starting evaluation with 8372 batches
Fold 2 - Epoch 26/50
Train Loss: 0.1206, Train Acc: 0.9529
Val Loss: 1.7756, Val Acc: 0.5688, ROC-AUC: 0.6111
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1054
Batch 20000/57005 completed, running loss: 0.1091
Batch 30000/57005 completed, running loss: 0.1114
Batch 40000/57005 completed, running loss: 0.1133
Batch 50000/57005 completed, running loss: 0.1145
Starting evaluation with 8372 batches
Fold 2 - Epoch 27/50
Train Loss: 0.1151, Train Acc: 0.9554
Val Loss: 1.8443, Val Acc: 0.5773, ROC-AUC: 0.6185
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1099
Batch 20000/57005 completed, running loss: 0.1106
Batch 30000/57005 completed, running loss: 0.1112
Batch 40000/57005 completed, running loss: 0.1121
Batch 50000/57005 completed, running loss: 0.1139
Starting evaluation with 8372 batches
Fold 2 - Epoch 28/50
Train Loss: 0.1148, Train Acc: 0.9557
Val Loss: 1.9235, Val Acc: 0.5694, ROC-AUC: 0.6137
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1063
Batch 20000/57005 completed, running loss: 0.1093
Batch 30000/57005 completed, running loss: 0.1111
Batch 40000/57005 completed, running loss: 0.1114
Batch 50000/57005 completed, running loss: 0.1120
Starting evaluation with 8372 batches
Fold 2 - Epoch 29/50
Train Loss: 0.1123, Train Acc: 0.9567
Val Loss: 1.8739, Val Acc: 0.5754, ROC-AUC: 0.6202
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1044
Batch 20000/57005 completed, running loss: 0.1060
Batch 30000/57005 completed, running loss: 0.1075
Batch 40000/57005 completed, running loss: 0.1078
Batch 50000/57005 completed, running loss: 0.1100
Starting evaluation with 8372 batches
Fold 2 - Epoch 30/50
Train Loss: 0.1108, Train Acc: 0.9577
Val Loss: 1.8622, Val Acc: 0.5737, ROC-AUC: 0.6180
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1013
Batch 20000/57005 completed, running loss: 0.1036
Batch 30000/57005 completed, running loss: 0.1057
Batch 40000/57005 completed, running loss: 0.1059
Batch 50000/57005 completed, running loss: 0.1070
Starting evaluation with 8372 batches
Fold 2 - Epoch 31/50
Train Loss: 0.1076, Train Acc: 0.9590
Val Loss: 1.8834, Val Acc: 0.5766, ROC-AUC: 0.6183
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1050
Batch 20000/57005 completed, running loss: 0.1069
Batch 30000/57005 completed, running loss: 0.1065
Batch 40000/57005 completed, running loss: 0.1077
Batch 50000/57005 completed, running loss: 0.1076
Starting evaluation with 8372 batches
Fold 2 - Epoch 32/50
Train Loss: 0.1087, Train Acc: 0.9589
Val Loss: 1.9391, Val Acc: 0.5687, ROC-AUC: 0.6103
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.1011
Batch 20000/57005 completed, running loss: 0.1015
Batch 30000/57005 completed, running loss: 0.1027
Batch 40000/57005 completed, running loss: 0.1032
Batch 50000/57005 completed, running loss: 0.1049
Starting evaluation with 8372 batches
Fold 2 - Epoch 33/50
Train Loss: 0.1057, Train Acc: 0.9600
Val Loss: 1.9008, Val Acc: 0.5762, ROC-AUC: 0.6189
Starting training epoch with 57005 batches
Batch 10000/57005 completed, running loss: 0.0987
Batch 20000/57005 completed, running loss: 0.0992
Batch 30000/57005 completed, running loss: 0.1003
Batch 40000/57005 completed, running loss: 0.1023
Batch 50000/57005 completed, running loss: 0.1029
Starting evaluation with 8372 batches
Fold 2 - Epoch 34/50
Train Loss: 0.1036, Train Acc: 0.9610
Val Loss: 1.9579, Val Acc: 0.5713, ROC-AUC: 0.6135
Fold 2 - Early stopping triggered
Starting evaluation with 57005 batches
Evaluation batch 10000/57005 completed
Evaluation batch 20000/57005 completed
Evaluation batch 30000/57005 completed
Evaluation batch 40000/57005 completed
Evaluation batch 50000/57005 completed
Starting evaluation with 8372 batches
Starting evaluation with 7094 batches
Fold 2 - Test Metrics:
Loss: 0.8466
Accuracy: 0.5826
Precision: 0.5683
Recall: 0.5644
Roc_auc: 0.6312
Specificity: 0.5995
F1: 0.5664

Starting Fold 3/10
Fold 3 - Training label counts: {0: 116663, 1: 108434}
Fold 3 - Validation label counts: {0: 15444, 1: 14378}
Fold 3 - Test label counts: {0: 20044, 1: 14915}
Preparing dataset with 225097 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 225097 samples
Preparing dataset with 29822 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 29822 samples
Preparing dataset with 34959 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 34959 samples
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.6170
Batch 20000/56275 completed, running loss: 0.5953
Batch 30000/56275 completed, running loss: 0.5825
Batch 40000/56275 completed, running loss: 0.5721
Batch 50000/56275 completed, running loss: 0.5641
Starting evaluation with 7456 batches
Fold 3 - Epoch 1/50
Train Loss: 0.5590, Train Acc: 0.6996
Val Loss: 0.6869, Val Acc: 0.6159, ROC-AUC: 0.6717
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.4858
Batch 20000/56275 completed, running loss: 0.4843
Batch 30000/56275 completed, running loss: 0.4835
Batch 40000/56275 completed, running loss: 0.4820
Batch 50000/56275 completed, running loss: 0.4810
Starting evaluation with 7456 batches
Fold 3 - Epoch 2/50
Train Loss: 0.4798, Train Acc: 0.7581
Val Loss: 0.7203, Val Acc: 0.6191, ROC-AUC: 0.6746
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.4292
Batch 20000/56275 completed, running loss: 0.4316
Batch 30000/56275 completed, running loss: 0.4317
Batch 40000/56275 completed, running loss: 0.4321
Batch 50000/56275 completed, running loss: 0.4315
Starting evaluation with 7456 batches
Fold 3 - Epoch 3/50
Train Loss: 0.4309, Train Acc: 0.7897
Val Loss: 0.7621, Val Acc: 0.6174, ROC-AUC: 0.6690
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.3841
Batch 20000/56275 completed, running loss: 0.3880
Batch 30000/56275 completed, running loss: 0.3910
Batch 40000/56275 completed, running loss: 0.3923
Batch 50000/56275 completed, running loss: 0.3938
Starting evaluation with 7456 batches
Fold 3 - Epoch 4/50
Train Loss: 0.3944, Train Acc: 0.8111
Val Loss: 0.7700, Val Acc: 0.6143, ROC-AUC: 0.6701
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.3531
Batch 20000/56275 completed, running loss: 0.3566
Batch 30000/56275 completed, running loss: 0.3597
Batch 40000/56275 completed, running loss: 0.3617
Batch 50000/56275 completed, running loss: 0.3637
Starting evaluation with 7456 batches
Fold 3 - Epoch 5/50
Train Loss: 0.3637, Train Acc: 0.8283
Val Loss: 0.8950, Val Acc: 0.5929, ROC-AUC: 0.6351
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.3303
Batch 20000/56275 completed, running loss: 0.3324
Batch 30000/56275 completed, running loss: 0.3352
Batch 40000/56275 completed, running loss: 0.3380
Batch 50000/56275 completed, running loss: 0.3401
Starting evaluation with 7456 batches
Fold 3 - Epoch 6/50
Train Loss: 0.3409, Train Acc: 0.8416
Val Loss: 0.8728, Val Acc: 0.6052, ROC-AUC: 0.6555
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.3049
Batch 20000/56275 completed, running loss: 0.3086
Batch 30000/56275 completed, running loss: 0.3106
Batch 40000/56275 completed, running loss: 0.3129
Batch 50000/56275 completed, running loss: 0.3165
Starting evaluation with 7456 batches
Fold 3 - Epoch 7/50
Train Loss: 0.3178, Train Acc: 0.8544
Val Loss: 0.9106, Val Acc: 0.6027, ROC-AUC: 0.6550
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2919
Batch 20000/56275 completed, running loss: 0.2917
Batch 30000/56275 completed, running loss: 0.2941
Batch 40000/56275 completed, running loss: 0.2969
Batch 50000/56275 completed, running loss: 0.2994
Starting evaluation with 7456 batches
Fold 3 - Epoch 8/50
Train Loss: 0.3003, Train Acc: 0.8637
Val Loss: 0.9671, Val Acc: 0.5969, ROC-AUC: 0.6493
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2711
Batch 20000/56275 completed, running loss: 0.2746
Batch 30000/56275 completed, running loss: 0.2787
Batch 40000/56275 completed, running loss: 0.2808
Batch 50000/56275 completed, running loss: 0.2826
Starting evaluation with 7456 batches
Fold 3 - Epoch 9/50
Train Loss: 0.2844, Train Acc: 0.8724
Val Loss: 0.9956, Val Acc: 0.6028, ROC-AUC: 0.6524
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2597
Batch 20000/56275 completed, running loss: 0.2625
Batch 30000/56275 completed, running loss: 0.2639
Batch 40000/56275 completed, running loss: 0.2662
Batch 50000/56275 completed, running loss: 0.2682
Starting evaluation with 7456 batches
Fold 3 - Epoch 10/50
Train Loss: 0.2697, Train Acc: 0.8810
Val Loss: 1.0254, Val Acc: 0.6052, ROC-AUC: 0.6571
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2460
Batch 20000/56275 completed, running loss: 0.2474
Batch 30000/56275 completed, running loss: 0.2515
Batch 40000/56275 completed, running loss: 0.2544
Batch 50000/56275 completed, running loss: 0.2564
Starting evaluation with 7456 batches
Fold 3 - Epoch 11/50
Train Loss: 0.2576, Train Acc: 0.8862
Val Loss: 1.0916, Val Acc: 0.5937, ROC-AUC: 0.6394
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2306
Batch 20000/56275 completed, running loss: 0.2357
Batch 30000/56275 completed, running loss: 0.2392
Batch 40000/56275 completed, running loss: 0.2418
Batch 50000/56275 completed, running loss: 0.2442
Starting evaluation with 7456 batches
Fold 3 - Epoch 12/50
Train Loss: 0.2453, Train Acc: 0.8929
Val Loss: 1.1263, Val Acc: 0.6099, ROC-AUC: 0.6594
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2211
Batch 20000/56275 completed, running loss: 0.2246
Batch 30000/56275 completed, running loss: 0.2287
Batch 40000/56275 completed, running loss: 0.2321
Batch 50000/56275 completed, running loss: 0.2341
Starting evaluation with 7456 batches
Fold 3 - Epoch 13/50
Train Loss: 0.2354, Train Acc: 0.8982
Val Loss: 1.0879, Val Acc: 0.6052, ROC-AUC: 0.6568
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2144
Batch 20000/56275 completed, running loss: 0.2175
Batch 30000/56275 completed, running loss: 0.2191
Batch 40000/56275 completed, running loss: 0.2206
Batch 50000/56275 completed, running loss: 0.2229
Starting evaluation with 7456 batches
Fold 3 - Epoch 14/50
Train Loss: 0.2242, Train Acc: 0.9043
Val Loss: 1.1390, Val Acc: 0.6090, ROC-AUC: 0.6624
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.2087
Batch 20000/56275 completed, running loss: 0.2091
Batch 30000/56275 completed, running loss: 0.2115
Batch 40000/56275 completed, running loss: 0.2125
Batch 50000/56275 completed, running loss: 0.2154
Starting evaluation with 7456 batches
Fold 3 - Epoch 15/50
Train Loss: 0.2165, Train Acc: 0.9080
Val Loss: 1.2261, Val Acc: 0.5914, ROC-AUC: 0.6369
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1943
Batch 20000/56275 completed, running loss: 0.1975
Batch 30000/56275 completed, running loss: 0.2031
Batch 40000/56275 completed, running loss: 0.2056
Batch 50000/56275 completed, running loss: 0.2076
Starting evaluation with 7456 batches
Fold 3 - Epoch 16/50
Train Loss: 0.2098, Train Acc: 0.9106
Val Loss: 1.2315, Val Acc: 0.5995, ROC-AUC: 0.6493
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1896
Batch 20000/56275 completed, running loss: 0.1925
Batch 30000/56275 completed, running loss: 0.1965
Batch 40000/56275 completed, running loss: 0.1979
Batch 50000/56275 completed, running loss: 0.1997
Starting evaluation with 7456 batches
Fold 3 - Epoch 17/50
Train Loss: 0.2009, Train Acc: 0.9149
Val Loss: 1.2756, Val Acc: 0.5925, ROC-AUC: 0.6443
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1828
Batch 20000/56275 completed, running loss: 0.1860
Batch 30000/56275 completed, running loss: 0.1884
Batch 40000/56275 completed, running loss: 0.1913
Batch 50000/56275 completed, running loss: 0.1931
Starting evaluation with 7456 batches
Fold 3 - Epoch 18/50
Train Loss: 0.1941, Train Acc: 0.9181
Val Loss: 1.3534, Val Acc: 0.5800, ROC-AUC: 0.6222
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1632
Batch 20000/56275 completed, running loss: 0.1622
Batch 30000/56275 completed, running loss: 0.1604
Batch 40000/56275 completed, running loss: 0.1610
Batch 50000/56275 completed, running loss: 0.1615
Starting evaluation with 7456 batches
Fold 3 - Epoch 19/50
Train Loss: 0.1616, Train Acc: 0.9337
Val Loss: 1.4771, Val Acc: 0.5920, ROC-AUC: 0.6389
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1443
Batch 20000/56275 completed, running loss: 0.1470
Batch 30000/56275 completed, running loss: 0.1492
Batch 40000/56275 completed, running loss: 0.1499
Batch 50000/56275 completed, running loss: 0.1508
Starting evaluation with 7456 batches
Fold 3 - Epoch 20/50
Train Loss: 0.1516, Train Acc: 0.9386
Val Loss: 1.4607, Val Acc: 0.6005, ROC-AUC: 0.6476
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1360
Batch 20000/56275 completed, running loss: 0.1392
Batch 30000/56275 completed, running loss: 0.1417
Batch 40000/56275 completed, running loss: 0.1428
Batch 50000/56275 completed, running loss: 0.1434
Starting evaluation with 7456 batches
Fold 3 - Epoch 21/50
Train Loss: 0.1444, Train Acc: 0.9419
Val Loss: 1.5776, Val Acc: 0.5860, ROC-AUC: 0.6304
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1336
Batch 20000/56275 completed, running loss: 0.1356
Batch 30000/56275 completed, running loss: 0.1358
Batch 40000/56275 completed, running loss: 0.1371
Batch 50000/56275 completed, running loss: 0.1384
Starting evaluation with 7456 batches
Fold 3 - Epoch 22/50
Train Loss: 0.1385, Train Acc: 0.9442
Val Loss: 1.5944, Val Acc: 0.6001, ROC-AUC: 0.6485
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1298
Batch 20000/56275 completed, running loss: 0.1288
Batch 30000/56275 completed, running loss: 0.1307
Batch 40000/56275 completed, running loss: 0.1325
Batch 50000/56275 completed, running loss: 0.1336
Starting evaluation with 7456 batches
Fold 3 - Epoch 23/50
Train Loss: 0.1343, Train Acc: 0.9467
Val Loss: 1.6491, Val Acc: 0.5900, ROC-AUC: 0.6333
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1209
Batch 20000/56275 completed, running loss: 0.1226
Batch 30000/56275 completed, running loss: 0.1257
Batch 40000/56275 completed, running loss: 0.1273
Batch 50000/56275 completed, running loss: 0.1285
Starting evaluation with 7456 batches
Fold 3 - Epoch 24/50
Train Loss: 0.1291, Train Acc: 0.9489
Val Loss: 1.6238, Val Acc: 0.5998, ROC-AUC: 0.6465
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1208
Batch 20000/56275 completed, running loss: 0.1234
Batch 30000/56275 completed, running loss: 0.1234
Batch 40000/56275 completed, running loss: 0.1259
Batch 50000/56275 completed, running loss: 0.1277
Starting evaluation with 7456 batches
Fold 3 - Epoch 25/50
Train Loss: 0.1284, Train Acc: 0.9494
Val Loss: 1.6486, Val Acc: 0.6023, ROC-AUC: 0.6507
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1194
Batch 20000/56275 completed, running loss: 0.1218
Batch 30000/56275 completed, running loss: 0.1221
Batch 40000/56275 completed, running loss: 0.1223
Batch 50000/56275 completed, running loss: 0.1236
Starting evaluation with 7456 batches
Fold 3 - Epoch 26/50
Train Loss: 0.1244, Train Acc: 0.9509
Val Loss: 1.6982, Val Acc: 0.5938, ROC-AUC: 0.6411
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1171
Batch 20000/56275 completed, running loss: 0.1183
Batch 30000/56275 completed, running loss: 0.1202
Batch 40000/56275 completed, running loss: 0.1207
Batch 50000/56275 completed, running loss: 0.1222
Starting evaluation with 7456 batches
Fold 3 - Epoch 27/50
Train Loss: 0.1229, Train Acc: 0.9522
Val Loss: 1.6779, Val Acc: 0.5955, ROC-AUC: 0.6437
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1159
Batch 20000/56275 completed, running loss: 0.1158
Batch 30000/56275 completed, running loss: 0.1178
Batch 40000/56275 completed, running loss: 0.1181
Batch 50000/56275 completed, running loss: 0.1184
Starting evaluation with 7456 batches
Fold 3 - Epoch 28/50
Train Loss: 0.1186, Train Acc: 0.9536
Val Loss: 1.7351, Val Acc: 0.5985, ROC-AUC: 0.6454
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1124
Batch 20000/56275 completed, running loss: 0.1130
Batch 30000/56275 completed, running loss: 0.1149
Batch 40000/56275 completed, running loss: 0.1163
Batch 50000/56275 completed, running loss: 0.1173
Starting evaluation with 7456 batches
Fold 3 - Epoch 29/50
Train Loss: 0.1175, Train Acc: 0.9546
Val Loss: 1.7915, Val Acc: 0.5911, ROC-AUC: 0.6339
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1093
Batch 20000/56275 completed, running loss: 0.1115
Batch 30000/56275 completed, running loss: 0.1129
Batch 40000/56275 completed, running loss: 0.1136
Batch 50000/56275 completed, running loss: 0.1153
Starting evaluation with 7456 batches
Fold 3 - Epoch 30/50
Train Loss: 0.1159, Train Acc: 0.9554
Val Loss: 1.7950, Val Acc: 0.5900, ROC-AUC: 0.6337
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1056
Batch 20000/56275 completed, running loss: 0.1101
Batch 30000/56275 completed, running loss: 0.1115
Batch 40000/56275 completed, running loss: 0.1133
Batch 50000/56275 completed, running loss: 0.1146
Starting evaluation with 7456 batches
Fold 3 - Epoch 31/50
Train Loss: 0.1154, Train Acc: 0.9560
Val Loss: 1.7395, Val Acc: 0.5995, ROC-AUC: 0.6477
Starting training epoch with 56275 batches
Batch 10000/56275 completed, running loss: 0.1058
Batch 20000/56275 completed, running loss: 0.1096
Batch 30000/56275 completed, running loss: 0.1098
Batch 40000/56275 completed, running loss: 0.1100
Batch 50000/56275 completed, running loss: 0.1111
Starting evaluation with 7456 batches
Fold 3 - Epoch 32/50
Train Loss: 0.1118, Train Acc: 0.9575
Val Loss: 1.8205, Val Acc: 0.5912, ROC-AUC: 0.6353
Fold 3 - Early stopping triggered
Starting evaluation with 56275 batches
Evaluation batch 10000/56275 completed
Evaluation batch 20000/56275 completed
Evaluation batch 30000/56275 completed
Evaluation batch 40000/56275 completed
Evaluation batch 50000/56275 completed
Starting evaluation with 7456 batches
Starting evaluation with 8740 batches
Fold 3 - Test Metrics:
Loss: 0.7753
Accuracy: 0.5819
Precision: 0.5094
Recall: 0.5415
Roc_auc: 0.6226
Specificity: 0.6120
F1: 0.5250

Starting Fold 4/10
Fold 4 - Training label counts: {0: 119065, 1: 108862}
Fold 4 - Validation label counts: {0: 16807, 1: 13362}
Fold 4 - Test label counts: {0: 16279, 1: 15503}
Preparing dataset with 227927 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 227927 samples
Preparing dataset with 30169 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 30169 samples
Preparing dataset with 31782 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 31782 samples
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.6098
Batch 20000/56982 completed, running loss: 0.5916
Batch 30000/56982 completed, running loss: 0.5782
Batch 40000/56982 completed, running loss: 0.5680
Batch 50000/56982 completed, running loss: 0.5591
Starting evaluation with 7543 batches
Fold 4 - Epoch 1/50
Train Loss: 0.5542, Train Acc: 0.7021
Val Loss: 0.7016, Val Acc: 0.5996, ROC-AUC: 0.6484
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.4824
Batch 20000/56982 completed, running loss: 0.4817
Batch 30000/56982 completed, running loss: 0.4809
Batch 40000/56982 completed, running loss: 0.4803
Batch 50000/56982 completed, running loss: 0.4780
Starting evaluation with 7543 batches
Fold 4 - Epoch 2/50
Train Loss: 0.4762, Train Acc: 0.7609
Val Loss: 0.7670, Val Acc: 0.5831, ROC-AUC: 0.6302
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.4266
Batch 20000/56982 completed, running loss: 0.4276
Batch 30000/56982 completed, running loss: 0.4280
Batch 40000/56982 completed, running loss: 0.4276
Batch 50000/56982 completed, running loss: 0.4273
Starting evaluation with 7543 batches
Fold 4 - Epoch 3/50
Train Loss: 0.4273, Train Acc: 0.7916
Val Loss: 0.8277, Val Acc: 0.5882, ROC-AUC: 0.6307
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.3880
Batch 20000/56982 completed, running loss: 0.3917
Batch 30000/56982 completed, running loss: 0.3905
Batch 40000/56982 completed, running loss: 0.3914
Batch 50000/56982 completed, running loss: 0.3923
Starting evaluation with 7543 batches
Fold 4 - Epoch 4/50
Train Loss: 0.3924, Train Acc: 0.8134
Val Loss: 0.8701, Val Acc: 0.5769, ROC-AUC: 0.6170
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.3498
Batch 20000/56982 completed, running loss: 0.3537
Batch 30000/56982 completed, running loss: 0.3576
Batch 40000/56982 completed, running loss: 0.3597
Batch 50000/56982 completed, running loss: 0.3617
Starting evaluation with 7543 batches
Fold 4 - Epoch 5/50
Train Loss: 0.3623, Train Acc: 0.8303
Val Loss: 0.8627, Val Acc: 0.5941, ROC-AUC: 0.6473
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.3253
Batch 20000/56982 completed, running loss: 0.3310
Batch 30000/56982 completed, running loss: 0.3342
Batch 40000/56982 completed, running loss: 0.3352
Batch 50000/56982 completed, running loss: 0.3361
Starting evaluation with 7543 batches
Fold 4 - Epoch 6/50
Train Loss: 0.3372, Train Acc: 0.8443
Val Loss: 0.9411, Val Acc: 0.5835, ROC-AUC: 0.6293
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.3061
Batch 20000/56982 completed, running loss: 0.3099
Batch 30000/56982 completed, running loss: 0.3108
Batch 40000/56982 completed, running loss: 0.3127
Batch 50000/56982 completed, running loss: 0.3144
Starting evaluation with 7543 batches
Fold 4 - Epoch 7/50
Train Loss: 0.3157, Train Acc: 0.8561
Val Loss: 0.9748, Val Acc: 0.5810, ROC-AUC: 0.6278
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2818
Batch 20000/56982 completed, running loss: 0.2863
Batch 30000/56982 completed, running loss: 0.2913
Batch 40000/56982 completed, running loss: 0.2944
Batch 50000/56982 completed, running loss: 0.2959
Starting evaluation with 7543 batches
Fold 4 - Epoch 8/50
Train Loss: 0.2971, Train Acc: 0.8662
Val Loss: 1.0117, Val Acc: 0.5880, ROC-AUC: 0.6385
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2653
Batch 20000/56982 completed, running loss: 0.2710
Batch 30000/56982 completed, running loss: 0.2726
Batch 40000/56982 completed, running loss: 0.2750
Batch 50000/56982 completed, running loss: 0.2773
Starting evaluation with 7543 batches
Fold 4 - Epoch 9/50
Train Loss: 0.2793, Train Acc: 0.8751
Val Loss: 1.0438, Val Acc: 0.5918, ROC-AUC: 0.6400
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2510
Batch 20000/56982 completed, running loss: 0.2588
Batch 30000/56982 completed, running loss: 0.2609
Batch 40000/56982 completed, running loss: 0.2627
Batch 50000/56982 completed, running loss: 0.2654
Starting evaluation with 7543 batches
Fold 4 - Epoch 10/50
Train Loss: 0.2662, Train Acc: 0.8829
Val Loss: 1.1101, Val Acc: 0.5882, ROC-AUC: 0.6343
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2383
Batch 20000/56982 completed, running loss: 0.2445
Batch 30000/56982 completed, running loss: 0.2484
Batch 40000/56982 completed, running loss: 0.2504
Batch 50000/56982 completed, running loss: 0.2520
Starting evaluation with 7543 batches
Fold 4 - Epoch 11/50
Train Loss: 0.2533, Train Acc: 0.8884
Val Loss: 1.1610, Val Acc: 0.5773, ROC-AUC: 0.6226
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2246
Batch 20000/56982 completed, running loss: 0.2308
Batch 30000/56982 completed, running loss: 0.2336
Batch 40000/56982 completed, running loss: 0.2366
Batch 50000/56982 completed, running loss: 0.2397
Starting evaluation with 7543 batches
Fold 4 - Epoch 12/50
Train Loss: 0.2412, Train Acc: 0.8948
Val Loss: 1.1670, Val Acc: 0.5872, ROC-AUC: 0.6349
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2210
Batch 20000/56982 completed, running loss: 0.2220
Batch 30000/56982 completed, running loss: 0.2236
Batch 40000/56982 completed, running loss: 0.2261
Batch 50000/56982 completed, running loss: 0.2282
Starting evaluation with 7543 batches
Fold 4 - Epoch 13/50
Train Loss: 0.2302, Train Acc: 0.9008
Val Loss: 1.2193, Val Acc: 0.5796, ROC-AUC: 0.6249
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2122
Batch 20000/56982 completed, running loss: 0.2132
Batch 30000/56982 completed, running loss: 0.2134
Batch 40000/56982 completed, running loss: 0.2170
Batch 50000/56982 completed, running loss: 0.2196
Starting evaluation with 7543 batches
Fold 4 - Epoch 14/50
Train Loss: 0.2222, Train Acc: 0.9056
Val Loss: 1.1464, Val Acc: 0.5931, ROC-AUC: 0.6452
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.2003
Batch 20000/56982 completed, running loss: 0.2032
Batch 30000/56982 completed, running loss: 0.2065
Batch 40000/56982 completed, running loss: 0.2081
Batch 50000/56982 completed, running loss: 0.2104
Starting evaluation with 7543 batches
Fold 4 - Epoch 15/50
Train Loss: 0.2121, Train Acc: 0.9105
Val Loss: 1.2549, Val Acc: 0.5887, ROC-AUC: 0.6373
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1928
Batch 20000/56982 completed, running loss: 0.1964
Batch 30000/56982 completed, running loss: 0.1990
Batch 40000/56982 completed, running loss: 0.2025
Batch 50000/56982 completed, running loss: 0.2045
Starting evaluation with 7543 batches
Fold 4 - Epoch 16/50
Train Loss: 0.2053, Train Acc: 0.9139
Val Loss: 1.2912, Val Acc: 0.5796, ROC-AUC: 0.6252
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1859
Batch 20000/56982 completed, running loss: 0.1888
Batch 30000/56982 completed, running loss: 0.1927
Batch 40000/56982 completed, running loss: 0.1952
Batch 50000/56982 completed, running loss: 0.1967
Starting evaluation with 7543 batches
Fold 4 - Epoch 17/50
Train Loss: 0.1977, Train Acc: 0.9172
Val Loss: 1.3395, Val Acc: 0.5838, ROC-AUC: 0.6311
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1665
Batch 20000/56982 completed, running loss: 0.1635
Batch 30000/56982 completed, running loss: 0.1625
Batch 40000/56982 completed, running loss: 0.1622
Batch 50000/56982 completed, running loss: 0.1621
Starting evaluation with 7543 batches
Fold 4 - Epoch 18/50
Train Loss: 0.1627, Train Acc: 0.9335
Val Loss: 1.4576, Val Acc: 0.5912, ROC-AUC: 0.6417
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1472
Batch 20000/56982 completed, running loss: 0.1484
Batch 30000/56982 completed, running loss: 0.1486
Batch 40000/56982 completed, running loss: 0.1493
Batch 50000/56982 completed, running loss: 0.1507
Starting evaluation with 7543 batches
Fold 4 - Epoch 19/50
Train Loss: 0.1514, Train Acc: 0.9387
Val Loss: 1.5177, Val Acc: 0.5828, ROC-AUC: 0.6281
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1385
Batch 20000/56982 completed, running loss: 0.1400
Batch 30000/56982 completed, running loss: 0.1410
Batch 40000/56982 completed, running loss: 0.1421
Batch 50000/56982 completed, running loss: 0.1442
Starting evaluation with 7543 batches
Fold 4 - Epoch 20/50
Train Loss: 0.1453, Train Acc: 0.9411
Val Loss: 1.5908, Val Acc: 0.5802, ROC-AUC: 0.6233
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1341
Batch 20000/56982 completed, running loss: 0.1374
Batch 30000/56982 completed, running loss: 0.1386
Batch 40000/56982 completed, running loss: 0.1380
Batch 50000/56982 completed, running loss: 0.1394
Starting evaluation with 7543 batches
Fold 4 - Epoch 21/50
Train Loss: 0.1400, Train Acc: 0.9441
Val Loss: 1.6233, Val Acc: 0.5846, ROC-AUC: 0.6320
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1305
Batch 20000/56982 completed, running loss: 0.1320
Batch 30000/56982 completed, running loss: 0.1327
Batch 40000/56982 completed, running loss: 0.1333
Batch 50000/56982 completed, running loss: 0.1349
Starting evaluation with 7543 batches
Fold 4 - Epoch 22/50
Train Loss: 0.1354, Train Acc: 0.9460
Val Loss: 1.6218, Val Acc: 0.5866, ROC-AUC: 0.6355
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1253
Batch 20000/56982 completed, running loss: 0.1286
Batch 30000/56982 completed, running loss: 0.1280
Batch 40000/56982 completed, running loss: 0.1303
Batch 50000/56982 completed, running loss: 0.1308
Starting evaluation with 7543 batches
Fold 4 - Epoch 23/50
Train Loss: 0.1316, Train Acc: 0.9487
Val Loss: 1.6585, Val Acc: 0.5886, ROC-AUC: 0.6330
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1218
Batch 20000/56982 completed, running loss: 0.1214
Batch 30000/56982 completed, running loss: 0.1235
Batch 40000/56982 completed, running loss: 0.1244
Batch 50000/56982 completed, running loss: 0.1256
Starting evaluation with 7543 batches
Fold 4 - Epoch 24/50
Train Loss: 0.1279, Train Acc: 0.9495
Val Loss: 1.6556, Val Acc: 0.5857, ROC-AUC: 0.6317
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1207
Batch 20000/56982 completed, running loss: 0.1214
Batch 30000/56982 completed, running loss: 0.1219
Batch 40000/56982 completed, running loss: 0.1230
Batch 50000/56982 completed, running loss: 0.1236
Starting evaluation with 7543 batches
Fold 4 - Epoch 25/50
Train Loss: 0.1246, Train Acc: 0.9509
Val Loss: 1.6972, Val Acc: 0.5892, ROC-AUC: 0.6349
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1123
Batch 20000/56982 completed, running loss: 0.1159
Batch 30000/56982 completed, running loss: 0.1170
Batch 40000/56982 completed, running loss: 0.1190
Batch 50000/56982 completed, running loss: 0.1199
Starting evaluation with 7543 batches
Fold 4 - Epoch 26/50
Train Loss: 0.1201, Train Acc: 0.9527
Val Loss: 1.8382, Val Acc: 0.5836, ROC-AUC: 0.6297
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1173
Batch 20000/56982 completed, running loss: 0.1166
Batch 30000/56982 completed, running loss: 0.1172
Batch 40000/56982 completed, running loss: 0.1192
Batch 50000/56982 completed, running loss: 0.1193
Starting evaluation with 7543 batches
Fold 4 - Epoch 27/50
Train Loss: 0.1201, Train Acc: 0.9534
Val Loss: 1.7459, Val Acc: 0.5890, ROC-AUC: 0.6351
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1111
Batch 20000/56982 completed, running loss: 0.1128
Batch 30000/56982 completed, running loss: 0.1128
Batch 40000/56982 completed, running loss: 0.1143
Batch 50000/56982 completed, running loss: 0.1157
Starting evaluation with 7543 batches
Fold 4 - Epoch 28/50
Train Loss: 0.1166, Train Acc: 0.9550
Val Loss: 1.7701, Val Acc: 0.5863, ROC-AUC: 0.6334
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1088
Batch 20000/56982 completed, running loss: 0.1095
Batch 30000/56982 completed, running loss: 0.1105
Batch 40000/56982 completed, running loss: 0.1118
Batch 50000/56982 completed, running loss: 0.1135
Starting evaluation with 7543 batches
Fold 4 - Epoch 29/50
Train Loss: 0.1143, Train Acc: 0.9560
Val Loss: 1.8075, Val Acc: 0.5846, ROC-AUC: 0.6314
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1056
Batch 20000/56982 completed, running loss: 0.1078
Batch 30000/56982 completed, running loss: 0.1092
Batch 40000/56982 completed, running loss: 0.1107
Batch 50000/56982 completed, running loss: 0.1111
Starting evaluation with 7543 batches
Fold 4 - Epoch 30/50
Train Loss: 0.1120, Train Acc: 0.9572
Val Loss: 1.8035, Val Acc: 0.5856, ROC-AUC: 0.6310
Starting training epoch with 56982 batches
Batch 10000/56982 completed, running loss: 0.1006
Batch 20000/56982 completed, running loss: 0.1045
Batch 30000/56982 completed, running loss: 0.1070
Batch 40000/56982 completed, running loss: 0.1087
Batch 50000/56982 completed, running loss: 0.1100
Starting evaluation with 7543 batches
Fold 4 - Epoch 31/50
Train Loss: 0.1104, Train Acc: 0.9573
Val Loss: 1.8521, Val Acc: 0.5877, ROC-AUC: 0.6330
Fold 4 - Early stopping triggered
Starting evaluation with 56982 batches
Evaluation batch 10000/56982 completed
Evaluation batch 20000/56982 completed
Evaluation batch 30000/56982 completed
Evaluation batch 40000/56982 completed
Evaluation batch 50000/56982 completed
Starting evaluation with 7543 batches
Starting evaluation with 7946 batches
Fold 4 - Test Metrics:
Loss: 0.7593
Accuracy: 0.5932
Precision: 0.5866
Recall: 0.5621
Roc_auc: 0.6233
Specificity: 0.6228
F1: 0.5741

Starting Fold 5/10
Fold 5 - Training label counts: {0: 121048, 1: 110209}
Fold 5 - Validation label counts: {0: 16911, 1: 13561}
Fold 5 - Test label counts: {0: 14192, 1: 13957}
Preparing dataset with 231257 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 231257 samples
Preparing dataset with 30472 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 30472 samples
Preparing dataset with 28149 samples
Extracting vectors
Normalizing embeddings
Dataset ready with 28149 samples
Starting training epoch with 57815 batches
Batch 10000/57815 completed, running loss: 0.6136
